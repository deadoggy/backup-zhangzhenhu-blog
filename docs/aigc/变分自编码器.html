

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<meta content="变分自编码器" lang="zh_CN" name="description" xml:lang="zh_CN" />
<meta content="变分自编码器,Variational Autoencoder,VAE,扩散模型,Diffusion Model,生成模型,图像生成" name="keywords" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>1. 变分自编码器（Variational Autoencoder） &mdash; 张振虎的博客 张振虎 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />

  
  

  
  

  
    <link rel="canonical" href="https://www.zhangzhenhu.com/aigc/变分自编码器.html" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/translations.js"></script>
        <script>window.MathJax = {"tex": {"macros": {"RR": "{\\bf R}", "bold": ["{\\bf #1}", 1], "KL": ["{D_\\textrm{KL}\\left ( #1 \\| #2 \\right )}", 2], "EE": ["{\\mathbb{E}_{#1} \\left [ #2 \\right ]}", 2, ""], "scalemath": ["{\\scalebox{#1}{\\mbox{\\ensuremath{\\displaystyle #2}}}}", 2], "argmin": ["{\\operatorname*{\\arg\\min}}"], "ind": ["{\\perp\\!\\!\\!\\!\\perp}"]}}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="2. 扩散概率模型（diffusion probabilistic models）" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html" />
    <link rel="prev" title="AI内容生成（ai-gc）" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> 张振虎的博客
          

          
          </a>

          
            
            
              <div class="version">
                acmtiger@outlook.com
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">目录:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../glm/source/index.html">广义线性模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id2">1.1. 概率模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id3">1.1.1. 概率律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id4">1.1.2. 离散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id5">1.1.3. 连续模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id6">1.2. 条件概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id7">1.3. 联合概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id8">1.4. 全概率与贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id9">1.5. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id10">1.6. 随机变量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id11">1.6.1. 离散随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id12">1.6.2. 连续随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id13">1.6.3. 累积分布函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id14">1.6.4. 随机变量的函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id15">1.6.5. 期望与方差</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id16">1.7. 边缘化</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id17">1.8. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#ch-basic-bernoulli">1.8.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id19">1.8.2. 二项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id20">1.8.3. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id21">1.8.4. 多项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id22">1.8.5. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id23">1.8.6. 卡方分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#t">1.8.7. t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#f">1.8.8. F分布</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html">2. 最大似然估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-liklihood">2.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id3">2.2. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id4">2.3. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-2-gaussian-ml">2.4. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id6">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html">3. 推断与检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id2">3.1. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution">3.2. 抽样分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution-normal">3.2.1. 正态分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#t">3.2.2. 学生t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id5">3.2.3. 卡方分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id6">3.3. 极限理论</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id7">3.3.1. 马尔可夫和切比雪夫不等式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id8">3.3.2. 弱大数定律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id9">3.3.3. 依概率收敛</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-clt">3.3.4. 中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id11">3.3.5. 强大数定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id12">3.4. 似然估计量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id13">3.4.1. 估计量的偏差与方差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-fisher-information">3.4.2. 信息量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-mle-estimator">3.4.3. 最大似然估计的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-interval">3.5. 置信区间</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#z">3.5.1. 均值参数的 Z 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id18">3.5.2. 均值参数的 T 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id19">3.5.3. 方差参数的区间估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-test">3.6. 简单假设检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id22">3.6.1. Z检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id23">3.6.2. T检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id24">3.6.3. 卡方检验</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html">4. 贝叶斯估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id2">4.1. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id3">4.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id4">4.1.2. 类别分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id5">4.2. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id6">4.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id7">4.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id8">4.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html">5. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-1">5.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id3">5.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id4">5.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id5">5.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id6">5.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id7">5.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-moments">5.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id9">5.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#kl">5.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html">6. 线性回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id2">6.1. 最小二乘</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id3">6.1.1. 最小误差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id4">6.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id5">6.2. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id6">6.2.1. 高斯假设</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id7">6.2.2. 参数估计</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html">7. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id2">7.1. 指数族分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id3">7.1.1. 自然指数族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id4">7.1.2. 示例：高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id5">7.1.3. 示例：伯努利分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id6">7.2. 广义线性模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id7">7.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html">8. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate">8.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id3">8.2. 泰勒级数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id4">8.3. 梯度下降法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id6">8.4. 牛顿法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id7">8.4.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id8">8.4.2. 标准连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id9">8.4.3. 迭代初始值的设定</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#irls">8.5. 迭代重加权最小二乘(IRLS)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id10">8.5.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id11">8.5.2. 算法过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id12">8.6. 估计量的标准误差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate-phi">8.7. 分散参数的估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html">9. 模型评估</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id2">9.1. 拟合优度</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id3">9.1.1. 嵌套模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#likelihood-ratio">9.1.2. 对数似然比(Likelihood ratio)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance">9.1.3. 偏差(deviance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#r-2">9.1.4. 决定系数 <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#ch-glm-gof-chi">9.1.5. 广义皮尔逊卡方统计量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#residual-analysis">9.2. 残差分析(Residual analysis)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#response-residuals">9.2.1. Response residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#working-residuals">9.2.2. Working residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#partial-residuals">9.2.3. Partial residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#pearson-residuals">9.2.4. Pearson residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance-residuals">9.2.5. Deviance residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#score-residuals">9.2.6. Score residuals</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#model-selection">9.3. 模型选择(model selection)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#aic">9.3.1. AIC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#bic">9.3.2. BIC</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html">10. 模型检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id2">10.1. 拉格朗日乘子检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id3">10.1.1. 得分统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id4">10.1.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#wald">10.2. wald 检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id5">10.2.1. 参数估计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id6">10.2.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id7">10.3. 似然比检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id8">10.3.1. 抽样分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id9">10.3.2. 模型比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id10">10.3.3. 偏差统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#f">10.3.4. F 检验</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id11">10.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">11. 高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">11.1. 传统线性回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">11.2. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">11.3. 高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">11.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">11.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">11.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">11.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id10">11.5. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">12. 逆高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">12.1. 逆高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">12.2. 逆高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">12.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">12.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">12.3.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">12.3.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">12.4. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">13. 二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">13.1. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">13.2. 逻辑回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">13.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">13.2.2. 参数估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#odds-logit">13.2.3. odds 与 logit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">13.3. 二项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">13.4. 二项式回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">13.4.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">13.4.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">13.5. 其它连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">13.5.1. 恒等连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#probit">13.5.2. probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#log-log-clog-log">13.5.3. log-log 和 clog-log</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">13.6. 分组数据与比例数据</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html">14. 泊松模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#poisson">14.1. 泊松(Poisson)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id2">14.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id3">14.1.2. 泊松分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id5">14.2. 泊松回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id6">14.3. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id7">14.4. 拟合统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id8">14.5. 频率模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id9">14.6. 泊松模型的局限性</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html">15. 指数模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#exponential">15.1. 指数(exponential)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id2">15.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id3">15.1.2. 分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id6">15.2. 指数回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id7">15.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id8">15.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id9">15.3.2. 拟合优度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#irls">15.3.3. IRLS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html">16. Gamma 模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id1">16.1. Gamma 函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id2">16.2. Gamma 分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id3">16.3. Gamma 回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id4">16.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id5">16.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#irls">16.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id6">16.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id7">16.5. 其他连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id8">16.5.1. 对数 Gamma 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#identity-gamma">16.5.2. 恒等(identity) Gamma 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html">17. 过度分散</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id2">17.1. 什么是过度分散</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id3">17.2. 过度分散的检测</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id4">17.3. 过度分散的影响</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id5">17.4. 标准误差的修正</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">18. 负二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">18.1. 负二项式分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">18.1.1. 从二项式分布推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">18.1.2. 泊松-伽马混合分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#alpha">18.1.3. 辅助参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的影响</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">18.2. 负二项回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">18.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#irls">18.3.1. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">18.3.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">18.4. 负二项式模型扩展</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">18.4.1. 对数连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">18.4.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">18.4.3. 几何模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">18.4.4. 广义负二项式模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html">19. 零计数问题</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id2">19.1. 零截断模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id3">19.1.1. 零截断泊松模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id4">19.1.2. 零截断负二项式模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id5">19.2. 零膨胀模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#hurdle">19.2.1. Hurdle 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#zero-inflate">19.2.2. Zero-inflate 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">20. 多项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">20.1. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#softmax">20.2. softmax 回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">20.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">20.3. 多项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id6">20.4. 多项式回归模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">21. 有序离散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">21.1. 有序逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">21.2. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">21.3. 连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#logit">21.3.1. logit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#probit">21.3.2. probit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#clog-log">21.3.3. clog-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#log-log">21.3.4. log-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#cauchit">21.3.5. cauchit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">21.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html">附录</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id2">标准正态累积分布表</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id3">卡方分布临界值表</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/content.html">参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../probability_model/index_html.html">概率图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id2">1.1. 概率分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id3">1.2. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#marginalization">1.3. 边缘化(marginalization)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id6">1.4. 贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id7">1.5. 期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id8">1.6. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id9">1.6.1. 离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id10">1.6.2. 连续变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id11">1.6.3. 计数变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id12">1.7. 大数定律</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id13">1.7.1. 独立同分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id14">1.7.2. 中心极限定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id15">1.8. 信息论基础</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id16">1.8.1. 信息熵</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#kl">1.8.2. KL散度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id18">1.8.3. 互信息</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html">2. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-liklihood">2.1. 极大似然估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id3">2.1.1. 二值离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id4">2.1.2. 一般离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-gaussian-ml">2.1.3. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id6">2.1.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-bayesian-estimation">2.2. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id8">2.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id9">2.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id10">2.3. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id11">2.3.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id12">2.3.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id13">2.4. 最大似然估计与贝叶斯估计的对比</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id14">2.5. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#fisher-information">2.6. Fisher Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id15">2.7. 估计量的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id16">2.7.1. 估计量的方差与偏差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id17">2.7.2. 大数定律和中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-mle-estimator">2.7.3. 最大似然估计的特性</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html">3. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-1">3.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id3">3.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id4">3.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id5">3.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id6">3.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id7">3.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-moments">3.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id9">3.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#kl">3.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/19.%E5%A4%9A%E7%BB%B4%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_26.html">4. 多维高斯分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html">5. 有向图(Directed Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id1">5.1. 有向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id2">5.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id3">5.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html">6. 无向图(Undirected Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id1">6.1. 无向图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id2">6.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id3">6.3. 图的分解</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#vs">6.4. 有向图 vs 无向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id4">6.5. 树</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id5">6.6. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html">7. 因子图</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id2">7.1. 因子图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id3">7.2. 图模型之间的转换</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id4">7.2.1. 转换为因子图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id5">7.2.2. 因子图转换为有向图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id6">7.3. 图模型的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#i-map">7.3.1. I-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#d-map">7.3.2. D-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#p-map">7.3.3. P-map</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html">8. 模型推断：消元法</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id2">8.1. 什么是模型的推断</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id3">8.2. 消元法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id4">8.2.1. 有向图消元算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#ch-condition-margin">8.2.2. 条件概率和边缘概率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id6">8.2.3. 无向图的消元法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id7">8.3. 图消除</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id9">8.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html">9. 加和乘积算法(sum-product algorithm)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id1">9.1. 树结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id2">9.2. 从消元法到信息传播</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id3">9.3. 树模型的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id4">9.4. 因子图的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id5">9.5. 类树结构图模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#polytrees">9.6. 多重树(polytrees)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id6">9.7. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html">10. 最大后验估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id2">10.1. 最大后验概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id3">10.2. 最大化后验的状态</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id4">10.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html">11. 完整观测的参数学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id2">11.1. 有向图的参数学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id3">11.2. 无向图的参数学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id4">11.2.1. 成对二值变量模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id5">11.2.2. 一般二值变量模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html">12. 不完整观测的学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#id2">12.1. 隐变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#em">12.2. 期望最大化算法(EM)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/14.%E5%9B%BE%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E5%AD%A6%E4%B9%A0_lecture_23.html">13. 有向图结构学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/16.%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD_lecture_17.html">14. 变分推断</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html">15. 马尔科夫蒙特卡洛</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#why-sampling">15.1. Why sampling？</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#monte-carlo">15.2. 蒙特卡罗(Monte Carlo)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain">15.3. 马尔科夫链(Markov Chain)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id2">15.3.1. 一个例子</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#time-reversibility">15.3.2. 时间可逆性(Time Reversibility)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id3">15.3.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain-monte-carlo">15.4. Markov Chain Monte Carlo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#metropolis-hastings">15.4.1. Metropolis-Hastings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id4">15.4.2. 例子：正态分布的采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id5">15.4.3. 多变量采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#gibbs">15.4.4. Gibbs 采样</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#mixing-time">15.5. Mixing Time</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#approximate-map-and-partitioning">15.6. Approximate MAP and Partitioning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html">16. 贝叶斯分类器</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id2">16.1. 朴素贝叶斯模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id3">16.1.1. 模型表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id4">16.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id5">16.2. 高斯判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id6">16.2.1. 一元高斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id7">16.2.2. 多元高斯模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id8">16.3. 逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id9">16.4. 生成模型和判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id10">16.5. 多分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id11">16.6. 其它扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html">17. 回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id2">17.1. 机器学习的概率解释</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id3">17.2. 经典线性回归</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id4">17.2.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id5">17.3. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id6">17.3.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id7">17.4. 凸函数最优化问题</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id8">17.5. 岭回归</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html">18. 分类模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id2">18.1. 生成模型与判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id3">18.2. 线性回归与线性分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id4">18.3. 生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id5">18.3.1. 高斯判别模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id6">18.3.2. 朴素贝叶斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id7">18.3.3. 指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id8">18.4. 判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id9">18.4.1. 逻辑回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id10">18.4.2. 多分类</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id11">18.4.3. 最大熵模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#probit">18.4.4. Probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#noisy-or">18.4.5. Noisy-OR 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id12">18.4.6. 其它指数模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html">19. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id2">19.1. 定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id3">19.1.1. 指数族分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id4">19.1.2. 链接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id5">19.1.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id6">19.2. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id7">19.2.1. 梯度下降法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id8">19.2.2. 牛顿法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#irls">19.2.3. 迭代重加权最小二乘(IRLS)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#goodness-of-fit">19.3. goodness of fit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id9">19.4. 连续值响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id10">19.4.1. 高斯族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#gamma">19.4.2. Gamma族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id12">19.5. 二项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id13">19.6. 多项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id14">19.7. 计数响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id15">19.7.1. 泊松分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id16">19.8. GLM扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html">20. 混合模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id2">20.1. 一般混合模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id3">20.1.1. 模型的有向图表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id5">20.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id8">20.2. 高斯混合模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id9">20.2.1. 模型的表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id10">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#k-means">20.3. K-means</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/32.%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90_42.html">21. 因子分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/%E4%BA%8C%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B.html">22. 二变量模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/33.LDA_43.html">23. 主题模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/33.LDA_43.html#plsa">23.1. PLSA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/33.LDA_43.html#lda">23.2. LDA</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html">24. 隐马尔可夫模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id2">24.1. 隐马尔可夫模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id3">24.1.1. 马尔可夫模型和朴素贝叶斯模型的关系</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id4">24.2. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/27.%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA_37.html">25. 条件随机场</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/28.%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8_38.html">26. 卡尔曼滤波器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/40.%E9%A1%B9%E7%9B%AE%E5%8F%8D%E5%BA%94%E7%90%86%E8%AE%BA_50.html">27. 项目反应理论</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/41.%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA_51.html">28. 贝叶斯知识追踪</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE.html">29. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">AI内容生成（ai-gc）</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">1. 变分自编码器（Variational Autoencoder）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#evidence-lower-bound-elbo">1.1. 证据下界(Evidence Lower Bound,ELBO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">1.2. 编码-解码</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">1.3. 总结</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#em">1.3.1. 和EM算法的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="#variational">1.3.2. 为什么叫变分（variational）？</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#vq-vae">1.4. VQ-VAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">1.5. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html">2. 扩散概率模型（diffusion probabilistic models）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#diffusion-probabilistic-model">2.1. 扩散概率模型（diffusion probabilistic model）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#markovian-hierarchical-variational-autoencoder-mhvae">2.1.1. 马尔科夫分层自编码器（Markovian Hierarchical Variational Autoencoder,MHVAE)</a></li>
<li class="toctree-l4"><a class="reference internal" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id6">2.1.2. 扩散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id8">2.1.3. 前向-后向</a></li>
<li class="toctree-l4"><a class="reference internal" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#elbo">2.1.4. 目标函数（ELBO）</a></li>
<li class="toctree-l4"><a class="reference internal" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id14">2.1.5. 图片生成（采样）过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#denoising-diffusion-probabilistic-model-ddpm">2.2. 降噪扩散概率模型（Denoising diffusion probabilistic model,DDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#score-based-ddpm">2.3. 基于分数的解释（Score-based DDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id23">2.4. 扩散模型的三种等价表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#improved-denoising-diffusion-probabilistic-models-iddpm">2.5. 改进降噪扩散概率模型（Improved Denoising Diffusion Probabilistic Models,IDDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id24">2.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ddim.html">3. 去噪扩散隐式模型（Denoising Diffusion Implicit Models,DDIM）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ddim.html#id1">3.1. 扩散模型的回顾</a></li>
<li class="toctree-l3"><a class="reference internal" href="ddim.html#id2">3.2. 非马尔科夫前向过程</a></li>
<li class="toctree-l3"><a class="reference internal" href="ddim.html#id4">3.3. 加速采样</a></li>
<li class="toctree-l3"><a class="reference internal" href="ddim.html#id5">3.4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="Score-Based_Generative_Models.html">4. 基于分数的生成模型（Score-based generative models）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="Score-Based_Generative_Models.html#id4">4.1. 基于分数的生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#score-matching">4.1.1. 分数匹配算法（Score Matching）</a></li>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id8">4.1.2. 基于分数的生成模型面临的困难</a></li>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id12">4.1.3. 通过加噪的方法估计分布的近似分数</a></li>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id13">4.1.4. 基于分数的改进采样算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id15">4.1.5. 改进的分数生成模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="Score-Based_Generative_Models.html#id17">4.2. 随机微分方程</a><ul>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id18">4.2.1. 微分方程</a></li>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id19">4.2.2. 随机微分方程</a></li>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id20">4.2.3. 基于随机微分方程的生成模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="Score-Based_Generative_Models.html#id21">4.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="Guidance.html">5. 条件控制扩散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="Guidance.html#classifier-guidance">5.1. classifier guidance</a></li>
<li class="toctree-l3"><a class="reference internal" href="Guidance.html#classifier-free-guidance">5.2. Classifier-free guidance</a></li>
<li class="toctree-l3"><a class="reference internal" href="Guidance.html#clip-guidance">5.3. CLIP Guidance</a><ul>
<li class="toctree-l4"><a class="reference internal" href="Guidance.html#id12">5.3.1. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dalle2.html">6. DALL·E 2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dalle2.html#glide">6.1. GLIDE</a></li>
<li class="toctree-l3"><a class="reference internal" href="dalle2.html#unclip">6.2. unCLIP</a></li>
<li class="toctree-l3"><a class="reference internal" href="dalle2.html#id3">6.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html">7. 稳定扩散模型（Stable diffusion model）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#latent-diffusion-model-ldm">7.1. 潜在扩散模型（Latent diffusion model,LDM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#stable-diffusion-sd">7.2. 稳定扩散模型（Stable diffusion,SD）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#id3">7.2.1. 推理过程代码</a></li>
<li class="toctree-l4"><a class="reference internal" href="%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#id4">7.2.2. 训练过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#id5">7.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="controlnet.html">8. 条件控制之ControlNet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="controlnet.html#id3">8.1. 算法原理</a></li>
<li class="toctree-l3"><a class="reference internal" href="controlnet.html#id6">8.2. 代码实现</a></li>
<li class="toctree-l3"><a class="reference internal" href="controlnet.html#id7">8.3. 最后的总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="controlnet.html#id8">8.4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dreamBooth.html">9. 条件控制之DreamBooth</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dreamBooth.html#id2">9.1. DreamBooth 技术</a></li>
<li class="toctree-l3"><a class="reference internal" href="dreamBooth.html#id3">9.2. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="imagen.html">10. Imagen</a><ul>
<li class="toctree-l3"><a class="reference internal" href="imagen.html#id2">10.1. 代码实现解读</a><ul>
<li class="toctree-l4"><a class="reference internal" href="imagen.html#id3">10.1.1. 第一阶段</a></li>
<li class="toctree-l4"><a class="reference internal" href="imagen.html#id4">10.1.2. 第二阶段</a></li>
<li class="toctree-l4"><a class="reference internal" href="imagen.html#id5">10.1.3. 第三阶段</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="imagen.html#id6">10.2. Imagen 总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="imagen.html#id7">10.3. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../deepspeed/index.html">deepspeed 详解-源码分析</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html">1. deepspeed - 预备知识</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html#torch-distribute">1.1. torch.distribute</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html#amp">1.2. 自动混合精度AMP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html#cuda-stream-and-event">1.3. cuda Stream and Event</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html#pin-memory">1.4. pin_memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html">2. deepspeed - 总入口</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#id1">2.1. 优化器的初始化</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#id2">2.1.1. 基础优化器</a></li>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#zero">2.1.2. 创建 ZeRO 优化器</a></li>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#f16">2.1.3. 创建 f16 半精度优化器</a></li>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#bf16">2.1.4. 创建 bf16 半精度优化器</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../deepspeed/stage2-%E5%88%9D%E5%A7%8B%E5%8C%96.html">3. stage2 - 初始化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage2-%E5%88%9D%E5%A7%8B%E5%8C%96.html#id1">3.1. 配置项初始化</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage2-%E5%88%9D%E5%A7%8B%E5%8C%96.html#id2">3.2. 参数分割</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage2-%E5%88%9D%E5%A7%8B%E5%8C%96.html#cpu-offload">3.3. cpu offload</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html">4. Stage3 - 参数分割</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#deepspeedzerooptimizer-stage3">4.1. DeepSpeedZeroOptimizer_Stage3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#deepspeedzerooffload">4.2. DeepSpeedZeRoOffload</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#init">4.3. Init 模块</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#convert-to-deepspeed-param">4.3.1. _convert_to_deepspeed_param</a></li>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#partition">4.3.2. partition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#partition-param-sec">4.3.3. partition_param_sec</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../deepspeed/stage3-hook.html">5. Stage3 - hook 注册</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html">6. Stage3 - 前后向过程</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html#id1">6.1. 参数还原</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html#all-gather-params">6.1.1. __all_gather_params</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html#id2">6.2. 参数重新分割</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html#release-param">6.2.1. release_param</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../deepspeed/index.html#id2">参考文献</a></li>
<li class="toctree-l1"><a class="reference internal" href="../audio/index.html">语音技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../audio/feature.html">1. 音频特征</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id2">1.1. 认识声音</a></li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id3">1.2. 认识声波</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id4">1.2.1. 物体的振动以及简谐振动</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id7">1.2.2. 什么是声波</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id8">1.2.3. 纯音和复合音</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#spectrum">1.2.4. 频谱 Spectrum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id10">1.2.5. 名词</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id13">1.3. 语音学</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id14">1.3.1. 发声原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id15">1.3.2. 听觉感应</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id16">1.4. 数字信号处理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id17">1.4.1. 模数转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#wav">1.4.2. 音频文件–WAV</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id18">1.5. 分帧与加窗</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id19">1.5.1. 预加重处理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id20">1.5.2. 分帧与加窗处理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id21">1.6. 声音的感官度量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#sound-pressure-level-spl">1.6.1. 声压与声压级(Sound Pressure Level,SPL)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#intensity-level-il">1.6.2. 声强与声强级(Intensity Level,IL）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id22">1.6.3. 声压与声强的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id23">1.6.4. 响度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id24">1.6.5. 音量计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id25">1.6.6. 频率与音高</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id26">1.7. 时域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id27">1.7.1. 短时能量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id28">1.7.2. 短时平均幅度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id29">1.7.3. 短时过零率</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id31">1.8. 频域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#spectrum-spectrogram">1.8.1. 声谱(spectrum)和时频谱(spectrogram)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#short-time-fourier-transform-stft">1.8.2. 短时傅里叶变换 Short-time Fourier transform (STFT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id33">1.8.3. 倒频谱</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id34">1.8.4. 色谱图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id35">1.9. 小波域特征</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id36">1.9.1. 离散小波域变换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id37">1.9.2. 小波域过零率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id38">1.9.3. 小波域质心</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id39">1.9.4. 小波域子带能量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#mfcc">1.10. 语音识别的音频特征–MFCC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id40">1.11. 参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../edm/index.html">教育领域数据挖掘</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../edm/bkt.html">1. 贝叶斯知识追踪(Bayesian Knowledge Tracing,BKT)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id1">1.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#hidden-markov-model-hmm">1.2. 隐马尔科夫模型(Hidden Markov Model,HMM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#bayesian-knowledge-tracing">1.3. 贝叶斯知识追踪(Bayesian Knowledge Tracing)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#bkt">1.3.1. BKT的参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#item-response-theory-irt">1.4. 项目反映理论(Item Response Theory,IRT)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#bktirt">1.5. BKT结合IRT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id5">1.6. 实验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id6">1.6.1. 数据集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id7">1.6.2. 实验方法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id8">1.6.3. 实验结果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id9">1.6.4. 项目代码</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id10">1.7. 未来工作</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id11">1.7.1. 题目难度的计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#irt">1.7.2. 多参数IRT模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id12">1.7.3. 参数估计算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id13">1.8. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../nlp/index.html">自然语言处理</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html">1. 文本去重</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id2">1.1. 背景</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id3">1.2. 技术思路</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id4">1.3. 相似（距离）算法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#euclidean-distance">1.3.1. 欧氏距离（Euclidean Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minkowski-distance">1.3.2. 闵科夫斯基距离（Minkowski Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#manhattan-distance">1.3.3. 曼哈顿距离（Manhattan Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#chebyshev-distance">1.3.4. 切比雪夫距离（Chebyshev Distance ）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#mahalanobis-distance">1.3.5. 马氏距离(Mahalanobis Distance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#cosine-similarity">1.3.6. 余弦夹角相似度(Cosine Similarity)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#hamming-distance">1.3.7. 汉明距离（Hamming Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#jaccard">1.3.8. Jaccard 系数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id5">1.3.9. 编辑距离</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id6">1.3.10. 最长公共字串</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id7">1.3.11. 最长公共子序列</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id8">1.4. 文本去重</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#kshingle">1.4.1. KShingle算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minhash">1.4.2. Minhash算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#simhash">1.4.3. simhash</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#ksentence">1.4.4. KSentence算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id9">1.5. 话术去重</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../nlp/bert/content.html">2. Attention&amp;Transformer&amp;Bert 简介</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#transformer">2.1. Transformer 从宏观到微观</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#seq2seq">2.1.1. seq2seq</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id1">2.1.2. 模型的输入</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#self-attention">2.2. Self-Attention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id2">2.2.1. 什么是注意力？</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id3">2.2.2. 加权求和</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id4">2.2.3. 位置编码</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#multi-head">2.2.4. 多头注意力（Multi-head）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#attention">2.3. Attention 机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#id5">2.4. 其它参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../rst_tutorial/latex.html">latex demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../rst_tutorial/latex.html#latex">latex</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../rst_tutorial/latex.html#how-to-write-an-m-x-n-matrix-in-latex">How to write an m x n matrix in LaTeX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-big-parentheses">With big parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-parentheses">With parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-brackets">With brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#latex-matrix-with-no-bracket">LateX matrix with no bracket</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-vertical-bar-brackets">With vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-curly-brackets">with curly brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-double-vertical-bar-brackets">with double vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#small-inline-matrix">small inline matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#examples-matrix-2-x-2-in-latex">Examples matrix 2 x 2 in LaTeX</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../rst_tutorial/graphviz.html">graphviz demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../rst_tutorial/graphviz.html#id1">布局</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst_tutorial/graphviz.html#id2">其它</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">读书笔记</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">《统计因果推断推理入门》读书笔记</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html">1. 第三章 干预的效果</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id2">1.1. 第3.1节 干预</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id3">1.2. 第3.2节 校正公式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id8">1.3. 第3.3节 后门准则</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html">《深度学习推荐系统》读书笔记</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id2">重点</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id3">冷启动</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id4">探索与利用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id5">召回层的主要策略</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#embedding">协同过滤 &amp; Embedding 向量</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">张振虎的博客</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">AI内容生成（ai-gc）</a> &raquo;</li>
        
      <li><span class="section-number">1. </span>变分自编码器（Variational Autoencoder）</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/aigc/变分自编码器.rst.txt" rel="nofollow"> 查看页面源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="variational-autoencoder">
<span id="ch-vae"></span><h1><span class="section-number">1. </span>变分自编码器（Variational Autoencoder）<a class="headerlink" href="#variational-autoencoder" title="此标题的永久链接">¶</a></h1>
<p>早在2013年，Kingma和Welling就推出了变分自动编码器（VAE），
简而言之，VAE的想法是训练具有正则化潜在空间的自动编码器。然后，正则化编码器被迫将数据编码为接近高斯的分布，而解码器则从潜在空间重建数据</p>
<p>在传统的自编码模型中，编码器输出的 <cite>code</cite> 仅仅是一个 <strong>数值</strong> 张量而已，
然而在变分自编码（Variational Autoencoders,VAE）的算法中，把这个 <cite>code</cite> 看做是由 <strong>随机变量</strong> 组成的张量。
变分自编码器早在2013 <a class="footnote-reference brackets" href="#footcite-kingma2013autoencoding" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> 年就提出了
，然而前几年并没有引起太大的关注，随着 OpenAI 的 DALL-E 的爆火，扩散模型（diffusion models）进入了大家的视野，
才使得 VAE 算法得到了关注，这是因为扩散模型是由于 VAE 发展而来的。在详细介绍扩散模型之前，我们先讨论下 VAE 算法。</p>
<figure class="align-center" id="id6">
<span id="fg-mixture-vae-001"></span><div class="graphviz"><object data="../_images/graphviz-2213136460ec4aa0cec348bf744b3d3fb7259580.svg" type="image/svg+xml" class="graphviz">
<p class="warning">digraph 编解码 {
node[shape=circle,fixedsize=false ];
rankdir = LR

input[label=&quot;input&quot; shape=&quot;plaintext&quot;]
output[label=&quot;output&quot; shape=&quot;plaintext&quot;]
code[label=&quot;code&quot; shape=&quot;rectangle&quot; style=filled fillcolor = &quot;#FEF9E7&quot; height=1 with=0.2 fixedsize=true]

encoder[label=&quot;encoder&quot; shape=&quot;rectangle&quot; style=filled fillcolor = &quot;#D6EAF8&quot;]
decoder[label=&quot;decoder&quot; shape=&quot;rectangle&quot; style=filled fillcolor = &quot;#D5F5E3&quot;];

input -&gt; encoder -&gt; code -&gt; decoder -&gt; output
}</p></object></div>
<figcaption>
<p><span class="caption-number">图 1.1 </span><span class="caption-text">编解码器</span><a class="headerlink" href="#id6" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>我们用符号 <span class="math notranslate nohighlight">\(Z\)</span> 表示编码器输出的 <cite>code</cite> ，它是一个由随机变量组成的张量。由于模型的输出是要还原输入，
因此输入输出可以看做是同一个变量，记作 <span class="math notranslate nohighlight">\(X\)</span> ，它同样是由多个随机变量组成的张量。
<span class="math notranslate nohighlight">\(X\)</span> 和 <span class="math notranslate nohighlight">\(Z\)</span> 的张量尺寸（大小）并不要求必须是一样的，事实上通常在编解码的架构中，
<span class="math notranslate nohighlight">\(Z\)</span> 的尺寸是远远小于 <span class="math notranslate nohighlight">\(X\)</span> 的，这样相当于有一个信息压缩的过程。
在本文的讨论中，符号简洁些和便于理解，暂时忽略张量的问题。</p>
<figure class="align-center" id="id7">
<span id="fg-mixture-2-00"></span><div class="graphviz"><object data="../_images/graphviz-238eee98877fcb7c992140a8ae99b75c0585415c.svg" type="image/svg+xml" class="graphviz">
<p class="warning">digraph VAE有向图 {
node[shape=circle,fixedsize=true,width=0.5];
rankdir = LR

X[label=X, style=filled];

X -&gt; Z[label=&quot;P(Z|X)&quot;, style=filled]
Z -&gt; X[label=&quot;P(X|Z)&quot;, style=filled]
}</p></object></div>
<figcaption>
<p><span class="caption-number">图 1.2 </span><span class="caption-text"><span class="math notranslate nohighlight">\(X\)</span> 和 <span class="math notranslate nohighlight">\(Z\)</span> 的图表示</span><a class="headerlink" href="#id7" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>从 <span class="math notranslate nohighlight">\(X\)</span> 到 <span class="math notranslate nohighlight">\(Z\)</span> 的过程，相当于条件概率 <span class="math notranslate nohighlight">\(P(Z|X)\)</span> ，
对应编码过程；反过来，从 <span class="math notranslate nohighlight">\(Z\)</span> 到 <span class="math notranslate nohighlight">\(X\)</span> 的过程，相当于条件概率 <span class="math notranslate nohighlight">\(P(X|Z)\)</span>，
对应解码过程。这个模型中包含两个变量 <span class="math notranslate nohighlight">\(X\)</span> 和 <span class="math notranslate nohighlight">\(Z\)</span>，
二者的联合概率记作 <span class="math notranslate nohighlight">\(P(Z,X)\)</span>。
我们期望得到一个生成模型，可以生成（或者是采样） <span class="math notranslate nohighlight">\(X\)</span> 的新样本，
这就需要得到这个联合概率 <span class="math notranslate nohighlight">\(P(Z,X)\)</span> 的完整形式才行。
根据链式法则，可以对联合概率 <span class="math notranslate nohighlight">\(P(Z,X)\)</span> 进行分解。</p>
<div class="math notranslate nohighlight" id="equation-aigc-0">
<span class="eqno">(1.1)<a class="headerlink" href="#equation-aigc-0" title="此公式的永久链接">¶</a></span>\[P(X,Z) = P(X)P(Z|X) = P(Z)P(X|Z)\]</div>
<p>通常我们能得到 <span class="math notranslate nohighlight">\(X\)</span> 的一些观测样本，比如一些图片样本，然而变量 <span class="math notranslate nohighlight">\(Z\)</span> 是没有真实的观测样本的，
因此称 <span class="math notranslate nohighlight">\(X\)</span> 为可观测变量（Observed variable），<span class="math notranslate nohighlight">\(Z\)</span> 为不可观测变量（Unobserved variable）或者隐变量（Latent variable）。
根据贝叶斯定理（公式）有</p>
<div class="math notranslate nohighlight" id="equation-aigc-1">
<span class="eqno">(1.2)<a class="headerlink" href="#equation-aigc-1" title="此公式的永久链接">¶</a></span>\[P(Z|X) = \frac{P(Z)P(X|Z)}{P(X)}\]</div>
<p>通常称 <span class="math notranslate nohighlight">\(P(Z|X)\)</span> 为变量 <span class="math notranslate nohighlight">\(Z\)</span> 的后验概率分布，<span class="math notranslate nohighlight">\(P(Z)\)</span> 为变量 <span class="math notranslate nohighlight">\(Z\)</span> 的先验概率分布，
<span class="math notranslate nohighlight">\(P(X|Z)\)</span> 代观测变量 <span class="math notranslate nohighlight">\(X\)</span> 的似然（生成概率），<span class="math notranslate nohighlight">\(P(X)\)</span> 为证据（evidence）。
当观测样本 <span class="math notranslate nohighlight">\(X=x\)</span> 确定的情况下，<span class="math notranslate nohighlight">\(p(x)\)</span> 实际上是常量值，它其实相当于对分子的归一化，这点很重要。
<span class="math notranslate nohighlight">\(p(x)\)</span> 可以通过对分子进行积分得到，相当于边际化（消除）变量 <span class="math notranslate nohighlight">\(Z\)</span>。</p>
<div class="math notranslate nohighlight" id="equation-aigc-2">
<span class="eqno">(1.3)<a class="headerlink" href="#equation-aigc-2" title="此公式的永久链接">¶</a></span>\[p(x) = \int_z p(x,z) dz\]</div>
<p>在这个模型中包含 <span class="math notranslate nohighlight">\(Z\)</span> 和 <span class="math notranslate nohighlight">\(X\)</span> 两个随机变量，完整的模型表示是二者的联合概率分布 <span class="math notranslate nohighlight">\(P(X,Y)\)</span>
，假设模型中的未知参数用 <span class="math notranslate nohighlight">\(\theta\)</span> 表示，模型的参数化表示可以记为 <span class="math notranslate nohighlight">\(P(X,Y;\theta)\)</span>。
根据最大似然理论，如果要求解模型的参数，可以通过极大化观测样本的发生概率实现，我们符号 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>
表示观测样本集合，如果变量  <span class="math notranslate nohighlight">\(Z\)</span> 和 <span class="math notranslate nohighlight">\(X\)</span>
都能观测到，即 <span class="math notranslate nohighlight">\(\mathcal{D}=\{(z^{(1)},x^{(1)}),\cdots,(z^{(N)},x^{(N)})\}\)</span>，
那么极大化的目标函数（对数似然函数）为</p>
<div class="math notranslate nohighlight" id="equation-aigc-3">
<span class="eqno">(1.4)<a class="headerlink" href="#equation-aigc-3" title="此公式的永久链接">¶</a></span>\[\ell(\theta;\mathcal{D}) = \sum_{i=1}^{N} \ln p_{\theta}(z^{(i)},x^{(i)})\]</div>
<p>然而在这里，我们并没有变量 <span class="math notranslate nohighlight">\(Z\)</span> 的观测样本，它是 <strong>隐变量</strong>，此时观测样本集为 <span class="math notranslate nohighlight">\(\mathcal{D}=\{x^{(1)},\cdots,x^{(N)}\}\)</span>，
极大化的目标函数（对数似然函数）变成</p>
<div class="math notranslate nohighlight" id="equation-eq-vae-004">
<span class="eqno">(1.5)<a class="headerlink" href="#equation-eq-vae-004" title="此公式的永久链接">¶</a></span>\[\ell(\theta;\mathcal{D}) = \sum_{i=1}^{N} \ln p_{\theta}(x^{(i)})
= \sum_{i=1}^{N} \ln  \int_z p_{\theta}(x^{(i)},z) dz\]</div>
<p>这时我们发现，在对数中存在积分操作，这个积分导致无法把对数进行展开，同时这个积分也是难以计算的，
最终导致我们无法通过极大化对数似然进行参数求解。问题的关键就是含有隐变量模型的如何进行参数求解，</p>
<section id="evidence-lower-bound-elbo">
<h2><span class="section-number">1.1. </span>证据下界(Evidence Lower Bound,ELBO)<a class="headerlink" href="#evidence-lower-bound-elbo" title="此标题的永久链接">¶</a></h2>
<p>直接极大化 <a class="reference internal" href="#equation-eq-vae-004">公式(1.5)</a> 进行参数求解是困难的，但这并没有难倒聪明的数学家们，
既然无法直接极大化 <a class="reference internal" href="#equation-eq-vae-004">公式(1.5)</a> ，那是不是可以找到一个替代品呢，或者说它的一个近似表达呢。
这个近似表达拥有更简单的形式，并且结果和直接极大化 <a class="reference internal" href="#equation-eq-vae-004">公式(1.5)</a> 是等价的。
还真的找到了一个这样的函数，它是 <a class="reference internal" href="#equation-eq-vae-004">公式(1.5)</a> 的一个下界函数，即这个相似函数永远是 <strong>小于等于</strong> <a class="reference internal" href="#equation-eq-vae-004">公式(1.5)</a>
的。因为 <a class="reference internal" href="#equation-eq-vae-004">公式(1.5)</a> 中的 <span class="math notranslate nohighlight">\(p(x)\)</span> 是证据（evidence），所以这个下界函数被称为证据下界函数(Evidence Lower Bound,ELBO)。</p>
<p>接下我们看一下这个下界函数是如何推导出来的，首先定义一个变量 <span class="math notranslate nohighlight">\(Z\)</span> 的概率密度函数，记作 <span class="math notranslate nohighlight">\(q_{\phi}(z)\)</span>
，<span class="math notranslate nohighlight">\({\phi}\)</span> 是它的未知参数。同时为了公式简洁，我们暂时忽略 <a class="reference internal" href="#equation-eq-vae-004">公式(1.5)</a> 中的对样本的求和操作 <span class="math notranslate nohighlight">\(\sum_{i=1}^{N}\)</span>
，有没有它不影响过程和结论。整个推导过程并不复杂，其实就是利用 <cite>Jensen</cite> 不等式得到 <a class="reference internal" href="#equation-eq-vae-004">公式(1.5)</a> 的一个下界函数。
过程如下：</p>
<div class="math notranslate nohighlight" id="equation-eq-vae-005">
<span class="eqno">(1.1.15)<a class="headerlink" href="#equation-eq-vae-005" title="此公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\ell(\theta;x) &amp;=  \ln  p_{\theta}(x)\\ &amp;=  \ln \int_{z} p_{\theta}(x,z)\\&amp;=  \ln \int_{z} q_{\phi}(z) \frac{p_{\theta}(x,z)}{q_{\phi}(z)}  \ \ \ \text{同时乘除} q_{\phi}(z) \text{，等于没变化}\\&amp;=  \ln\mathbb{E}_{q_{\phi}(z) } \left [ \frac{p_{\theta}(x,z)}{q_{\phi}(z)} \right ]\\&amp; \ge  \mathbb{E}_{q_{\phi}(z) } \ln\left [ \frac{p_{\theta}(x,z)}{q_{\phi}(z)} \right ]
\ \ \ \text{根据Jensen不等式}\\
&amp;=  \int_{z} q_{\phi}(z) \ln \left [ \frac{p_{\theta}(x,z)}{q_{\phi}(z)} \right ]\\&amp;=  \left [  \int_{z} q_{\phi}(z) \ln  p_{\theta}(x,z)
-   \int_{z} q_{\phi}(z) \ln q_{\phi}(z) \right ]\\&amp;\triangleq \mathcal{L}(q,\theta)\end{aligned}\end{align} \]</div>
<p>利用 <code class="docutils literal notranslate"><span class="pre">Jensen</span></code> 不等式的性质，可以为对数似然函数 <span class="math notranslate nohighlight">\(\ell(\theta;x)\)</span> 找到一个下界函数
，把这个下界函数记作 <span class="math notranslate nohighlight">\(\mathcal{L}(q,\theta)\)</span>。</p>
<div class="math notranslate nohighlight" id="equation-eq-vae-006">
<span class="eqno">(1.1.16)<a class="headerlink" href="#equation-eq-vae-006" title="此公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\mathcal{L}(q,\theta) &amp;= \int_{z} q_{\phi}(z) \ln  p_{\theta}(x,z)
-   \int_{z} q_{\phi}(z) \ln q_{\phi}(z)\\&amp;=   \mathbb{E}_{z \sim q_{\phi}} [ \ln  p_{\theta}(x,z) ]
- \mathbb{E}_{z \sim q_{\phi}}[ \ln q_{\phi}(z) ]\end{aligned}\end{align} \]</div>
<p>下界函数中含有联合概率 <span class="math notranslate nohighlight">\(p(x,z)\)</span>
，根据链式法则，有两种分解方式。</p>
<div class="math notranslate nohighlight" id="equation-aigc-4">
<span class="eqno">(1.1.17)<a class="headerlink" href="#equation-aigc-4" title="此公式的永久链接">¶</a></span>\[p(x,z) = p(z) p(x|z)=p(x)p(z|x)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(p(z)\)</span> 表示 <span class="math notranslate nohighlight">\(z\)</span> 的先验概率，<span class="math notranslate nohighlight">\(p(z|x)\)</span> 表示其后验概率。
接下来，分别用分解两种方式变换下界函数（<a class="reference internal" href="#equation-eq-vae-006">公式(1.1.16)</a>）。</p>
<p><strong>第一种形式</strong>，首先使用 <span class="math notranslate nohighlight">\(z\)</span> 的后验 <span class="math notranslate nohighlight">\(p(z|x)\)</span> 分解 <span class="math notranslate nohighlight">\(p(x,z)\)</span> :</p>
<div class="math notranslate nohighlight" id="equation-eq-vae-0081">
<span class="eqno">(1.1.18)<a class="headerlink" href="#equation-eq-vae-0081" title="此公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}    \mathcal{L}(q,\theta) &amp;= \mathbb{E}_{z \sim q_{\phi} } [ \ln  p_{\theta}(x,z) ]
    - \mathbb{E}_{z \sim q_{\phi} }[ \ln q_{\phi}(z)]\\    &amp;= \mathbb{E}_{z \sim q_{\phi}} \left [ \ln  p_{\theta}(x) + \ln  p_{\theta}(z|x) \right ]
        - \mathbb{E}_{z \sim q_{\phi}}[ \ln q_{\phi}(z)]\\    &amp;= \underbrace{\mathbb{E}_{z \sim q_{\phi}} [ \ln  p_{\theta}(x) ]}_{\text{与}z\text{无关，期望符号可以去掉}}
        + \mathbb{E}_{z \sim q_{\phi}} [\ln  p_{\theta}(z|x) ]
        - \mathbb{E}_{z \sim q_{\phi}}[ \ln q_{\phi}(z)]\\    &amp;= \underbrace{\ln  p_{\theta}(x)}_{\text{观测数据对数似然/证据}}
        +
        \underbrace{
        \mathbb{E}_{z \sim q_{\phi}} [ \ln  p_{\theta}(z|x) ]
        - \mathbb{E}_{z \sim q_{\phi}}[ \ln q_{\phi}(z)] }_{\text{KL散度}}\\    &amp;= \underbrace{ \ell(\theta;x) }_{\text{观测数据对数似然/证据}}
        - \underbrace{ KL( q_{\phi}(z) || p_{\theta}(z|x) ) }_{ q_{\phi}(z) \text{和后验验} P(Z|X)  \text{的KL散度}}\end{aligned}\end{align} \]</div>
<p>对 <a class="reference internal" href="#equation-eq-vae-0081">公式(1.1.18)</a> 移项整理，可以得出</p>
<div class="math notranslate nohighlight" id="equation-eq-vae-009">
<span class="eqno">(1.1.19)<a class="headerlink" href="#equation-eq-vae-009" title="此公式的永久链接">¶</a></span>\[\underbrace{ \ell(\theta;x)  }_{\text{观测数据对数似然/证据}}
= \underbrace{ \mathcal{L}(q,\theta) }_{\text{证据下界函数 ELBO}}
+  \underbrace{  KL(q_{\phi}(z)|| p_{\theta}( z |x ) }_{ \text{KL散度}}\]</div>
<p>可以看出观测数据的对数似然 <span class="math notranslate nohighlight">\(\ell(\theta;x)\)</span> 就等于下界函数 <span class="math notranslate nohighlight">\(\mathcal{L}(q,\theta)\)</span>
加上 <span class="math notranslate nohighlight">\(q_{\phi}(z)\)</span> 与后验 <span class="math notranslate nohighlight">\(p_{\theta}(z|x)\)</span> 的KL散度，
我们知道 KL 散度是衡量两个概率分布是否相近的，如果两个概率分布完全一样，则 KL 散度的值为 <span class="math notranslate nohighlight">\(0\)</span>。
也就是说如果令 <span class="math notranslate nohighlight">\(q_{\phi}(z)= p_{\theta}(z|x)\)</span> 成立，则 <span class="math notranslate nohighlight">\(KL(q_{\phi}(z)|| p_{\theta}(z|x))=0\)</span>
，此时就有 <span class="math notranslate nohighlight">\(\ell(\theta;x) =  \mathcal{L}(q,\theta)\)</span> 成立。</p>
<div class="admonition important">
<p class="admonition-title">重要</p>
<p><strong>关键结论</strong></p>
<p>至此，我们已经证明了下界函数存在，并且只要令 <span class="math notranslate nohighlight">\(q_{\phi}(z)= p_{\theta}(z|x)\)</span>，
下界函数就等于对数似然函数，此时极大化下界函数 <span class="math notranslate nohighlight">\(\mathcal{L}(q,\theta)\)</span>
就等同于极大化观测数据的对数似然函数 <span class="math notranslate nohighlight">\(\ell(\theta;x)\)</span>。</p>
</div>
<p>接下来，我们使用 <span class="math notranslate nohighlight">\(z\)</span> 的先验 <span class="math notranslate nohighlight">\(p(z)\)</span> 对 <span class="math notranslate nohighlight">\(p(x,z)\)</span> 进行分解，重新改写 <a class="reference internal" href="#equation-eq-vae-006">公式(1.1.16)</a>
表示的下界函数，即下界函数 <span class="math notranslate nohighlight">\(\mathcal{L}(q,\theta)\)</span> 的 <strong>第二种形式</strong>
。</p>
<div class="math notranslate nohighlight" id="equation-eq-vae-007">
<span class="eqno">(1.1.20)<a class="headerlink" href="#equation-eq-vae-007" title="此公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}    \mathcal{L}(q,\theta) &amp;= \mathbb{E}_{z \sim q_{\phi}} [ \ln  p_{\theta}(x,z) ]
    - \mathbb{E}_{z \sim q_{\phi}}[ \ln q_{\phi}(z))]\\    &amp;= \mathbb{E}_{z \sim q_{\phi}} [ \ln  p(z) + \ln  p_{\theta}(x|z) ]
        - \mathbb{E}_{z \sim q_{\phi}}[ \ln q_{\phi}(z))]\\    &amp;= \mathbb{E}_{z \sim q_{\phi}} [ \ln  p(z) ]
        + \mathbb{E}_{z \sim q_{\phi}} [\ln  p_{\theta}(x|z) ]
        - \mathbb{E}_{z \sim q_{\phi}}[ \ln q_{\phi}(z))]\\    &amp;= \mathbb{E}_{z \sim q_{\phi}} [\ln  p_{\theta}(x|z) ] +
        \underbrace{
        \mathbb{E}_{z \sim q_{\phi}} [ \ln  p(z) ]
        - \mathbb{E}_{z \sim q_{\phi}}[ \ln q_{\phi}(z))] }_{\text{KL散度}}\\    &amp;=   \mathbb{E}_{z \sim q_{\phi}} [\ln  p_{\theta}(x|z) ]
        - \underbrace{ KL(q_{\phi}(z)||p(z))}_{q_{\phi}(z) \text{和先验} p(z)  \text{的KL散度}}\end{aligned}\end{align} \]</div>
<p>根据前文的结论，当 <span class="math notranslate nohighlight">\(q_{\phi}(z)\)</span> 等于 <span class="math notranslate nohighlight">\(z\)</span> 的后验 <span class="math notranslate nohighlight">\(p_{\theta}(z|x)\)</span>
时，下界函数 <span class="math notranslate nohighlight">\(\mathcal{L}(q,\theta)\)</span> 和观测数据的对数似然函数 <span class="math notranslate nohighlight">\(\ell(\theta;x)\)</span>
是相等的。因此，我们令 <span class="math notranslate nohighlight">\(q_{\phi}(z)=p_{\theta}(z|x)\)</span>，
为了符号区分这里记作 <span class="math notranslate nohighlight">\(q_{\phi}(z)=q_{\phi}(z|x)\)</span>
，并代入到 <a class="reference internal" href="#equation-eq-vae-007">公式(1.1.20)</a> 中，可得</p>
<div class="math notranslate nohighlight" id="equation-eq-vae-0082">
<span class="eqno">(1.1.21)<a class="headerlink" href="#equation-eq-vae-0082" title="此公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}  \mathcal{L}(q,\theta) &amp;=
    \underbrace{ \mathbb{E}_{z \sim q_{\phi}(z|x) } [\ln  p_{\theta}(x|z) ] }_{\text{①重建项（reconstruction term）}}
        - \underbrace{ KL( q_{\phi}(z|x) ||p(z))}_{\text{②先验匹配项（prior matching term）}}\\  &amp;=  \ell(\theta;x)\end{aligned}\end{align} \]</div>
<p><a class="reference internal" href="#equation-eq-vae-0082">公式(1.1.21)</a> 包含两部分，这里详细分析讨论一下这两项。</p>
<ol class="arabic simple">
<li><p>首先看第①项，其中条件概率 <span class="math notranslate nohighlight">\(p_{\theta}(x|z)\)</span> ，代表着从隐变量 <span class="math notranslate nohighlight">\(z\)</span> 到观测变量 <span class="math notranslate nohighlight">\(x\)</span> 的转换，
其中作用就是从 <span class="math notranslate nohighlight">\(z\)</span> 重建 <span class="math notranslate nohighlight">\(x\)</span> ，因此可以称为 <strong>重建项（reconstruction term）</strong> ，相当于解码器（decoder）。
同时由于隐变量 <span class="math notranslate nohighlight">\(Z\)</span> 是一个随机变量，没有具体数值，因此需要对 <span class="math notranslate nohighlight">\(Z\)</span> 求期望（或者说积分），并且求期望时采用的是 <span class="math notranslate nohighlight">\(Z\)</span>
的后验概率密度 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 。而后验 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 表示从观测变量 <span class="math notranslate nohighlight">\(x\)</span> 到隐变量 <span class="math notranslate nohighlight">\(z\)</span> 的转换过程，
其过程相当于编码器。</p></li>
<li><p>接下来看第②项，这一项是 <span class="math notranslate nohighlight">\(z\)</span> 的后验 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 和先验 <span class="math notranslate nohighlight">\(p(z)\)</span> 的KL散度，取值为非负，最小值为0。
因为我们是要极大化整个式子，而这一项前面有个负号，这一项的值又是非负的，所以相当于要极小化这一项，令它趋近于0，
也就是使得后验 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 和先验 <span class="math notranslate nohighlight">\(p(z)\)</span> 尽量接近（或者说匹配），
所以可以称这一项为 <strong>先验匹配项（prior matching term）</strong>，它的作用其实就相当于一个约束或者正则项。</p></li>
</ol>
<p>至此，我们找到了原来含有隐变量的对数似然函数 <span class="math notranslate nohighlight">\(\ell(\theta;x)\)</span> 的一个下界函数（ELBO） <span class="math notranslate nohighlight">\(\mathcal{L}(q,\theta)\)</span>
，极大化这个下界函数和极大化对数似然函数求解参数是等价的。并且这个下界函数中包 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 和 <span class="math notranslate nohighlight">\(p_{\theta}(x|z)\)</span>
，这两项正好分别对应编码器和解码器，因此本算法可以看做是自编码器的一种变体。需要注意的是，
这里 <span class="math notranslate nohighlight">\(Z\)</span> 被解释成随机变量，而不是数值变量。
然而问题到这里并没有结束，<span class="math notranslate nohighlight">\(p_{\theta}(x|z)\)</span> 和 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 的具体形式还不知道，
接下来介绍这两项具体是什么样的。</p>
</section>
<section id="id2">
<h2><span class="section-number">1.2. </span>编码-解码<a class="headerlink" href="#id2" title="此标题的永久链接">¶</a></h2>
<figure class="align-default" id="fg-vae-008">
<img alt="https://data-science-blog.com/wp-content/uploads/2022/04/variational-auto-encoder-image-encoding-decoding.png" src="https://data-science-blog.com/wp-content/uploads/2022/04/variational-auto-encoder-image-encoding-decoding.png" />
</figure>
<p>已知  <span class="math notranslate nohighlight">\(Z\)</span> 是一个随机变量，在 VAE 算法中假设 <span class="math notranslate nohighlight">\(Z\)</span> 是服从高斯分布的，它的先验分布 <span class="math notranslate nohighlight">\(P(Z)\)</span> 是均值为 <span class="math notranslate nohighlight">\(0\)</span>
方差为 <span class="math notranslate nohighlight">\(1\)</span> 的标准正态分布，由于这里 <span class="math notranslate nohighlight">\(Z\)</span> 是一个张量（向量），所以用一个多维的高斯分布表示，
记作 <span class="math notranslate nohighlight">\(P(Z) \sim \mathcal{N}(0,\textit{I})\)</span> 。</p>
<p>这里我们回顾一下多维高斯分布的概率密度函数，接下来会多次用到，</p>
<div class="math notranslate nohighlight" id="equation-eq-vae-010">
<span class="eqno">(1.2.10)<a class="headerlink" href="#equation-eq-vae-010" title="此公式的永久链接">¶</a></span>\[p(x)  = \frac{1}{(2\pi)^{n/2} |\Sigma |^{1/2}}exp\{-\frac{1}{2}(x - \mu)^{T}\Sigma^{-1}(x - \mu)\}\]</div>
<p><a class="reference internal" href="#equation-eq-vae-010">公式(1.2.10)</a> 是多维高斯分布概率密度函数的标准形式，如果是单位协方差矩阵 <span class="math notranslate nohighlight">\(\textit{I}\)</span>
，把式中 <span class="math notranslate nohighlight">\(\Sigma\)</span> 替换成 <span class="math notranslate nohighlight">\(\textit{I}\)</span> 即可。</p>
<p><strong>后验分布-编码器</strong></p>
<p>即然变量 <span class="math notranslate nohighlight">\(Z\)</span> 是一个高斯变量，自然其后验分布 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> <strong>也是一个高斯分布</strong>
，只是我们并不知道后验分布的均值和方差，这里我们分别用 <span class="math notranslate nohighlight">\(\mu_{z}\)</span> 和 <span class="math notranslate nohighlight">\(\Sigma_{z}\)</span>
表示后验分布的均值参数和方差参数，
此时有 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)=\mathcal{N}(\mu_{z},\Sigma_{z})\)</span>，
由于模型假设 <span class="math notranslate nohighlight">\(Z\)</span> 的各维度之间是相互独立的，
因此其协方差 <span class="math notranslate nohighlight">\(\Sigma_z\)</span> 是一个对角矩阵，非对角线位置元素值为 <span class="math notranslate nohighlight">\(0\)</span>
，对角线元素是未知参数。</p>
<p>在传统的自编码（AE）模型中，<span class="math notranslate nohighlight">\(Z\)</span> 是一个数值变量，输入变量 <span class="math notranslate nohighlight">\(X\)</span>
经过编码器（encoder）直接输出 <span class="math notranslate nohighlight">\(Z\)</span> 的值。
然而在 VAE 中，<span class="math notranslate nohighlight">\(Z\)</span> 是一个 <strong>随机变量</strong>，不能从 <span class="math notranslate nohighlight">\(X\)</span> 直接映射到  <span class="math notranslate nohighlight">\(Z\)</span>
。后验（编码器） <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 是一个条件概率，它的概率密度函数是和 <span class="math notranslate nohighlight">\(x\)</span> 相关的，
，输入变量 <span class="math notranslate nohighlight">\(X\)</span> 是通过影响 <span class="math notranslate nohighlight">\(Z\)</span> 的均值参数和方差参数间接影响到 <span class="math notranslate nohighlight">\(Z\)</span>
。也就是说均值参数 <span class="math notranslate nohighlight">\(\mu_{z}\)</span> 和方差参数 <span class="math notranslate nohighlight">\(\Sigma_{z}\)</span> 是和 <span class="math notranslate nohighlight">\(x\)</span> 相关的。
这里分别用函数表述它们之间的关系。</p>
<div class="math notranslate nohighlight" id="equation-aigc-5">
<span class="eqno">(1.2.11)<a class="headerlink" href="#equation-aigc-5" title="此公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\mu_{z} &amp;= \mu_{\phi}(x) = encoder_{\phi}(x)\\\Sigma_{z} &amp;= \Sigma_{\phi}(x) =  encoder_{\phi}(x)\end{aligned}\end{align} \]</div>
<p>函数 <span class="math notranslate nohighlight">\(\mu_{\phi}(x)\)</span> 和 <span class="math notranslate nohighlight">\(\Sigma_{\phi}(x)\)</span> 分别是 <span class="math notranslate nohighlight">\(x\)</span> 到 <span class="math notranslate nohighlight">\(\mu_{z}\)</span> 和 <span class="math notranslate nohighlight">\(\Sigma_{z}\)</span>
的映射，理论上这里可以选择任意合适的映射函数。
当然，在VAE中映射函数是由神经网络（Neural Network）实现的，由神经网络拟合并输出 <span class="math notranslate nohighlight">\(\mu_{z}\)</span> 和 <span class="math notranslate nohighlight">\(\Sigma_{z}\)</span>
。encoder 输出两个只，分别对应 <span class="math notranslate nohighlight">\(\mu_{z}\)</span> 和 <span class="math notranslate nohighlight">\(\Sigma_{z}\)</span>
，参数 <span class="math notranslate nohighlight">\(\phi\)</span> 就是 encoder 网络的参数。</p>
<div class="admonition tip">
<p class="admonition-title">小技巧</p>
<p>如果你熟悉广义线性模型（Generalized linear models,GLM）就会发现，
这里 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 中的 <span class="math notranslate nohighlight">\(x\)</span> 到 <span class="math notranslate nohighlight">\(\mu_z\)</span> 的映射过程和 GLM 中的连接函数的反函数（响应函数）本质上是相同的，
区别在于 GLM 中的响应函数是线性函数，
相当于单层的（感知器）网络。而VAE中，采用的是更复杂（任意结构的）多层神经网络，更一般化。</p>
</div>
<p><strong>KL散度-正则项</strong></p>
<p>接下来看下 <a class="reference internal" href="#equation-eq-vae-0082">公式(1.1.21)</a> 中完整的第②项，即 KL散度部分 <span class="math notranslate nohighlight">\(KL( q_{\phi}(z|x) || p(z) )\)</span>
，KL散度中的 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)=\mathcal{N}(\mu_z,\Sigma_z)\)</span>
和 <span class="math notranslate nohighlight">\(p(z)=  \mathcal{N}(0,\textit{I})\)</span> 都是高斯分布，
而两个高斯分布的 KL 散度是可以直接计算得到的。</p>
<div class="math notranslate nohighlight" id="equation-eq-vae-012">
<span class="eqno">(1.2.12)<a class="headerlink" href="#equation-eq-vae-012" title="此公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}KL( q_{\phi}(z|x) || p(z)  ) &amp;= KL( \mathcal{N}(\mu_z,\Sigma_z) || \mathcal{N}(0,\textit{I}))\\&amp;= \frac{1}{2} \left ( tr ( \Sigma_z) + \mu_z^T \mu_z − k − \log det(\Sigma_z) \right )\end{aligned}\end{align} \]</div>
<p><a class="reference internal" href="#equation-eq-vae-012">公式(1.2.12)</a> 就是模型训练过程目标函数的一项
，其中 <span class="math notranslate nohighlight">\(k\)</span> 是向量的维度，它的作用 <strong>相当于一个正则项</strong> ，使得后验 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 尽量接近 <span class="math notranslate nohighlight">\(z\)</span> 先验。
KL 散度这一部分就算是解决了，它可以通过 <a class="reference internal" href="#equation-eq-vae-012">公式(1.2.12)</a> 计算，其中 <span class="math notranslate nohighlight">\(\mu_z\)</span>
和 <span class="math notranslate nohighlight">\(\Sigma_z\)</span> 都是由编码器网络输出。</p>
<p><strong>生成分布-解码器</strong></p>
<p>现在看下 <a class="reference internal" href="#equation-eq-vae-0082">公式(1.1.21)</a> 中第①部分，它的作用对应着解码器（decoder）部分，
从变量 <span class="math notranslate nohighlight">\(Z\)</span> 重建回 <span class="math notranslate nohighlight">\(X\)</span>。
先单独看条件概率分布 <span class="math notranslate nohighlight">\(p_{\theta}(x|z)\)</span>，
如果把 <span class="math notranslate nohighlight">\(Z\)</span> 看做输入变量，把 <span class="math notranslate nohighlight">\(X\)</span> 看做输出变量，
就是一个从 <span class="math notranslate nohighlight">\(z\)</span> 到 <span class="math notranslate nohighlight">\(x\)</span> 的过程，
可以把它看做是一个 <strong>回归模型</strong> 。
<span class="math notranslate nohighlight">\(X\)</span> 可以是任意分布的变量，比如高斯分布（回归、最小二乘）、
伯努利分布（二分类）、类别分布（多分类）等等，
如果你熟悉广义线性模型，会更容易理解和扩展。</p>
<p>和解码器部分同理，由于 <span class="math notranslate nohighlight">\(X\)</span> 是一个随机变量，<span class="math notranslate nohighlight">\(z\)</span> 到 <span class="math notranslate nohighlight">\(x\)</span> 的映射是通过
为 <span class="math notranslate nohighlight">\(z\)</span> 和 <span class="math notranslate nohighlight">\(X\)</span> 的均值参数 <span class="math notranslate nohighlight">\(\mu_x\)</span> 建立映射函数实现的。
这里并没有建立从 <span class="math notranslate nohighlight">\(z\)</span> 到 <span class="math notranslate nohighlight">\(X\)</span> 方差参数 <span class="math notranslate nohighlight">\(\Sigma_x\)</span> 之间的映射，
这是因为模型为了简单，假设 <span class="math notranslate nohighlight">\(X\)</span> 的方差为常量，即单位方差 <span class="math notranslate nohighlight">\(\textit{I}\)</span>
，事实上，在大部分回归模型中都是这样假设的。</p>
<div class="math notranslate nohighlight" id="equation-aigc-6">
<span class="eqno">(1.2.13)<a class="headerlink" href="#equation-aigc-6" title="此公式的永久链接">¶</a></span>\[\mu_x = \mu_{\theta}(z) = decoder_{\theta}(z)\]</div>
<p>这里 <span class="math notranslate nohighlight">\(\mu_x\)</span> 是 <span class="math notranslate nohighlight">\(X\)</span> 的期望参数，
<span class="math notranslate nohighlight">\(\mu_{\theta}(z)\)</span> 是一个从 <span class="math notranslate nohighlight">\(z\)</span> 到 <span class="math notranslate nohighlight">\(\mu_x\)</span> 的映射函数。
在VAE中，这里同样是一个神经网络（Neural Network）实现，
如果只有一层感知机网络，就退化成了（广义）线性模型。</p>
<div class="admonition tip">
<p class="admonition-title">小技巧</p>
<p>同理，这里的映射函数 <span class="math notranslate nohighlight">\(\mu_{\theta}(z)\)</span> 就相当于广义线性模型中的连接函数。</p>
<ul class="simple">
<li><p>当 <span class="math notranslate nohighlight">\(\mu_{\theta}(z)\)</span> 是一个线性函数 <span class="math notranslate nohighlight">\(\mu_{\theta}(z)=\theta z +b\)</span> 时，就相当于传统线性回归模型（最小二乘）。</p></li>
<li><p>当 <span class="math notranslate nohighlight">\(\mu_{\theta}(z)\)</span> 是一个线性+sigmoid函数 <span class="math notranslate nohighlight">\(\mu_{\theta}(z)=sigmoid(\theta z +b)\)</span> 时，就相当于逻辑回归。</p></li>
<li><p>当 <span class="math notranslate nohighlight">\(\mu_{\theta}(z)\)</span> 是一个线性+softmax函数 <span class="math notranslate nohighlight">\(\mu_{\theta}(z)=softmax(\theta z +b)\)</span> 时，就相当于多分类。</p></li>
</ul>
</div>
<p>再次强调，encoder 直接输出的并不是变量 <span class="math notranslate nohighlight">\(Z\)</span> 的具体数值，
而是 <span class="math notranslate nohighlight">\(Z\)</span> 的后验分布 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 的期望参数和方差参数，
相当于 encoder 得到的后验分布 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 本身。
因此完整的第①项是一个关于 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 的期望，
就是通过边缘化的方法得到变量 <span class="math notranslate nohighlight">\(X\)</span> 的重建分布 <span class="math notranslate nohighlight">\(p_{\theta}(x|z)\)</span>
（实际上解码器输出的是 <span class="math notranslate nohighlight">\(\mu_x\)</span>）。</p>
<p>事情到这里并没有结束，这个期望是没办法直接解析计算的，因为后验
<span class="math notranslate nohighlight">\(q_{\phi}(z|x)=\mathcal{N}(\mu_z,\Sigma_z)=\mathcal{N}(\mu_{\phi}(x),\Sigma_{\phi}(x))\)</span>
的表达式中是有一个神经网络（encoder）存在的，所以没有办法用解析计算的方式计算它的积分（求期望就是计算积分）。
这时可以借助马尔可夫链蒙特卡罗法(Markov Chain Monte Carlo，MCMC)，即采样法 <strong>近似</strong> 实现。
其实就是从后验概率分布 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 随机采样很多个 <span class="math notranslate nohighlight">\(z\)</span> 值，代入进去算平均值。
假设随机采样出 <span class="math notranslate nohighlight">\(L\)</span> 个 <span class="math notranslate nohighlight">\(z\)</span>，
则 <a class="reference internal" href="#equation-eq-vae-0082">公式(1.1.21)</a> 中第①部分期望项可以近似等价于</p>
<div class="math notranslate nohighlight" id="equation-aigc-7">
<span class="eqno">(1.2.14)<a class="headerlink" href="#equation-aigc-7" title="此公式的永久链接">¶</a></span>\[\mathbb{E}_{z \sim q_{\phi}(z|x) } [\ln  p_{\theta}(x|z) ] \approx
\frac{1}{L} \sum_{l=1}^L  [ \ln  p_{\theta}(x|z^{(l)}) ]\]</div>
<p>采样次数 <span class="math notranslate nohighlight">\(L\)</span> 可以作为模型的超参数，可以人为指定，根据作者的经验 <span class="math notranslate nohighlight">\(L=1\)</span> 其实也可以。
然而这有产生了新的问题，从编码器网络到解码器网络中间有个随机采样，即 <span class="math notranslate nohighlight">\(z\)</span> 是通过随机采样参数的，
而随机采样过程是不可导的，这导致梯度不能从解码器传递到编码器。VAE 的作者，
在这里采用重参数化（reparameterization trick）的技巧来解决这个问题。</p>
<p><strong>重参数化（reparameterization trick）</strong></p>
<p>重参数化的思想其实很简单，就是稍微调整了一下采样的方法。
我们需要从后验分布 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 中随机采样 <span class="math notranslate nohighlight">\(z\)</span> 的值，
这个后验分布是一个高斯分布 <span class="math notranslate nohighlight">\(\mathcal{N}(\mu_z,\Sigma_z)\)</span>
，直接从这个分布中采样会导致模型不可导，梯度无法传递。
这里可以利用高斯分布的一个特点来改变采样过程，任意均值和方差的高斯分布都可以从一个标准正态分布 <span class="math notranslate nohighlight">\(\mathcal{N}(0,\textit{I})\)</span>
变换得到，我们用符号 <span class="math notranslate nohighlight">\(\epsilon\)</span> 表示一个多维标准正态分布，即 <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0,\textit{I})\)</span>
，任意另一个高斯分布 <span class="math notranslate nohighlight">\(\mathcal{N}(\mu_z,\Sigma_z)\)</span> 的值可以通过下式直接计算得到</p>
<div class="math notranslate nohighlight" id="equation-eq-vae-018">
<span class="eqno">(1.2.15)<a class="headerlink" href="#equation-eq-vae-018" title="此公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned} z &amp;= \mu_z + \sqrt{\Sigma_z} \odot  \epsilon\\ &amp;= \mu_{\phi}(x) + \sqrt{\Sigma_{\phi}(x)} \odot \epsilon \ \ ,\epsilon \sim \mathcal{N}(0,\textit{I})\end{aligned}\end{align} \]</div>
<p>也就是说，可以先从标准正态分布 <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0,\textit{I})\)</span> 随机采样一个值，
然后再通过 <a class="reference internal" href="#equation-eq-vae-018">公式(1.2.15)</a> 计算得到 <span class="math notranslate nohighlight">\(z\)</span> 的值，其中 <span class="math notranslate nohighlight">\(\odot\)</span> 表示元素乘法。
这就相当于在 encoder 的输出 <span class="math notranslate nohighlight">\(\mu_{\phi}(x)\)</span> 的基础上加上随机高斯噪声，再乘上 encoder 的另一个输出
<span class="math notranslate nohighlight">\(\sqrt{\Sigma_{\phi}(x)}\)</span>
，随机采样的是高斯噪声 <span class="math notranslate nohighlight">\(\epsilon\)</span>
，而它不影响模型的梯度传递。</p>
<p>有了 <span class="math notranslate nohighlight">\(z\)</span> 的值后就可以计算生成条件概率 <span class="math notranslate nohighlight">\(p_{\theta}(x|z)\)</span> 了，
现在看下具体是怎么计算的，已知 <span class="math notranslate nohighlight">\(p_{\theta}(x|z)\)</span> 是一个单位方差的高斯分布，
根据高斯分布的概率密度函数 <a class="reference internal" href="#equation-eq-vae-010">公式(1.2.10)</a>
，<span class="math notranslate nohighlight">\(p_{\theta}(x|z)\)</span> 的形式为</p>
<div class="math notranslate nohighlight" id="equation-aigc-8">
<span class="eqno">(1.2.16)<a class="headerlink" href="#equation-aigc-8" title="此公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}p_{\theta}(x|z) &amp;=  \frac{1}{(2\pi)^{n/2} |\Sigma |^{1/2}}exp\{-\frac{1}{2}(x - \mu)^{T}\Sigma^{-1}(x - \mu)\}\\&amp; \propto exp\{-\frac{1}{2}(x - \mu_{x})^{T}(x - \mu_x)\}\\&amp;=  exp\{-\frac{1}{2}(x - \mu_{\theta}(z) )^{T}(x - \mu_{\theta}(z))\}\\&amp;=  exp\{ -\frac{1}{2}(x - decoder (z) )^{T}(x - decoder(z))\}\end{aligned}\end{align} \]</div>
<p>最后给出最终下界函数（ELBO）的形式，</p>
<div class="math notranslate nohighlight" id="equation-eq-vae-020">
<span class="eqno">(1.2.17)<a class="headerlink" href="#equation-eq-vae-020" title="此公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}  \mathcal{L}(q,\theta) &amp;=
    \underbrace{ \mathbb{E}_{z \sim q_{\phi}(z|x) } [\ln  p_{\theta}(x|z) ] }_{\text{①对应解码过程}}
        - \underbrace{ KL( q_{\phi}(z|x) ||p(z))}_{\text{②对应编码过程}}\\
&amp;=  \frac{1}{L} \sum_{l=1}^L \left [ \ln  p_{\theta}(x|z^{(l)}) \right ]  - KL( \mathcal{N}(\mu_z,\Sigma_z) || \mathcal{N}(0,\textit{I}))\\
&amp; \propto \frac{1}{L} \sum_{l=1}^L \left [    -\frac{1}{2}(x -  \mu_{x}  )^{T}(x -  \mu_{x}  )    \right  ]
- \left [ \frac{1}{2} \left ( tr ( \Sigma_z) + \mu_z^T \mu_z − k − \log det(\Sigma_z) \right )   \right ]\\&amp; =  -\frac{1}{2} \frac{1}{L} \sum_{l=1}^L \left [   (x -  \mu_{x}  )^{T}(x -  \mu_{x}  )    \right  ]
- \frac{1}{2} \left [  tr ( \Sigma_z) + \mu_z^T \mu_z − k − \log det(\Sigma_z)  \right ]\\
&amp; \propto  - \frac{1}{L} \sum_{l=1}^L \left [ \underbrace{   (x -  \mu_{x}  )^{T}(x -  \mu_{x}  )  }_{\text{均方误差}}  \right  ]
-  \left [  tr ( \Sigma_z) + \mu_z^T \mu_z − k − \log det(\Sigma_z)  \right ]\\
&amp; \text{其中，}\\&amp; \mu_{x}=\mu_{\theta}(z^{(l)}) = decoder(z^{(l)})\\&amp; z^{(l)} =  \mu_{z} + \sqrt{\Sigma_{z}}  \odot \epsilon \ \ ,\epsilon \sim \mathcal{N}(0,\textit{I})\\&amp; \mu_z = \mu_{\phi}(x) = encoder(x)_{\mu_z}\\&amp; \Sigma_z =\Sigma_{\phi}(x) = encoder(x)_{\Sigma_z}\end{aligned}\end{align} \]</div>
<p>由于我们是通过极大化 <span class="math notranslate nohighlight">\(\mathcal{L}(q,\theta)\)</span> 进行参数求解，
所以其中一些常数乘项可以去掉，只保留正比（ <span class="math notranslate nohighlight">\(\propto\)</span> ）项即可，
极大化上述公式，其实就等价于同时极小化下面两项</p>
<div class="math notranslate nohighlight" id="equation-aigc-9">
<span class="eqno">(1.2.18)<a class="headerlink" href="#equation-aigc-9" title="此公式的永久链接">¶</a></span>\[\frac{1}{L} \sum_{l=1}^L \left [ \underbrace{   (x -  \mu_{x}  )^{T}(x -  \mu_{x}  )  }_{\text{均方误差}}  \right  ]\]</div>
<p>和</p>
<div class="math notranslate nohighlight" id="equation-aigc-10">
<span class="eqno">(1.2.19)<a class="headerlink" href="#equation-aigc-10" title="此公式的永久链接">¶</a></span>\[\left [  tr ( \Sigma_z) + \mu_z^T \mu_z − k − \log det(\Sigma_z)  \right ]\]</div>
<p>这两项，第一项相当于均方误差，第二项是正则项。
万变不离其宗，最终回到了带有正则项的最小化均方误差。</p>
</section>
<section id="id3">
<h2><span class="section-number">1.3. </span>总结<a class="headerlink" href="#id3" title="此标题的永久链接">¶</a></h2>
<section id="em">
<h3><span class="section-number">1.3.1. </span>和EM算法的关系<a class="headerlink" href="#em" title="此标题的永久链接">¶</a></h3>
<p>未完成， to be continue</p>
<ol class="arabic">
<li><dl>
<dt>计算 <span class="math notranslate nohighlight">\(q(Z)=P(Z|X)\)</span> 的方式不同。</dt><dd><ul class="simple">
<li><p>EM 是用贝叶斯公式的方式计算，需要已知 <span class="math notranslate nohighlight">\(P(X|Z;\theta)\)</span> 和 <span class="math notranslate nohighlight">\(P(Z;\theta)\)</span> 具体形式，
以及参数 <span class="math notranslate nohighlight">\(\theta\)</span> 的值。</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-aigc-11">
<span class="eqno">(1.3.9)<a class="headerlink" href="#equation-aigc-11" title="此公式的永久链接">¶</a></span>\[q(Z)=P(Z|X) = \frac{P(X|Z;\theta) P(Z;\theta) }{ \int_{z}  P(X|Z;\theta) P(Z;\theta)}\]</div>
<ul class="simple">
<li><p>VAE 假设 <span class="math notranslate nohighlight">\(q(Z)=P(Z|X)\)</span> 是高斯分布 <span class="math notranslate nohighlight">\(\mathcal{N}(\mu_q,\sigma^2_q)\)</span>，然后用某个函数直接映射分布的期望和方差参数。</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-aigc-12">
<span class="eqno">(1.3.10)<a class="headerlink" href="#equation-aigc-12" title="此公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\mu_q = f(x,\theta)\\\sigma^2_q = g(x,\theta)\end{aligned}\end{align} \]</div>
<p>理论上，这里的 <span class="math notranslate nohighlight">\(f\)</span> 和 <span class="math notranslate nohighlight">\(g\)</span> 可以是任何合理的函数，在VAE中用神经网络(Neural Networks)实现。</p>
</dd>
</dl>
</li>
<li><p>条件概率 <span class="math notranslate nohighlight">\(P(Z|X)\)</span></p></li>
</ol>
</section>
<section id="variational">
<h3><span class="section-number">1.3.2. </span>为什么叫变分（variational）？<a class="headerlink" href="#variational" title="此标题的永久链接">¶</a></h3>
<p>在贝叶斯网领域，经常会遇到隐变量问题，当贝叶斯图中同时含有隐变量和观测变量时，通常需要利用观测变量去求出隐变量的后验概率分布，
这个过程也被称为推断（inference）。我们知道这个后验分布一般是根据贝叶斯定理得到，
如果可以通过贝叶斯公式直接解析计算得到后验分布，那么就得到了这个后验分布的准确形式，此时称为精准推断。
然而贝叶斯公式中的分母部分（也就是证据 <span class="math notranslate nohighlight">\(p(x)\)</span> ）是含有积分的，多数情况下是难以计算的，
这时就无法直接得到后验概率分布的准确形式。既然无法得到准确的结果，那是否可以得到一个近似的结果呢？
显然是可行的，这类得到后验概率分布近似结果的方法就成为近似推断（Approximate Inference）。
变分推断 （variational inference，VI）就是一种近似推断的方法，除此外还有采样法-马尔科夫蒙特卡洛（(Markov chain Monte Carlo, MCMC)）
等等。</p>
<p>回到本文的 VAE 算法，其中关键的编码器部分的 <span class="math notranslate nohighlight">\(q_{\phi}(z|x)\)</span> 就是隐变量的后验分布，算法中就采用一个参数化的神经网络（或者说一个参数化的函数）
去近似拟合这个后验概率分布，因此这个算法被称为 <em>变分</em> 自编码器。
其实解码器部分 <span class="math notranslate nohighlight">\(q_{\theta}(x|z)\)</span> 从本质上讲，也是在进行推断，只不过 <span class="math notranslate nohighlight">\(x\)</span> 和 <span class="math notranslate nohighlight">\(z\)</span> 反过来而已，
这一部分也是用一个参数化的神经网络近似推断出这个条件概率分布，因此解码器部分也是一个变分推断的过程。</p>
</section>
</section>
<section id="vq-vae">
<h2><span class="section-number">1.4. </span>VQ-VAE<a class="headerlink" href="#vq-vae" title="此标题的永久链接">¶</a></h2>
<p>待完成…..</p>
</section>
<section id="id4">
<h2><span class="section-number">1.5. </span>参考文献<a class="headerlink" href="#id4" title="此标题的永久链接">¶</a></h2>
<div class="docutils container" id="id5">
<aside class="footnote brackets" id="footcite-kingma2013autoencoding" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Diederik P Kingma and Max Welling. Auto-encoding variational bayes. 2013. <a class="reference external" href="https://arxiv.org/abs/1312.6114">arXiv:1312.6114</a>.</p>
</aside>
</aside>
</div>
</section>
</section>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html" class="btn btn-neutral float-right" title="2. 扩散概率模型（diffusion probabilistic models）" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="index.html" class="btn btn-neutral float-left" title="AI内容生成（ai-gc）" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2018, zhangzhenhu(acmtiger@outlook.com) 禁止一切形式的转载！.

    </p>
  </div>
    
    
    
    利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.

<script src="https://giscus.app/client.js"
        data-repo="zhangzhenhu/blog"
        data-repo-id="MDEwOlJlcG9zaXRvcnkzNDc4NzY1MTA="
        data-category="General"
        data-category-id="DIC_kwDOFLwsns4Ca2IR"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>
</script>










</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>