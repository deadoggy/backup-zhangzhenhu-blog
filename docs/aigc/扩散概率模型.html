<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<meta content="扩散模型" lang="zh_CN" name="description" xml:lang="zh_CN" />
<meta content="变分自编码器,Variational Autoencoder,VAE,扩散模型,Diffusion Model,生成模型,图像生成,DDPM,DDIM" name="keywords" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2. 扩散概率模型（diffusion probabilistic models） &mdash; 张振虎的博客 张振虎 文档</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
    <link rel="canonical" href="https://www.zhangzhenhu.com/aigc/扩散概率模型.html" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/translations.js"></script>
        <script>window.MathJax = {"tex": {"macros": {"RR": "{\\bf R}", "bold": ["{\\bf #1}", 1], "KL": ["{D_\\textrm{KL}\\left ( #1 \\| #2 \\right )}", 2], "EE": ["{\\mathbb{E}_{#1} \\left [ #2 \\right ]}", 2, ""], "scalemath": ["{\\scalebox{#1}{\\mbox{\\ensuremath{\\displaystyle #2}}}}", 2], "argmin": ["{\\operatorname*{\\arg\\min}}"], "ind": ["{\\perp\\!\\!\\!\\!\\perp}"]}}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="3. 去噪扩散隐式模型（Denoising Diffusion Implicit Models,DDIM）" href="ddim.html" />
    <link rel="prev" title="1. 变分自编码器（Variational Autoencoder）" href="%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            张振虎的博客
          </a>
              <div class="version">
                acmtiger@outlook.com
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="搜索文档" aria-label="搜索文档" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="导航菜单">
              <p class="caption" role="heading"><span class="caption-text">目录:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../glm/source/index.html">广义线性模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id2">1.1. 概率模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id3">1.1.1. 概率律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id4">1.1.2. 离散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id5">1.1.3. 连续模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id6">1.2. 条件概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id7">1.3. 联合概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id8">1.4. 全概率与贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id9">1.5. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id10">1.6. 随机变量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id11">1.6.1. 离散随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id12">1.6.2. 连续随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id13">1.6.3. 累积分布函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id14">1.6.4. 随机变量的函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id15">1.6.5. 期望与方差</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id16">1.7. 边缘化</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id17">1.8. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#ch-basic-bernoulli">1.8.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id19">1.8.2. 二项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id20">1.8.3. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id21">1.8.4. 多项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id22">1.8.5. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id23">1.8.6. 卡方分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#t">1.8.7. t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#f">1.8.8. F分布</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html">2. 最大似然估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-liklihood">2.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id3">2.2. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id4">2.3. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-2-gaussian-ml">2.4. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id6">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html">3. 推断与检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id2">3.1. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution">3.2. 抽样分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution-normal">3.2.1. 正态分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#t">3.2.2. 学生t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id5">3.2.3. 卡方分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id6">3.3. 极限理论</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id7">3.3.1. 马尔可夫和切比雪夫不等式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id8">3.3.2. 弱大数定律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id9">3.3.3. 依概率收敛</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-clt">3.3.4. 中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id11">3.3.5. 强大数定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id12">3.4. 似然估计量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id13">3.4.1. 估计量的偏差与方差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-fisher-information">3.4.2. 信息量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-mle-estimator">3.4.3. 最大似然估计的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-interval">3.5. 置信区间</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#z">3.5.1. 均值参数的 Z 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id18">3.5.2. 均值参数的 T 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id19">3.5.3. 方差参数的区间估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-test">3.6. 简单假设检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id22">3.6.1. Z检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id23">3.6.2. T检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id24">3.6.3. 卡方检验</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html">4. 贝叶斯估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id2">4.1. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id3">4.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id4">4.1.2. 类别分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id5">4.2. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id6">4.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id7">4.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id8">4.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html">5. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-1">5.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id3">5.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id4">5.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id5">5.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id6">5.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id7">5.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-moments">5.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id9">5.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#kl">5.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html">6. 线性回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id2">6.1. 最小二乘</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id3">6.1.1. 最小误差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id4">6.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id5">6.2. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id6">6.2.1. 高斯假设</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id7">6.2.2. 参数估计</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html">7. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id2">7.1. 指数族分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id3">7.1.1. 自然指数族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id4">7.1.2. 示例：高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id5">7.1.3. 示例：伯努利分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id6">7.2. 广义线性模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id7">7.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html">8. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate">8.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id3">8.2. 泰勒级数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id4">8.3. 梯度下降法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id6">8.4. 牛顿法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id7">8.4.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id8">8.4.2. 标准连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id9">8.4.3. 迭代初始值的设定</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#irls">8.5. 迭代重加权最小二乘(IRLS)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id10">8.5.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id11">8.5.2. 算法过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id12">8.6. 估计量的标准误差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate-phi">8.7. 分散参数的估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html">9. 模型评估</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id2">9.1. 拟合优度</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id3">9.1.1. 嵌套模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#likelihood-ratio">9.1.2. 对数似然比(Likelihood ratio)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance">9.1.3. 偏差(deviance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#r-2">9.1.4. 决定系数 <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#ch-glm-gof-chi">9.1.5. 广义皮尔逊卡方统计量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#residual-analysis">9.2. 残差分析(Residual analysis)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#response-residuals">9.2.1. Response residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#working-residuals">9.2.2. Working residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#partial-residuals">9.2.3. Partial residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#pearson-residuals">9.2.4. Pearson residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance-residuals">9.2.5. Deviance residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#score-residuals">9.2.6. Score residuals</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#model-selection">9.3. 模型选择(model selection)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#aic">9.3.1. AIC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#bic">9.3.2. BIC</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html">10. 模型检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id2">10.1. 拉格朗日乘子检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id3">10.1.1. 得分统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id4">10.1.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#wald">10.2. wald 检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id5">10.2.1. 参数估计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id6">10.2.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id7">10.3. 似然比检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id8">10.3.1. 抽样分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id9">10.3.2. 模型比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id10">10.3.3. 偏差统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#f">10.3.4. F 检验</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id11">10.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">11. 高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">11.1. 传统线性回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">11.2. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">11.3. 高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">11.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">11.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">11.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">11.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id10">11.5. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">12. 逆高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">12.1. 逆高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">12.2. 逆高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">12.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">12.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">12.3.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">12.3.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">12.4. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">13. 二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">13.1. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">13.2. 逻辑回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">13.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">13.2.2. 参数估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#odds-logit">13.2.3. odds 与 logit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">13.3. 二项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">13.4. 二项式回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">13.4.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">13.4.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">13.5. 其它连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">13.5.1. 恒等连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#probit">13.5.2. probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#log-log-clog-log">13.5.3. log-log 和 clog-log</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">13.6. 分组数据与比例数据</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html">14. 泊松模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#poisson">14.1. 泊松(Poisson)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id2">14.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id3">14.1.2. 泊松分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id5">14.2. 泊松回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id6">14.3. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id7">14.4. 拟合统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id8">14.5. 频率模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id9">14.6. 泊松模型的局限性</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html">15. 指数模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#exponential">15.1. 指数(exponential)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id2">15.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id3">15.1.2. 分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id6">15.2. 指数回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id7">15.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id8">15.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id9">15.3.2. 拟合优度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#irls">15.3.3. IRLS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html">16. Gamma 模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id1">16.1. Gamma 函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id2">16.2. Gamma 分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id3">16.3. Gamma 回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id4">16.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id5">16.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#irls">16.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id6">16.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id7">16.5. 其他连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id8">16.5.1. 对数 Gamma 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#identity-gamma">16.5.2. 恒等(identity) Gamma 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html">17. 过度分散</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id2">17.1. 什么是过度分散</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id3">17.2. 过度分散的检测</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id4">17.3. 过度分散的影响</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id5">17.4. 标准误差的修正</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">18. 负二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">18.1. 负二项式分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">18.1.1. 从二项式分布推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">18.1.2. 泊松-伽马混合分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#alpha">18.1.3. 辅助参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的影响</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">18.2. 负二项回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">18.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#irls">18.3.1. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">18.3.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">18.4. 负二项式模型扩展</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">18.4.1. 对数连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">18.4.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">18.4.3. 几何模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">18.4.4. 广义负二项式模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html">19. 零计数问题</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id2">19.1. 零截断模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id3">19.1.1. 零截断泊松模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id4">19.1.2. 零截断负二项式模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id5">19.2. 零膨胀模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#hurdle">19.2.1. Hurdle 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#zero-inflate">19.2.2. Zero-inflate 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">20. 多项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">20.1. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#softmax">20.2. softmax 回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">20.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">20.3. 多项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id6">20.4. 多项式回归模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">21. 有序离散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">21.1. 有序逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">21.2. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">21.3. 连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#logit">21.3.1. logit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#probit">21.3.2. probit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#clog-log">21.3.3. clog-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#log-log">21.3.4. log-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#cauchit">21.3.5. cauchit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">21.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html">附录</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id2">标准正态累积分布表</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id3">卡方分布临界值表</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/content.html">参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../probability_model/index_html.html">概率图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id2">1.1. 概率分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id3">1.2. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#marginalization">1.3. 边缘化(marginalization)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id6">1.4. 贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id7">1.5. 期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id8">1.6. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id9">1.6.1. 离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id10">1.6.2. 连续变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id11">1.6.3. 计数变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id12">1.7. 大数定律</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id13">1.7.1. 独立同分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id14">1.7.2. 中心极限定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id15">1.8. 信息论基础</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id16">1.8.1. 信息熵</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#kl">1.8.2. KL散度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id18">1.8.3. 互信息</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html">2. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-liklihood">2.1. 极大似然估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id3">2.1.1. 二值离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id4">2.1.2. 一般离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-gaussian-ml">2.1.3. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id6">2.1.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-bayesian-estimation">2.2. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id8">2.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id9">2.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id10">2.3. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id11">2.3.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id12">2.3.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id13">2.4. 最大似然估计与贝叶斯估计的对比</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id14">2.5. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#fisher-information">2.6. Fisher Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id15">2.7. 估计量的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id16">2.7.1. 估计量的方差与偏差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id17">2.7.2. 大数定律和中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-mle-estimator">2.7.3. 最大似然估计的特性</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html">3. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-1">3.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id3">3.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id4">3.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id5">3.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id6">3.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id7">3.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-moments">3.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id9">3.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#kl">3.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/19.%E5%A4%9A%E7%BB%B4%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_26.html">4. 多维高斯分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html">5. 有向图(Directed Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id1">5.1. 有向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id2">5.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id3">5.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html">6. 无向图(Undirected Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id1">6.1. 无向图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id2">6.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id3">6.3. 图的分解</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#vs">6.4. 有向图 vs 无向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id4">6.5. 树</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id5">6.6. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html">7. 因子图</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id2">7.1. 因子图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id3">7.2. 图模型之间的转换</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id4">7.2.1. 转换为因子图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id5">7.2.2. 因子图转换为有向图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id6">7.3. 图模型的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#i-map">7.3.1. I-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#d-map">7.3.2. D-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#p-map">7.3.3. P-map</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html">8. 模型推断：消元法</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id2">8.1. 什么是模型的推断</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id3">8.2. 消元法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id4">8.2.1. 有向图消元算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#ch-condition-margin">8.2.2. 条件概率和边缘概率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id6">8.2.3. 无向图的消元法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id7">8.3. 图消除</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id9">8.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html">9. 加和乘积算法(sum-product algorithm)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id1">9.1. 树结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id2">9.2. 从消元法到信息传播</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id3">9.3. 树模型的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id4">9.4. 因子图的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id5">9.5. 类树结构图模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#polytrees">9.6. 多重树(polytrees)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id6">9.7. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html">10. 最大后验估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id2">10.1. 最大后验概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id3">10.2. 最大化后验的状态</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id4">10.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html">11. 完整观测的参数学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id2">11.1. 有向图的参数学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id3">11.2. 无向图的参数学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id4">11.2.1. 成对二值变量模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id5">11.2.2. 一般二值变量模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html">12. 不完整观测的学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#id2">12.1. 隐变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#em">12.2. 期望最大化算法(EM)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/14.%E5%9B%BE%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E5%AD%A6%E4%B9%A0_lecture_23.html">13. 有向图结构学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/16.%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD_lecture_17.html">14. 变分推断</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html">15. 马尔科夫蒙特卡洛</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#why-sampling">15.1. Why sampling？</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#monte-carlo">15.2. 蒙特卡罗(Monte Carlo)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain">15.3. 马尔科夫链(Markov Chain)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id2">15.3.1. 一个例子</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#time-reversibility">15.3.2. 时间可逆性(Time Reversibility)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id3">15.3.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain-monte-carlo">15.4. Markov Chain Monte Carlo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#metropolis-hastings">15.4.1. Metropolis-Hastings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id4">15.4.2. 例子：正态分布的采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id5">15.4.3. 多变量采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#gibbs">15.4.4. Gibbs 采样</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#mixing-time">15.5. Mixing Time</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#approximate-map-and-partitioning">15.6. Approximate MAP and Partitioning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html">16. 贝叶斯分类器</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id2">16.1. 朴素贝叶斯模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id3">16.1.1. 模型表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id4">16.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id5">16.2. 高斯判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id6">16.2.1. 一元高斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id7">16.2.2. 多元高斯模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id8">16.3. 逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id9">16.4. 生成模型和判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id10">16.5. 多分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id11">16.6. 其它扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html">17. 回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id2">17.1. 机器学习的概率解释</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id3">17.2. 经典线性回归</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id4">17.2.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id5">17.3. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id6">17.3.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id7">17.4. 凸函数最优化问题</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id8">17.5. 岭回归</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html">18. 分类模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id2">18.1. 生成模型与判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id3">18.2. 线性回归与线性分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id4">18.3. 生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id5">18.3.1. 高斯判别模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id6">18.3.2. 朴素贝叶斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id7">18.3.3. 指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id8">18.4. 判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id9">18.4.1. 逻辑回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id10">18.4.2. 多分类</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id11">18.4.3. 最大熵模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#probit">18.4.4. Probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#noisy-or">18.4.5. Noisy-OR 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id12">18.4.6. 其它指数模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html">19. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id2">19.1. 定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id3">19.1.1. 指数族分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id4">19.1.2. 链接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id5">19.1.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id6">19.2. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id7">19.2.1. 梯度下降法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id8">19.2.2. 牛顿法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#irls">19.2.3. 迭代重加权最小二乘(IRLS)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#goodness-of-fit">19.3. goodness of fit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id9">19.4. 连续值响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id10">19.4.1. 高斯族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#gamma">19.4.2. Gamma族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id12">19.5. 二项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id13">19.6. 多项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id14">19.7. 计数响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id15">19.7.1. 泊松分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id16">19.8. GLM扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html">20. 混合模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id2">20.1. 一般混合模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id3">20.1.1. 模型的有向图表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id5">20.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id8">20.2. 高斯混合模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id9">20.2.1. 模型的表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id10">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#k-means">20.3. K-means</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/32.%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90_42.html">21. 因子分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/%E4%BA%8C%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B.html">22. 二变量模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/33.LDA_43.html">23. 主题模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/33.LDA_43.html#plsa">23.1. PLSA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/33.LDA_43.html#lda">23.2. LDA</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html">24. 隐马尔可夫模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id2">24.1. 隐马尔可夫模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id3">24.1.1. 马尔可夫模型和朴素贝叶斯模型的关系</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id4">24.2. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/27.%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA_37.html">25. 条件随机场</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/28.%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8_38.html">26. 卡尔曼滤波器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/40.%E9%A1%B9%E7%9B%AE%E5%8F%8D%E5%BA%94%E7%90%86%E8%AE%BA_50.html">27. 项目反应理论</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/41.%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA_51.html">28. 贝叶斯知识追踪</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_model/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE.html">29. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">AI内容生成（ai-gc）</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html">1. 变分自编码器（Variational Autoencoder）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#evidence-lower-bound-elbo">1.1. 证据下界(Evidence Lower Bound,ELBO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id2">1.2. 编码-解码</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id3">1.3. 总结</a><ul>
<li class="toctree-l4"><a class="reference internal" href="%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#em">1.3.1. 和EM算法的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#variational">1.3.2. 为什么叫变分（variational）？</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#vq-vae">1.4. VQ-VAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id4">1.5. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">2. 扩散概率模型（diffusion probabilistic models）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#diffusion-probabilistic-model">2.1. 扩散概率模型（diffusion probabilistic model）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#markovian-hierarchical-variational-autoencoder-mhvae">2.1.1. 马尔科夫分层自编码器（Markovian Hierarchical Variational Autoencoder,MHVAE)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">2.1.2. 扩散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">2.1.3. 前向-后向</a></li>
<li class="toctree-l4"><a class="reference internal" href="#elbo">2.1.4. 目标函数（ELBO）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id14">2.1.5. 图片生成（采样）过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#denoising-diffusion-probabilistic-model-ddpm">2.2. 降噪扩散概率模型（Denoising diffusion probabilistic model,DDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#score-based-ddpm">2.3. 基于分数的解释（Score-based DDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id23">2.4. 扩散模型的三种等价表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="#improved-denoising-diffusion-probabilistic-models-iddpm">2.5. 改进降噪扩散概率模型（Improved Denoising Diffusion Probabilistic Models,IDDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id24">2.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ddim.html">3. 去噪扩散隐式模型（Denoising Diffusion Implicit Models,DDIM）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ddim.html#id1">3.1. 扩散模型的回顾</a></li>
<li class="toctree-l3"><a class="reference internal" href="ddim.html#id2">3.2. 非马尔科夫前向过程</a></li>
<li class="toctree-l3"><a class="reference internal" href="ddim.html#id4">3.3. 加速采样</a></li>
<li class="toctree-l3"><a class="reference internal" href="ddim.html#id5">3.4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="Score-Based_Generative_Models.html">4. 基于分数的生成模型（Score-based generative models）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="Score-Based_Generative_Models.html#id4">4.1. 基于分数的生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#score-matching">4.1.1. 分数匹配算法（Score Matching）</a></li>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id8">4.1.2. 基于分数的生成模型面临的困难</a></li>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id12">4.1.3. 通过加噪的方法估计分布的近似分数</a></li>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id13">4.1.4. 基于分数的改进采样算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id15">4.1.5. 改进的分数生成模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="Score-Based_Generative_Models.html#id17">4.2. 随机微分方程</a><ul>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id18">4.2.1. 微分方程</a></li>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id19">4.2.2. 随机微分方程</a></li>
<li class="toctree-l4"><a class="reference internal" href="Score-Based_Generative_Models.html#id20">4.2.3. 基于随机微分方程的生成模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="Score-Based_Generative_Models.html#id21">4.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="Guidance.html">5. 条件控制扩散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="Guidance.html#classifier-guidance">5.1. classifier guidance</a></li>
<li class="toctree-l3"><a class="reference internal" href="Guidance.html#classifier-free-guidance">5.2. Classifier-free guidance</a></li>
<li class="toctree-l3"><a class="reference internal" href="Guidance.html#clip-guidance">5.3. CLIP Guidance</a><ul>
<li class="toctree-l4"><a class="reference internal" href="Guidance.html#id12">5.3.1. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html">6. 稳定扩散模型（Stable diffusion model）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#latent-diffusion-model-ldm">6.1. 潜在扩散模型（Latent diffusion model,LDM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#stable-diffusion-sd">6.2. 稳定扩散模型（Stable diffusion,SD）</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#id3">6.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="controlnet.html">7. 条件控制之ControlNet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="controlnet.html#id3">7.1. 算法原理</a></li>
<li class="toctree-l3"><a class="reference internal" href="controlnet.html#id6">7.2. 代码实现</a></li>
<li class="toctree-l3"><a class="reference internal" href="controlnet.html#id7">7.3. 最后的总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="controlnet.html#id8">7.4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dreamBooth.html">8. 条件控制之DreamBooth</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dreamBooth.html#id2">8.1. DreamBooth 技术</a></li>
<li class="toctree-l3"><a class="reference internal" href="dreamBooth.html#id3">8.2. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dalle2.html">9. DALL·E 2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dalle2.html#glide">9.1. GLIDE</a></li>
<li class="toctree-l3"><a class="reference internal" href="dalle2.html#unclip">9.2. Unclip</a></li>
<li class="toctree-l3"><a class="reference internal" href="dalle2.html#id1">9.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="imgen.html">10. Imagen</a><ul>
<li class="toctree-l3"><a class="reference internal" href="imgen.html#id2">10.1. 代码实现解读</a><ul>
<li class="toctree-l4"><a class="reference internal" href="imgen.html#id3">10.1.1. 第一阶段</a></li>
<li class="toctree-l4"><a class="reference internal" href="imgen.html#id4">10.1.2. 第二阶段</a></li>
<li class="toctree-l4"><a class="reference internal" href="imgen.html#id5">10.1.3. 第三阶段</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="imgen.html#id6">10.2. Imagen 总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="imgen.html#id7">10.3. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../audio/index.html">语音技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../audio/feature.html">1. 音频特征</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id2">1.1. 认识声音</a></li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id3">1.2. 认识声波</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id4">1.2.1. 物体的振动以及简谐振动</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id7">1.2.2. 什么是声波</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id8">1.2.3. 纯音和复合音</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#spectrum">1.2.4. 频谱 Spectrum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id10">1.2.5. 名词</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id13">1.3. 语音学</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id14">1.3.1. 发声原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id15">1.3.2. 听觉感应</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id16">1.4. 数字信号处理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id17">1.4.1. 模数转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#wav">1.4.2. 音频文件–WAV</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id18">1.5. 分帧与加窗</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id19">1.5.1. 预加重处理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id20">1.5.2. 分帧与加窗处理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id21">1.6. 声音的感官度量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#sound-pressure-level-spl">1.6.1. 声压与声压级(Sound Pressure Level,SPL)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#intensity-level-il">1.6.2. 声强与声强级(Intensity Level,IL）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id22">1.6.3. 声压与声强的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id23">1.6.4. 响度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id24">1.6.5. 音量计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id25">1.6.6. 频率与音高</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id26">1.7. 时域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id27">1.7.1. 短时能量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id28">1.7.2. 短时平均幅度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id29">1.7.3. 短时过零率</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id31">1.8. 频域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#spectrum-spectrogram">1.8.1. 声谱(spectrum)和时频谱(spectrogram)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#short-time-fourier-transform-stft">1.8.2. 短时傅里叶变换 Short-time Fourier transform (STFT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id33">1.8.3. 倒频谱</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id34">1.8.4. 色谱图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id35">1.9. 小波域特征</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id36">1.9.1. 离散小波域变换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id37">1.9.2. 小波域过零率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id38">1.9.3. 小波域质心</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id39">1.9.4. 小波域子带能量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#mfcc">1.10. 语音识别的音频特征–MFCC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id40">1.11. 参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../edm/index.html">教育领域数据挖掘</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../edm/bkt.html">1. 贝叶斯知识追踪(Bayesian Knowledge Tracing,BKT)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id1">1.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#hidden-markov-model-hmm">1.2. 隐马尔科夫模型(Hidden Markov Model,HMM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#bayesian-knowledge-tracing">1.3. 贝叶斯知识追踪(Bayesian Knowledge Tracing)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#bkt">1.3.1. BKT的参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#item-response-theory-irt">1.4. 项目反映理论(Item Response Theory,IRT)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#bktirt">1.5. BKT结合IRT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id5">1.6. 实验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id6">1.6.1. 数据集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id7">1.6.2. 实验方法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id8">1.6.3. 实验结果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id9">1.6.4. 项目代码</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id10">1.7. 未来工作</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id11">1.7.1. 题目难度的计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#irt">1.7.2. 多参数IRT模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id12">1.7.3. 参数估计算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id13">1.8. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../nlp/index.html">自然语言处理</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html">1. 文本去重</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id2">1.1. 背景</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id3">1.2. 技术思路</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id4">1.3. 相似（距离）算法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#euclidean-distance">1.3.1. 欧氏距离（Euclidean Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minkowski-distance">1.3.2. 闵科夫斯基距离（Minkowski Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#manhattan-distance">1.3.3. 曼哈顿距离（Manhattan Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#chebyshev-distance">1.3.4. 切比雪夫距离（Chebyshev Distance ）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#mahalanobis-distance">1.3.5. 马氏距离(Mahalanobis Distance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#cosine-similarity">1.3.6. 余弦夹角相似度(Cosine Similarity)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#hamming-distance">1.3.7. 汉明距离（Hamming Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#jaccard">1.3.8. Jaccard 系数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id5">1.3.9. 编辑距离</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id6">1.3.10. 最长公共字串</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id7">1.3.11. 最长公共子序列</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id8">1.4. 文本去重</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#kshingle">1.4.1. KShingle算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minhash">1.4.2. Minhash算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#simhash">1.4.3. simhash</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#ksentence">1.4.4. KSentence算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id9">1.5. 话术去重</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../nlp/bert/content.html">2. Attention&amp;Transformer&amp;Bert 简介</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#transformer">2.1. Transformer 从宏观到微观</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#seq2seq">2.1.1. seq2seq</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id1">2.1.2. 模型的输入</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#self-attention">2.2. Self-Attention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id2">2.2.1. 什么是注意力？</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id3">2.2.2. 加权求和</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id4">2.2.3. 位置编码</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#multi-head">2.2.4. 多头注意力（Multi-head）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#attention">2.3. Attention 机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#id5">2.4. 其它参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../rst_tutorial/latex.html">latex demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../rst_tutorial/latex.html#latex">latex</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../rst_tutorial/latex.html#how-to-write-an-m-x-n-matrix-in-latex">How to write an m x n matrix in LaTeX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-big-parentheses">With big parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-parentheses">With parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-brackets">With brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#latex-matrix-with-no-bracket">LateX matrix with no bracket</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-vertical-bar-brackets">With vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-curly-brackets">with curly brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-double-vertical-bar-brackets">with double vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#small-inline-matrix">small inline matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#examples-matrix-2-x-2-in-latex">Examples matrix 2 x 2 in LaTeX</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../rst_tutorial/graphviz.html">graphviz demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../rst_tutorial/graphviz.html#id1">布局</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst_tutorial/graphviz.html#id2">其它</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">读书笔记</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">《统计因果推断推理入门》读书笔记</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html">1. 第三章 干预的效果</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id2">1.1. 第3.1节 干预</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id3">1.2. 第3.2节 校正公式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id8">1.3. 第3.3节 后门准则</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html">《深度学习推荐系统》读书笔记</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id2">重点</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id3">冷启动</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id4">探索与利用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id5">召回层的主要策略</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#embedding">协同过滤 &amp; Embedding 向量</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="移动版导航菜单" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">张振虎的博客</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="页面导航">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">AI内容生成（ai-gc）</a></li>
      <li class="breadcrumb-item active"><span class="section-number">2. </span>扩散概率模型（diffusion probabilistic models）</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/aigc/扩散概率模型.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="diffusion-probabilistic-models">
<span id="ch-ddpm"></span><h1><span class="section-number">2. </span>扩散概率模型（diffusion probabilistic models）<a class="headerlink" href="#diffusion-probabilistic-models" title="此标题的永久链接"></a></h1>
<p>随着 OpenAI 发布 DALL-E 模型，图像生成一下子火爆起来，其中的扩散模型算法也步入了众多工程师的视野。
扩散模型于2015年引入，其动机来自非平衡热力学(non-equilibrium thermodynamics)，
本章我们介绍扩散模型的原理。</p>
<section id="diffusion-probabilistic-model">
<h2><span class="section-number">2.1. </span>扩散概率模型（diffusion probabilistic model）<a class="headerlink" href="#diffusion-probabilistic-model" title="此标题的永久链接"></a></h2>
<p>早在2015年，就由 <cite>Jascha Sohl-Dickstein</cite> 和 <cite>Eric</cite>
等人提出了概率扩散模型（diffusion probabilistic model,DPM） <a class="footnote-reference brackets" href="#footcite-sohldickstein2015deep" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>，可以简称为扩散模型（diffusion model,DM）。
在原论文 <a class="footnote-reference brackets" href="#footcite-sohldickstein2015deep" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> 中作者表示是受到了非平衡统计物理学（non-equilibrium statistical physics）
的启发进而提出了这个模型。当然这个劳什子 <cite>non-equilibrium statistical physics</cite> 大部分人包括我完全不了解，
所以我们不管它，我们从概率学的角度来阐述这个模型。</p>
<section id="markovian-hierarchical-variational-autoencoder-mhvae">
<h3><span class="section-number">2.1.1. </span>马尔科夫分层自编码器（Markovian Hierarchical Variational Autoencoder,MHVAE)<a class="headerlink" href="#markovian-hierarchical-variational-autoencoder-mhvae" title="此标题的永久链接"></a></h3>
<p>事实上，从某个角度上看，扩散模型可以看做是VAE（变分自编码器）的一个扩展，因此我们从 VAE 的角度入手。
首先回顾一下VAE模型（如 <a class="reference internal" href="#fg-dm-001"><span class="std std-numref">图 2.1.1</span></a>），其中有两个变量 <span class="math notranslate nohighlight">\(X\)</span> 和 <span class="math notranslate nohighlight">\(Z\)</span>。
自编码的过程是把输入 <span class="math notranslate nohighlight">\(x\)</span> 编码成 <span class="math notranslate nohighlight">\(z\)</span>，然后在从 <span class="math notranslate nohighlight">\(z\)</span> 解码回 <span class="math notranslate nohighlight">\(x\)</span>，
<span class="math notranslate nohighlight">\(x\)</span> 表示编码器的输入和解码器的输出，<span class="math notranslate nohighlight">\(z\)</span> 表示编码器的输出和解码器的输入。
就像图中所示那样，<span class="math notranslate nohighlight">\(p(z|x)\)</span> 对应编码过程，<span class="math notranslate nohighlight">\(p(x|z)\)</span> 对应解码过程，
<span class="math notranslate nohighlight">\(x\)</span> 是可观测的，<span class="math notranslate nohighlight">\(z\)</span> 是不可观测的。不熟悉的读者可以复习一下上一章节
<a class="reference internal" href="%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#ch-vae"><span class="std std-numref">1变分自编码器</span></a>
。</p>
<figure class="align-center" id="id26">
<span id="fg-dm-001"></span><a class="reference internal image-reference" href="../_images/diffusion_vae.png"><img alt="../_images/diffusion_vae.png" src="../_images/diffusion_vae.png" style="width: 175.68px; height: 166.56px;" /></a>
<figcaption>
<p><span class="caption-number">图 2.1.1 </span><span class="caption-text">变分自编码器（VAE）的图表示 （图片来自 <a class="footnote-reference brackets" href="#footcite-luo2022understanding" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>）</span><a class="headerlink" href="#id26" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>在 VAE 中编码过程只进行了一次，是否可以进行多次呢
（当然扩散模型的原论文 <a class="footnote-reference brackets" href="#footcite-sohldickstein2015deep" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> 并不是从这个角度出发进而提出扩散模型的）？
如<a class="reference internal" href="#fg-dm-002"><span class="std std-numref">图 2.1.2</span></a> 所示，把 VAE 的编码过程扩展成多次，从图上看，形如一个马尔科夫链的链式结构。
整个链是双向的，从左到右是一个逐步编码的过程，相当于把 VAE 的编码器循环执行了 <span class="math notranslate nohighlight">\(T\)</span> 次；
从右到左是一个逐步解码的过程，相当于把 VAE 的解码器循环执行了 <span class="math notranslate nohighlight">\(T\)</span> 次。
我们把编码过程称为前向过程（Forward Trajectory），解码过程称逆向过程（Reverse Trajectory），
无论前向还是逆向，每一个步骤（时刻） <span class="math notranslate nohighlight">\(t\)</span> 仅与它的上一个步骤（时刻）相关，
这是一个典型的马尔科夫过程（Markov chain）。</p>
<figure class="align-center" id="id27">
<span id="fg-dm-002"></span><a class="reference internal image-reference" href="../_images/diffusion_hvae.png"><img alt="../_images/diffusion_hvae.png" src="../_images/diffusion_hvae.png" style="width: 470.64px; height: 166.56px;" /></a>
<figcaption>
<p><span class="caption-number">图 2.1.2 </span><span class="caption-text">马尔科夫分层自编码器（MHVAE）的图表示（图片来自 <a class="footnote-reference brackets" href="#footcite-luo2022understanding" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>）</span><a class="headerlink" href="#id27" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p><span class="math notranslate nohighlight">\(q(z_{t}|z_{t-1})\)</span> 表示一个步骤的编码过程，<span class="math notranslate nohighlight">\(p(z_{t}|z_{t-1})\)</span>
表示一个步骤的解码过程。
模型所有的时刻的 <span class="math notranslate nohighlight">\(z_{1:T}\)</span> 构成了模型的隐变量集合，
整个模型的联合概率分布可以是表示为</p>
<div class="math notranslate nohighlight" id="equation-aigc-0">
<span class="eqno">(2.1.39)<a class="headerlink" href="#equation-aigc-0" title="此公式的永久链接"></a></span>\[p(x,z_{1:T}) = p(z_T) p_{\theta}(x|z_1) \prod_{t=2}^T p_{\theta}(z_{t-1}|z)\]</div>
<p>隐变量 <span class="math notranslate nohighlight">\(Z_{1:T}\)</span> 的后验概率 <span class="math notranslate nohighlight">\(q_{\phi}(z_{1:T}|x)\)</span> 可以分解为</p>
<div class="math notranslate nohighlight" id="equation-aigc-1">
<span class="eqno">(2.1.40)<a class="headerlink" href="#equation-aigc-1" title="此公式的永久链接"></a></span>\[q_{\phi}(z_{1:T}|x) = q_{\phi}(z_{1}|x) \prod_{t=2}^T q_{\phi}(z_{t}|z_{t-1})\]</div>
<p>在这个模型中，仍然是只有变量 <span class="math notranslate nohighlight">\(X\)</span> 是可观测的，观测样本的对数似然为 <span class="math notranslate nohighlight">\(\ln p(x)\)</span>
，和 VAE 中的推导是相同的，我们需要找到它的下界函数（ELBO）。</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-010">
<span class="eqno">(2.1.41)<a class="headerlink" href="#equation-eq-ddpm-010" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned} \ln p(x) &amp;= \ln \int p(x,z_{1:T}) d z_{1:T}\\ &amp;= \ln \int \frac{p(x,z_{1:T}) q_{\phi}(z_{1:T}|x) }{  q_{\phi}(z_{1:T}|x) }  d z_{1:T}
 &amp;(\text{乘上} 1= \frac{ q_{\phi}(z_{1:T}|x) }{  q_{\phi}(z_{1:T}|x) } \text{相当于没变化}  )\\ &amp;= \ln \mathbb{E}_{ q_{\phi}(z_{1:T}|x)} \left [ \frac{ p(x,z_{1:T}) }{  q_{\phi}(z_{1:T}|x) }  \right ]
  &amp;(\text{积分变期望})\\&amp; \ge \mathbb{E}_{ q_{\phi}(z_{1:T}|x)} \left [  \ln \frac{ p(x,z_{1:T}) }{  q_{\phi}(z_{1:T}|x) }    \right ]
&amp;(\text{应用 Jensen 不等式})\end{aligned}\end{align} \]</div>
<p><a class="reference internal" href="#equation-eq-ddpm-010">公式(2.1.41)</a> 和 VAE 相比并没有太大变化，只是把 VAE 中的 <span class="math notranslate nohighlight">\(z\)</span> 换成了 <span class="math notranslate nohighlight">\(z_{1:T}\)</span>
，有关 EBLO 更详细的推导和介绍可以参考 VAE 的章节：<a class="reference internal" href="%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#ch-vae"><span class="std std-numref">1变分自编码器</span></a>
。</p>
</section>
<section id="id6">
<h3><span class="section-number">2.1.2. </span>扩散模型<a class="headerlink" href="#id6" title="此标题的永久链接"></a></h3>
<p>把 VAE 扩展成马尔科夫链式结构就得到了 MHVAE，
MHVAE 是扩散模型的雏形，只需要在 MHVAE 的基础上再稍微调整一下就得到了扩散模型。</p>
<figure class="align-center" id="id28">
<span id="fg-dm-003"></span><a class="reference internal image-reference" href="../_images/diffusion_vdm_base.png"><img alt="../_images/diffusion_vdm_base.png" src="../_images/diffusion_vdm_base.png" style="width: 554.2px; height: 217.5px;" /></a>
<figcaption>
<p><span class="caption-number">图 2.1.3 </span><span class="caption-text">扩散模型的图表示（图片来自 <a class="footnote-reference brackets" href="#footcite-luo2022understanding" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>）</span><a class="headerlink" href="#id28" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>首先，不再区分 <span class="math notranslate nohighlight">\(x\)</span> 和 <span class="math notranslate nohighlight">\(z\)</span>，反正都是对输入数据 <span class="math notranslate nohighlight">\(x\)</span> 进行变换，
每个节点都用符号 <span class="math notranslate nohighlight">\(x\)</span> 来表示。不用额外单独处理第一步了，这样整个过程可以是一个循环执行编码或者解码。
此时由于每一步的输出是下一步的输入，所以输入和输出数据的尺寸必须是一样的了，即所有 <span class="math notranslate nohighlight">\(x_t\)</span> 的尺寸必须是相同的，
这一点和原来的 VAE 不一样了，在 VAE 中 <span class="math notranslate nohighlight">\(z\)</span> 的尺寸一般是小于 <span class="math notranslate nohighlight">\(x\)</span> 的。</p></li>
<li><p>其次，前向编码过程每一个步骤的编码器 <span class="math notranslate nohighlight">\(q(x_t|x_{t-1})\)</span> 不再通过神经网络去学习，而是直接固定为一个高斯线性变换。
这点很重要，在扩散模型中，编码器不再使用参数化的模型去学习拟合了，而是直接假设其是一个线性高斯变换，即后验条件概率 <span class="math notranslate nohighlight">\(q(x_t|x_{t-1})\)</span>
是一个已知的不含未知参数的高斯条件分布，具体方程稍后再介绍。</p></li>
<li><p>最后，由于上一步编码器为线性高斯的假设，结合马尔科夫链的特征，理论上当 <span class="math notranslate nohighlight">\(T \to \infty\)</span> 时， <span class="math notranslate nohighlight">\(x_T\)</span> 是一个正态分布，
即随着 <span class="math notranslate nohighlight">\(T\)</span> 的增大，<span class="math notranslate nohighlight">\(x_T\)</span> 趋近于正态分布。通过为这个线性高斯设定一个小于 <span class="math notranslate nohighlight">\(1\)</span> 的渐系数，
可以使得 <span class="math notranslate nohighlight">\(x_T\)</span> 收敛到一个<strong>标准正态分布</strong>，即 <span class="math notranslate nohighlight">\(\mathcal{N}(0,\textit{I})\)</span>
（均值为 <span class="math notranslate nohighlight">\(0\)</span> ，方差为单位方差 <span class="math notranslate nohighlight">\(\textit{I}\)</span>） 。</p></li>
</ul>
</section>
<section id="id8">
<h3><span class="section-number">2.1.3. </span>前向-后向<a class="headerlink" href="#id8" title="此标题的永久链接"></a></h3>
<p>如<a class="reference internal" href="#fg-dm-003"><span class="std std-numref">图 2.1.3</span></a> 所示，它是一个马尔科夫链式结构的贝叶斯网络（有向图网络），
<span class="math notranslate nohighlight">\(x_0\)</span> 是模型的起点变量，它可以代表真实图片的分布（或者说随机变量）。
从 <span class="math notranslate nohighlight">\(x_0\)</span> 开始，按照这个链式结构逐步的演变成一个标准的高斯分布 <span class="math notranslate nohighlight">\(x_T\)</span>。
整个网络的联合概率分布可以表示为 <span class="math notranslate nohighlight">\(p(x_{0:T})\)</span>，
如果按照从左到右的顺序对这个联合概率进行链式拆解</p>
<div class="math notranslate nohighlight" id="equation-aigc-2">
<span class="eqno">(2.1.42)<a class="headerlink" href="#equation-aigc-2" title="此公式的永久链接"></a></span>\[p(x_{0:T}) = q(x_0) \prod_{t=1}^T q(x_t|x_{t-1})\]</div>
<p>我们把从左到右的演变过程称为前向过程，因为它是把一个真实图片 <span class="math notranslate nohighlight">\(x_0\)</span>
逐步添加噪声，并最终变成一个纯粹的随机高斯变量 <span class="math notranslate nohighlight">\(x_T\)</span> 的过程，所以也可以称其扩散过程。</p>
<p>如果按照从右到左的顺序，正好是前向扩散过程的逆过程，所以称之为逆过程，
它可以从一个纯粹的高斯噪声（标准高斯分布）随机变量 <span class="math notranslate nohighlight">\(x_T\)</span> 逐步演变，最终得到一个真实图片 <span class="math notranslate nohighlight">\(x_0\)</span>
，这个过程相当于生成了一张新图片，所以也称为（图像）生成过程
，而这个过程在统计概率学上，本质是从一个联合概率分布进行采样的过程，所以也可以称为采样（sample）过程。
同样，可以按照从右到左的顺序对联合概率 <span class="math notranslate nohighlight">\(p(x_{0:T})\)</span> 进行链式分解</p>
<div class="math notranslate nohighlight" id="equation-aigc-3">
<span class="eqno">(2.1.43)<a class="headerlink" href="#equation-aigc-3" title="此公式的永久链接"></a></span>\[p(x_{0:T}) = p(x_T) \prod_{t=T}^1 p(x_{t-1}|x_{t})\]</div>
<p><strong>前向过程</strong></p>
<p>然而，代表真实图像 <span class="math notranslate nohighlight">\(x_0\)</span> 的真实概率分布 <span class="math notranslate nohighlight">\(p(x_0)\)</span> 的概率密度函数是不知道的，
但我们能得到一批真实的图像样本，也就是我们有 <span class="math notranslate nohighlight">\(x_0\)</span> 的观测样本，
此时 <span class="math notranslate nohighlight">\(x_0\)</span> 是已知观测值，<span class="math notranslate nohighlight">\(x_{1:T}\)</span> 是未知的隐变量，
这时整个马尔科夫网络的联合概率变成了一个条件概率 <span class="math notranslate nohighlight">\(q(x_{1:T}|x_0)\)</span>，
根据链式法则，可以展开为</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-020">
<span class="eqno">(2.1.44)<a class="headerlink" href="#equation-eq-ddpm-020" title="此公式的永久链接"></a></span>\[q(x_{1:T}|x_0) = q(x_1|x_0) \prod_{t=2}^T q(x_t|x_{t-1}) =  \prod_{t=1}^T q(x_t|x_{t-1})\]</div>
<p>根据第二项假设，每一个 <span class="math notranslate nohighlight">\(x_t\)</span> 都是一个高斯变量，
前向过程每一个步骤的编码器 <span class="math notranslate nohighlight">\(q(x_t|x_{t-1})\)</span> 固定为一个线性高斯变换，
所谓线性高斯变换是指 <span class="math notranslate nohighlight">\(x_t\)</span> 的均值和 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> 的值是线性关系。</p>
<p>在本论文中，定义 <span class="math notranslate nohighlight">\(q(x_t|x_{t-1})\)</span> 的方差与 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> 是独立的，
并且为 <span class="math notranslate nohighlight">\(\beta_t \textit{I}\)</span>，
其中 <span class="math notranslate nohighlight">\(0 \lt \beta_1 \lt \beta_2 \lt \dots \beta_T \lt 1\)</span>
。这么做的意义：<strong>前期方差较小，添加的噪声少，扩散速度慢；随着方差逐步加大，添加的噪声越来越多，扩散的速度加快。</strong>
<span class="math notranslate nohighlight">\(\beta_t\)</span> 作为一个超参数存在，人工指定它的值。</p>
<p>定义 <span class="math notranslate nohighlight">\(q(x_t|x_{t-1})\)</span> 的均值 <span class="math notranslate nohighlight">\(\mu_{x_t}\)</span>
和 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> 是线性关系，
这里设定另外一个系数 <span class="math notranslate nohighlight">\(\alpha_t\)</span>，并且令 <span class="math notranslate nohighlight">\(\alpha_t = 1-\beta_t\)</span>。
<span class="math notranslate nohighlight">\(\mu_{x_t}\)</span> 与 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> 的关系定义为：</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-021">
<span class="eqno">(2.1.45)<a class="headerlink" href="#equation-eq-ddpm-021" title="此公式的永久链接"></a></span>\[\mu_{x_t} =  \sqrt{\alpha_t} \ x_{t-1}\]</div>
<p><span class="math notranslate nohighlight">\(x_t\)</span> 的方差定义成与 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> <strong>无关</strong>，而是经过缩放的单位方差，
定义为</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-022">
<span class="eqno">(2.1.46)<a class="headerlink" href="#equation-eq-ddpm-022" title="此公式的永久链接"></a></span>\[\Sigma_{x_t} = \beta_t \textit{I} =  (1- \alpha_t ) \textit{I}\]</div>
<p><span class="math notranslate nohighlight">\(q(x_t|x_{t-1})\)</span> 就是一个以 <span class="math notranslate nohighlight">\(\sqrt{\alpha_t} \ x_{t-1}\)</span> 为均值，
以 <span class="math notranslate nohighlight">\((1- \alpha_t ) \textit{I}\)</span> 为方差的高斯分布。
按照线性高斯的特征，如 <a class="reference internal" href="#equation-eq-ddpm-023">公式(2.1.47)</a> 所示，
它可以看做是在 <span class="math notranslate nohighlight">\(\sqrt{\alpha_t} \ x_{t-1}\)</span> 基础上加上一个
<span class="math notranslate nohighlight">\(\mathcal{N} (0, (1- \alpha_t ) \textit{I} )\)</span> 的随机高斯噪声。
<strong>这就相当于每一个步骤都在前一个步骤的基础上加上一个随机高斯噪声数据，随着</strong><span class="math notranslate nohighlight">\(t\)</span>
<strong>的增加，</strong><span class="math notranslate nohighlight">\(x_t\)</span> <strong>逐步变成一个高斯噪声数据</strong>。
如<a class="reference internal" href="#fg-dm-003"><span class="std std-numref">图 2.1.3</span></a> 所示，一张正常的图片逐步变成一张高斯噪声图。
这个过程好比把一滴墨水滴入一杯水中，随着时间的推移，墨水逐步扩散分不到各处，这杯水变成浑浊的一杯水，
原来的墨滴再也看不到了，这就是这个模型被称为扩散模型（diffusion model）的原因。</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-023">
<span class="eqno">(2.1.47)<a class="headerlink" href="#equation-eq-ddpm-023" title="此公式的永久链接"></a></span>\[q(x_t|x_{t-1}) = \mathcal{N} (\sqrt{\alpha_t} \ x_{t-1}, (1- \alpha_t ) \textit{I} )\]</div>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-024">
<span class="eqno">(2.1.48)<a class="headerlink" href="#equation-eq-ddpm-024" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}x_{t} &amp;=\sqrt{\alpha_t} \ x_{t-1} + \mathcal{N} (0, (1- \alpha_t ) \textit{I} )\\&amp;=\sqrt{\alpha_t} \ x_{t-1} +  \sqrt{1- \alpha_t } \ \epsilon \ \ \ ,\epsilon \sim \mathcal{N} (0, \textit{I} )\end{aligned}\end{align} \]</div>
<p>至于系数 <span class="math notranslate nohighlight">\(\beta_t\)</span> 的值可以作为一个超参数有人工指定<a class="footnote-reference brackets" href="#footcite-ho2020denoising" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>
，也可以作为一个可学习的参数由模型学习<a class="footnote-reference brackets" href="#footcite-kingma2022variational" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>，
这里我们从直觉的角度来理解系数 <span class="math notranslate nohighlight">\(\beta_t\)</span> 的作用。</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>为什么要有系数 <span class="math notranslate nohighlight">\(\beta_t\)</span> ？</dt><dd><p>前向扩散过程是希望 <span class="math notranslate nohighlight">\(x_t\)</span> 渐进趋近于<strong>标准正态分布</strong>，即均值趋近于 <span class="math notranslate nohighlight">\(0\)</span>，
方差趋近于单位方差 <span class="math notranslate nohighlight">\(\textit{I}\)</span>，并且 <span class="math notranslate nohighlight">\(x_t\)</span> 只和它的<strong>上一步</strong> <span class="math notranslate nohighlight">\(x_{t-1}\)</span>
有关，同时  <span class="math notranslate nohighlight">\(x_t\)</span> 和 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> 是线性高斯变化，也就是说 <span class="math notranslate nohighlight">\(\mu_{x_t}\)</span>
和 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> 的关系必然是某个系数乘上 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> 的线性关系。</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><span class="math notranslate nohighlight">\(\alpha_t\)</span> 为什么小于 <span class="math notranslate nohighlight">\(1\)</span>？</dt><dd><p>因为需要 <span class="math notranslate nohighlight">\(\mu_{x_t} \to 0\)</span>，所以这个系数必然是小于 <span class="math notranslate nohighlight">\(1\)</span> 的。</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>什么要有根号？</dt><dd><p>在满足 <span class="math notranslate nohighlight">\(\mu_{x_t} \to 0\)</span> 的同时，也要满足 <span class="math notranslate nohighlight">\(\Sigma_{x_t} \to \textit{I}\)</span>。
用一个系数同时控制均值和方差的变化，可以扩散过程更稳健。而 <span class="math notranslate nohighlight">\(\mu_{x_t}\)</span> 和 <span class="math notranslate nohighlight">\(\Sigma_{x_t}\)</span>
并不是同一个量纲，方差是平方的，原论文是直接把 <span class="math notranslate nohighlight">\(\alpha_t\)</span> 定义在了方差上，所以在均值上就要开根号。</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>这个线性关系（线性高斯），为什么没有截距项？</dt><dd><p>观察下 <a class="reference internal" href="#equation-eq-ddpm-023">公式(2.1.47)</a>，其中的高斯噪声 <span class="math notranslate nohighlight">\(\sqrt{1- \alpha_t } \ \epsilon\)</span> 就相当于截距。</p>
</dd>
</dl>
</li>
</ol>
<p><span class="math notranslate nohighlight">\(\alpha_t\)</span> 并不是一个固定值，是可以随着 <span class="math notranslate nohighlight">\(t\)</span> 的增长逐渐变小的，这可以使得扩散过程中加的高斯噪声比重逐渐加重。
分析一下这样做的好处，前期如果加的噪声太多，会使得数据扩展的太快（比如突变），使得逆向还原变得困难；
同样因为后期数据本身已经接近随机噪声数据了，后期如果加的噪声不够多，相当于变化幅度小，扩散的太慢，这会使得链路变长需要的事件变多。
我们希望扩散的前期慢一点，后期快一点，因此 <span class="math notranslate nohighlight">\(\alpha_t\)</span> 最好是逐渐变小的。</p>
<p>总的来说，前向过程就是一个逐步添加高斯噪声，并且最终变成一个纯的高斯噪声数据。
另外前向过程中的编码分布 <span class="math notranslate nohighlight">\(q(x_t|x_{t-1})\)</span> 并没有进行参数化表示，而是假定是一个确定的线性高斯变换。</p>
<p>在前向过程我们需要计算每一个时刻的 <span class="math notranslate nohighlight">\(x_t\)</span>，最直接的计算方式就是写一个循环，按照公式 <a class="reference internal" href="#equation-eq-ddpm-023">公式(2.1.47)</a>
逐步从 <span class="math notranslate nohighlight">\(x_0\)</span> 算到 <span class="math notranslate nohighlight">\(x_T\)</span>，公式中的 <span class="math notranslate nohighlight">\(\epsilon_{t}\)</span> 就是从一个标准正态分布随机采样，
这和 VAE 中是一样的。这样用循环计算当然可以，但是当 <span class="math notranslate nohighlight">\(T\)</span> 较大时，明显是效率低下的。
事实上，得益于 <span class="math notranslate nohighlight">\(q(x_t|x_{t-1})\)</span> 是线性高斯的假设，这里有一个计算上的小技巧，可以直接从 <span class="math notranslate nohighlight">\(x_0\)</span>
一步计算任意的 <span class="math notranslate nohighlight">\(t\)</span>，这样就可以并行计算全部的 <span class="math notranslate nohighlight">\(x_t\)</span>。
如下是具体的推导过程：</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-025">
<span class="eqno">(2.1.49)<a class="headerlink" href="#equation-eq-ddpm-025" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}x_t &amp;= \sqrt{\alpha_t} \ x_{t-1} + \sqrt{1-\alpha_t} \ \epsilon_{t}\\&amp;= \sqrt{\alpha_t} \left(   \sqrt{\alpha_{t-1}} \ x_{t-2} + \sqrt{1-\alpha_{t-1}} \ \epsilon_{t-1}  \right ) + \sqrt{1-\alpha_t} \ \epsilon_{t}\\&amp;=  \sqrt{\alpha_t \alpha_{t-1} }  \ x_{t-2}
+ \underbrace{ \sqrt{\alpha_t - \alpha_t \alpha_{t-1} }\ \epsilon_{t-1} + \sqrt{1- \alpha_t} \ \epsilon_{t}
  }_{\text{两个相互独立的0均值的高斯分布相加}}\\
&amp;=  \sqrt{\alpha_t \alpha_{t-1} }  \ x_{t-2}
+ \underbrace{  \sqrt{ \sqrt{\alpha_t - \alpha_t \alpha_{t-1} }^2 + \sqrt{1- \alpha_t}^2  } \ \epsilon
}_{\text{两个方差相加，用一个新的高斯分布代替}}\\&amp;= \sqrt{\alpha_t \alpha_{t-1} }  \ x_{t-2} + \sqrt{1- \alpha_t \alpha_{t-1}} \ \epsilon\\&amp;= ...\\&amp;= \sqrt{\prod_{i=1}^t \alpha_i} \ x_0 + \sqrt{1- \prod_{i=1}^t \alpha_i }  \ \epsilon\\&amp;= \sqrt{\bar{ \alpha}_t } \ x_0 + \sqrt{1- \bar{ \alpha}_t }  \ \epsilon  \ \ \ ,
\bar{\alpha} = \prod_{i=1}^t \alpha_i ,\ \ \epsilon \sim \mathcal{N}(0,\textit{I})\\&amp;\sim \mathcal{N}(\sqrt{\bar{\alpha}_t } \ x_0,  (1- \bar{ \alpha}_t)    \textit{I})\end{aligned}\end{align} \]</div>
<p>前项过程的每一个步骤的转换条件概率 <span class="math notranslate nohighlight">\(q(x_t|x_{t-1})\)</span>
代表着从 <span class="math notranslate nohighlight">\(x_{t-1})\)</span> 到 <span class="math notranslate nohighlight">\(x_t\)</span> 的转换方法（过程），
而且我们已经得到了条件概率分布 <span class="math notranslate nohighlight">\(q(x_t|x_{t-1})\)</span> 的具体形式（ <a class="reference internal" href="#equation-eq-ddpm-023">公式(2.1.47)</a> ）
，具体的，当有了 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> 的值后，可以根据 <a class="reference internal" href="#equation-eq-ddpm-024">公式(2.1.48)</a>
计算 <span class="math notranslate nohighlight">\(x_{t}\)</span> 的值，就这样循环计算，直到计算出 <span class="math notranslate nohighlight">\(x_{T}\)</span> 的值。
得益于条件高斯分布的特性，我们推导出一个计算小技巧，<a class="reference internal" href="#equation-eq-ddpm-025">公式(2.1.49)</a>，
可以直接从 <span class="math notranslate nohighlight">\(x_0\)</span> 计算出任意时刻的 <span class="math notranslate nohighlight">\(x_t\)</span>
，不用再逐步循环计算，这极大的提高了我们的计算速度。</p>
<p>最后的重点，我们发现只要设置了超参数 <span class="math notranslate nohighlight">\(\alpha_{0:T}\)</span> 的值，这个前向计算过程是可以直接解析（使用公式）计算的，
没有未知参数，不需要用一个模型学习这个过程。
然而逆向过程就不能直接计算的，是需要用模型学习的。</p>
<p><strong>逆向过程</strong></p>
<p>如<a class="reference internal" href="#fg-dm-003"><span class="std std-numref">图 2.1.3</span></a>，逆向过程是从右到左的解码过程，从 <span class="math notranslate nohighlight">\(x_T\)</span> （随机高斯噪声）开始，
逐步的解码成一个有意义的数据，比如生成一张图片。
解码过程的每一个时刻 <span class="math notranslate nohighlight">\(t\)</span> 要把 <span class="math notranslate nohighlight">\(x_{t+1}\)</span> 还原成 <span class="math notranslate nohighlight">\(x_t\)</span>，
每一步骤对应的条件概率表示成 <span class="math notranslate nohighlight">\(p(x_t|x_{t+1})\)</span>。
按照逆向过程，对联合概率 <span class="math notranslate nohighlight">\(p(x_{0:T})\)</span>
进行分解为：</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-026">
<span class="eqno">(2.1.50)<a class="headerlink" href="#equation-eq-ddpm-026" title="此公式的永久链接"></a></span>\[p(x_{0:T}) = p(x_T) \prod_{t={T-1}}^0 p(x_{t}|x_{t+1})\]</div>
<p>其中 <span class="math notranslate nohighlight">\(p(x_T)\)</span> 的概率密度是知道的，它是一个标准高斯分布，即 <span class="math notranslate nohighlight">\(p(x_T) \sim \mathcal{N}(0,\textit{I})\)</span>
。然而 <span class="math notranslate nohighlight">\(p_{\theta}(x_{t}|x_{t+1})\)</span> 是难以计算的，
即使根据贝叶斯定理得出 <span class="math notranslate nohighlight">\(p_{\theta}(x_{t}|x_{t+1})\)</span> 的表达式，
它的分母部分（归一化项）是含有积分的，这个积分是没有解析解的，是非常难以计算的。
因此，我们想到用一个模型（神经网络）取拟合学习条件概率分布 <span class="math notranslate nohighlight">\(p_{\theta}(x_{t}|x_{t+1})\)</span>
，然后利用学习好的模型，充当 <span class="math notranslate nohighlight">\(p_{\theta}(x_{t}|x_{t+1})\)</span>
，这样就有了逆向过程的完整链式结构（ <a class="reference internal" href="#equation-eq-ddpm-026">公式(2.1.50)</a>）
，也就可以通过逆向过程，从一个随机高斯噪声 <span class="math notranslate nohighlight">\(x_T\)</span>
逐步生成一张真实的图片 <span class="math notranslate nohighlight">\(x_0\)</span>。</p>
<p>为了符号区分，
定义符号 <span class="math notranslate nohighlight">\(\theta\)</span> 表示模型的学习参数，
不带 <span class="math notranslate nohighlight">\(\theta\)</span> 的 <span class="math notranslate nohighlight">\(p(x_t|x_{t+1})\)</span> 表示解码器的真实分布，
带 <span class="math notranslate nohighlight">\(\theta\)</span> 的 <span class="math notranslate nohighlight">\(p_{\theta}(x_t|x_{t+1})\)</span> 表示参数化模型学习的近似分布。
下一步就是如何学习 <span class="math notranslate nohighlight">\(p_{\theta}(x_t|x_{t+1})\)</span>，
类似于 VAE 模型，通过最大化 ELBO 来进行参数学习。</p>
</section>
<section id="elbo">
<h3><span class="section-number">2.1.4. </span>目标函数（ELBO）<a class="headerlink" href="#elbo" title="此标题的永久链接"></a></h3>
<p>我们知道学习一个概率分布的未知参数的常用算法是极大似然估计，
极大似然估计是通过极大化观测数据的对数概率（似然）实现的。
在这里，整个网络的联合概率分布是 <span class="math notranslate nohighlight">\(p(x_{0:T})\)</span>
，其中，我们只有 <span class="math notranslate nohighlight">\(x_0\)</span> 的观测数据（样本）
，没有 <span class="math notranslate nohighlight">\(x_{1:T}\)</span> 的观测样本，
因此我们极大化的是边缘分布 <span class="math notranslate nohighlight">\(p(x_0)\)</span>，而不是联合分布 <span class="math notranslate nohighlight">\(p(x_{0:T})\)</span>
。边缘分布 <span class="math notranslate nohighlight">\(p(x_0)\)</span> 可以通过边际化得到</p>
<div class="math notranslate nohighlight" id="equation-aigc-4">
<span class="eqno">(2.1.51)<a class="headerlink" href="#equation-aigc-4" title="此公式的永久链接"></a></span>\[p(x_0) = \int p(x_{0:T}) dx_{1:T}\]</div>
<p>显然扩散模型的学习过程和原始的VAE以及上一节介绍的 MHVAE 相似的，原始的观测数据对数似然 <span class="math notranslate nohighlight">\(\ln p(x_0)\)</span>
包含隐变量的积分操作，是无法直接极大化的，需要用它的下界函数（ELBO）进行替代。
接下来我们推导一下扩散模型的 ELBO 函数，我们在 <a class="reference internal" href="#equation-eq-ddpm-010">公式(2.1.41)</a> 的基础上继续推演，
推导过程如下：</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-031">
<span class="eqno">(2.1.52)<a class="headerlink" href="#equation-eq-ddpm-031" title="此公式的永久链接"></a></span>\[\begin{split}\begin{align}
{\ln p(x_0)}
&amp;= {\ln \int p(x_{0:T}) dx_{1:T}}\\
&amp;= {\ln \int \frac{p(x_{0:T})q(x_{1:T}|x_0)}{q(x_{1:T}|x_0)} dx_{1:T}}\\
&amp;= {\ln \mathbb{E}_{q(x_{1:T}|x_0)}\left[\frac{p(x_{0:T})}{q(x_{1:T}|x_0)}\right]}\\
&amp;\geq {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_{0:T})}{q(x_{1:T}|x_0)}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)\prod_{t=1}^{T}p_{x{\theta}}(x_{t-1}|x_t)}{\prod_{t = 1}^{T}q(x_{t}|x_{t-1})}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)p_{x{\theta}}(x_0|x_1)\prod_{t=2}^{T}p_{x{\theta}}(x_{t-1}|x_t)}{q(x_T|x_{T-1})\prod_{t = 1}^{T-1}q(x_{t}|x_{t-1})}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)p_{x{\theta}}(x_0|x_1)\prod_{t=1}^{T-1}p_{x{\theta}}(x_{t}|x_{t+1})}{q(x_T|x_{T-1})\prod_{t = 1}^{T-1}q(x_{t}|x_{t-1})}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)p_{x{\theta}}(x_0|x_1)}{q(x_T|x_{T-1})}\right] + \mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \prod_{t = 1}^{T-1}\frac{p_{x{\theta}}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln p_{x{\theta}}(x_0|x_1)\right] + \mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)}{q(x_T|x_{T-1})}\right] + \mathbb{E}_{q(x_{1:T}|x_0)}\left[ \sum_{t=1}^{T-1} \ln \frac{p_{x{\theta}}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln p_{x{\theta}}(x_0|x_1)\right] + \mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)}{q(x_T|x_{T-1})}\right] + \sum_{t=1}^{T-1}\mathbb{E}_{q(x_{1:T}|x_0)}\left[ \ln \frac{p_{x{\theta}}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}\right]}\\
&amp;= \begin{aligned}[t]
    \mathbb{E}_{q(x_{1}|x_0)}\left[\ln p_{x{\theta}}(x_0|x_1)\right]
    &amp;+ \mathbb{E}_{q(x_{T-1}, x_T|x_0)}\left[\ln \frac{p(x_T)}{q(x_T|x_{T-1})}\right] \\
    &amp;+ \sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1}, x_t, x_{t+1}|x_0)}\left[\ln \frac{p_{x{\theta}}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}\right]\\
    \end{aligned}\\
&amp;=  \begin{aligned}[t]
      {\underbrace{\mathbb{E}_{q(x_{1}|x_0)}\left[\ln p_{\theta}(x_0|x_1)\right]}_\text{reconstruction term}}
      &amp;- {\underbrace{\mathbb{E}_{q(x_{T-1}|x_0)}\left[ \KL{q(x_T|x_{T-1})}{p(x_T)}\right]}_\text{prior matching term}} \\
      &amp;- {\sum_{t=1}^{T-1}\underbrace{\mathbb{E}_{q(x_{t-1}, x_{t+1}|x_0)}\left[ \KL {q(x_{t}|x_{t-1})}{p_{\theta}(x_{t}|x_{t+1})}\right]}_\text{consistency term}}
    \end{aligned}
\end{align}\end{split}\]</div>
<p>这个推导过程看上去很复杂，但最后的结果却很简洁，由3个子项构成，现在分别介绍一下各项的含义。</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_{q(x_{1}|x_0)}\left[\ln p_{\theta}(x_0|x_1)\right]\)</span> 可以被看做是重建项，
回顾一下原始 VAE 模型的 ELBO 函数，它和原始 VAE 的第一项是一样的，从第一步的隐变量 <span class="math notranslate nohighlight">\(x_1\)</span> 重建成原来的数据 <span class="math notranslate nohighlight">\(x_0\)</span>。</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_{q(x_{T-1}|x_0)}\left[\KL{q(x_T|x_{T-1})}{p(x_T)}\right]\)</span> 这一项是先验匹配项，
这一项和原始 VAE 也能对应上。稍微有点区别的是，这里这一项是没有学习参数的，<span class="math notranslate nohighlight">\(q(x_T|x_{T-1})\)</span> 是前向过程，
我们已经假设它是已知的线性高斯变化了，不需要再用模型学习了。当 <span class="math notranslate nohighlight">\(T\)</span> 足够大时，可以认为这一项就是 <span class="math notranslate nohighlight">\(0\)</span>。</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_{q(x_{t-1}, x_{t+1}|x_0)}\left[\KL{q(x_{t}|x_{t-1})}{p_{{\theta}}(x_{t}|x_{t+1})}\right]\)</span>
是个 KL 散度度量，这一项称为一致项（consistency term），显然它使得在每一个时刻 <span class="math notranslate nohighlight">\(t\)</span>，解码器 <span class="math notranslate nohighlight">\(p_{{\theta}}(x_{t}|x_{t+1})\)</span>
得到的 <span class="math notranslate nohighlight">\(x_t\)</span> 尽量和编码器 <span class="math notranslate nohighlight">\(q(x_{t}|x_{t-1})\)</span> 生成的内容保值一致。
<span class="math notranslate nohighlight">\(q(x_{t}|x_{t-1})\)</span> 是已知的线性高斯变换，不需要学习，而 <span class="math notranslate nohighlight">\(p_{{\theta}}(x_{t}|x_{t+1})\)</span>
是需要一个参数化的模型去学习的，也就是我们的模型学习的是 <span class="math notranslate nohighlight">\(p_{{\theta}}(x_{t}|x_{t+1})\)</span>，其中
<span class="math notranslate nohighlight">\(\theta\)</span> 表示模型需要学习的参数集合。</p></li>
</ul>
<figure class="align-center" id="id29">
<span id="fg-dm-006"></span><a class="reference internal image-reference" href="../_images/diffusion_first_derivation.png"><img alt="../_images/diffusion_first_derivation.png" src="../_images/diffusion_first_derivation.png" style="width: 554.2px; height: 217.60000000000002px;" /></a>
<figcaption>
<p><span class="caption-number">图 2.1.4 </span><span class="caption-text">根据 ELBO（ <a class="reference internal" href="#equation-eq-ddpm-031">公式(2.1.52)</a>）显示，对于每一个时刻 <span class="math notranslate nohighlight">\(t\)</span>，最小化绿色和粉色箭头所代表的分布之间的差异。（图片来自 <a class="footnote-reference brackets" href="#footcite-luo2022understanding" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>）</span><a class="headerlink" href="#id29" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>在极大化 <a class="reference internal" href="#equation-eq-ddpm-031">公式(2.1.52)</a> 时，第二项不包含任意可学习参数，因此可以忽略掉。
另外两项都是一个期望项，解决方法和 VAE 一样，可以通过采样法（MCMC）近似求解期望。
然而，第三项存在一个小问题，它的期望是关于两个变量的， <span class="math notranslate nohighlight">\((x_{t-1},x_{t+1})\)</span>，
用采样法（MCMC）同时对两个随机变量进行采样，会导致更大的方差，这会使得优化过程不稳定，不容易收敛 <a class="footnote-reference brackets" href="#footcite-luo2022understanding" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>。
因此直接优化 <a class="reference internal" href="#equation-eq-ddpm-031">公式(2.1.52)</a> 并不是最佳的选择，我们继续看能不能改进一下。</p>
<p>改进方法的推导中依赖几个等式替换，在推导前先给出这几个依赖等式。
首先根据条件独立性原则，有下式成立</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-032">
<span class="eqno">(2.1.53)<a class="headerlink" href="#equation-eq-ddpm-032" title="此公式的永久链接"></a></span>\[q(x_t | x_{t-1}) = q(x_t | x_{t-1}, x_0)\]</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>条件独立性</p>
<p>假设有三个随机变量，分别是 <span class="math notranslate nohighlight">\(A,B,C\)</span>，三者之间的依赖关系为(有向图表示) <span class="math notranslate nohighlight">\(A \to B \to C\)</span> ，
当变量 <span class="math notranslate nohighlight">\(B\)</span> 的值已知（确定）时，变量 <span class="math notranslate nohighlight">\(A\)</span> 和变量 <span class="math notranslate nohighlight">\(B\)</span> 是相互独立的，记作 <span class="math notranslate nohighlight">\(A \ind C \mid B\)</span>，
读作：在 <span class="math notranslate nohighlight">\(B\)</span> 的条件下变量 <span class="math notranslate nohighlight">\(A\)</span> 和变量 <span class="math notranslate nohighlight">\(C\)</span> 相互独立，
此时有 <span class="math notranslate nohighlight">\(P(C|B)=P(C|B,A)\)</span> 成立。</p>
</div>
<p><span class="math notranslate nohighlight">\(q(x_t | x_{t-1}, x_0)\)</span> 可以进一步根据贝叶斯定理求得，如下：</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-033">
<span class="eqno">(2.1.54)<a class="headerlink" href="#equation-eq-ddpm-033" title="此公式的永久链接"></a></span>\[q(x_t | x_{t-1}, x_0) = \frac{q(x_{t-1} \mid x_t, x_0)q(x_t \mid x_0)}{q(x_{t-1} \mid x_0)}\]</div>
<p>有了 <a class="reference internal" href="#equation-eq-ddpm-032">公式(2.1.53)</a> 和 <a class="reference internal" href="#equation-eq-ddpm-033">公式(2.1.54)</a>，
我再重新推导 ELBO 函数，推导过程如下：</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-035">
<span class="eqno">(2.1.55)<a class="headerlink" href="#equation-eq-ddpm-035" title="此公式的永久链接"></a></span>\[\begin{split}\begin{align}
{\ln p(x_0)}
&amp;\geq {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_{0:T})}{q(x_{1:T}|x_0)}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)\prod_{t=1}^{T}p_{{\theta}}(x_{t-1}|x_t)}{\prod_{t = 1}^{T}q(x_{t}|x_{t-1})}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)p_{{\theta}}(x_0|x_1)\prod_{t=2}^{T}p_{{\theta}}(x_{t-1}|x_t)}{q(x_1|x_0)\prod_{t = 2}^{T}q(x_{t}|x_{t-1})}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)p_{{\theta}}(x_0|x_1)\prod_{t=2}^{T}p_{{\theta}}(x_{t-1}|x_t)}{q(x_1|x_0)\prod_{t = 2}^{T}q(x_{t}|x_{t-1}, x_0)}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p_{{\theta}}(x_T)p_{{\theta}}(x_0|x_1)}{q(x_1|x_0)} + \ln \prod_{t=2}^{T}\frac{p_{{\theta}}(x_{t-1}|x_t)}{q(x_{t}|x_{t-1}, x_0)}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)p_{{\theta}}(x_0|x_1)}{q(x_1|x_0)} + \ln \prod_{t=2}^{T}\frac{p_{{\theta}}(x_{t-1}|x_t)}{\frac{p(x_{t-1}|x_{t}, x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)p_{{\theta}}(x_0|x_1)}{q(x_1|x_0)} + \ln \prod_{t=2}^{T}\frac{p_{{\theta}}(x_{t-1}|x_t)}{\frac{p(x_{t-1}|x_{t}, x_0)\cancel{q(x_t|x_0)}}{\cancel{q(x_{t-1}|x_0)}}}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)p_{{\theta}}(x_0|x_1)}{\cancel{q(x_1|x_0)}} + \ln \frac{\cancel{q(x_1|x_0)}}{q(x_T|x_0)} + \ln \prod_{t=2}^{T}\frac{p_{{\theta}}(x_{t-1}|x_t)}{q(x_{t-1}|x_{t}, x_0)}\right]}\\
&amp;= {\mathbb{E}_{q(x_{1:T}|x_0)}\left[\ln \frac{p(x_T)p_{{\theta}}(x_0|x_1)}{q(x_T|x_0)} +  \sum_{t=2}^{T}\ln\frac{p_{{\theta}}(x_{t-1}|x_t)}{q(x_{t-1}|x_{t}, x_0)}\right]}\\
&amp;=  \begin{aligned}[t]
     { \mathbb{E}_{ q(x_{1:T}|x_0) } \left[ \ln p_{{\theta}}(x_0|x_1) \right]\\
     \\+ \mathbb{E}_{ q(x_{1:T}|x_0)} \left[ \ln \frac{p(x_T)}{q(x_T|x_0)} \right ]\\
     \\+ \sum_{t=2}^{T} \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\frac{p_{{\theta}}(x_{t-1}|x_t)}{q(x_{t-1}|x_{t}, x_0)}\right] }\\
    \end{aligned}\\
&amp;= \begin{aligned}[t]{
    \mathbb{E}_{q(x_{1}|x_0)}\left[\ln p_{{\theta}}(x_0|x_1)\right]
    + \mathbb{E}_{q(x_{T}|x_0)}\left[\ln \frac{p(x_T)}{q(x_T|x_0)}\right] \\
    + \sum_{t=2}^{T}\mathbb{E}_{q(x_{t}, x_{t-1}|x_0)}\left[\ln\frac{p_{{\theta}}(x_{t-1}|x_t)}{q(x_{t-1}|x_{t}, x_0)}\right] \\
}\end{aligned} \\
&amp;= \begin{aligned}[t]
    \underbrace{\mathbb{E}_{q(x_{1}|x_0)}\left[\ln p_{{\theta}}(x_0|x_1)\right]}_\text{reconstruction term}
    &amp;- \underbrace{\KL{q(x_T|x_0)}{p(x_T)}}_\text{prior matching term}\\
    &amp;- \sum_{t=2}^{T} \underbrace{\mathbb{E}_{q(x_{t}|x_0)}\left[\KL{q(x_{t-1}|x_t, x_0)}{p_{{\theta}}(x_{t-1}|x_t)}\right]}_\text{denoising matching term}\\
    \end{aligned}
\end{align}\end{split}\]</div>
<p>对比一下 <a class="reference internal" href="#equation-eq-ddpm-035">公式(2.1.55)</a> 和 <a class="reference internal" href="#equation-eq-ddpm-031">公式(2.1.52)</a> 发生了一些变化，</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_{q(x_{1}|x_0)}\left[\ln p_{{\theta}}(x_0|x_1)\right]\)</span>，这一项没有变化。</p></li>
<li><p><span class="math notranslate nohighlight">\(\KL{q(x_T|x_0)}{p(x_T)}\)</span>，这一项发生变化，其中由 <span class="math notranslate nohighlight">\(q(x_T|x_{T-1})\)</span> 变成了 <span class="math notranslate nohighlight">\(q(x_T|x_0)\)</span>，
这其实没啥区别，都是约束最终的 <span class="math notranslate nohighlight">\(x_{T}\)</span> 要尽量接近先验 <span class="math notranslate nohighlight">\(p(x_T)\)</span> 的标准高斯分布。</p></li>
<li><p>关键是第3项的变化，需要积分（采样）的变量只剩一个了，这样就简单了很多。
其中 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span> 表示解码器的真实分布，带有 <span class="math notranslate nohighlight">\(\theta\)</span> 的 <span class="math notranslate nohighlight">\(p_{{\theta}}(x_{t-1}|x_t)\)</span>
表示参数化的模型，二者的 KL 散度正好表达了参数化模型 <span class="math notranslate nohighlight">\(p_{{\theta}}(x_{t-1}|x_t)\)</span> 要尽量和真实分布
<span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span> 接近，如图 <a class="reference internal" href="#fg-dm-007"><span class="std std-numref">图 2.1.5</span></a> 所示。</p></li>
</ul>
<figure class="align-center" id="id30">
<span id="fg-dm-007"></span><a class="reference internal image-reference" href="../_images/diffusion_second_derivation.png"><img alt="../_images/diffusion_second_derivation.png" src="../_images/diffusion_second_derivation.png" style="width: 554.2px; height: 257.1px;" /></a>
<figcaption>
<p><span class="caption-number">图 2.1.5 </span><span class="caption-text"><a class="reference internal" href="#equation-eq-ddpm-035">公式(2.1.55)</a> 对应的可视化图形表示。粉色箭头表示根据贝叶斯公式得到的逆过程中解码器的真实分布 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span>，
绿色箭头表示需要学习的近似分布 <span class="math notranslate nohighlight">\(p_{{\theta}}(x_{t-1}|x_t)\)</span>，
二者的 KL 散度表示必须领二者进尽可能的接近。（图片来自 <a class="footnote-reference brackets" href="#footcite-luo2022understanding" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>）</span><a class="headerlink" href="#id30" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>要想极大化 <a class="reference internal" href="#equation-eq-ddpm-035">公式(2.1.55)</a> 形式的 ELBO 函数，还需要其中每一项展开成具体的参数化公式，
第二项不包含参数 <span class="math notranslate nohighlight">\(\theta\)</span>，所以可以直接忽略。</p>
<p><strong>reconstruction term</strong></p>
<p>第一项 <span class="math notranslate nohighlight">\(p_{{\theta}}(x_0|x_1)\)</span> 和 VAE 中是解码器是类似的， 它是一个依赖 <span class="math notranslate nohighlight">\(x_1\)</span>
的条件高斯分布。 假设它的均值 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span> 是一个关于 <span class="math notranslate nohighlight">\(x_1\)</span> 和 <span class="math notranslate nohighlight">\(t=1\)</span> 的参数化函数，
记作 <span class="math notranslate nohighlight">\(\mu_{\theta}(x_1,t=1)\)</span>，根据以前的套路，我们可以用一个参数化的神经网络模型去拟合它。
和 VAE 不同的是，这里我们不再用模型去预测 <span class="math notranslate nohighlight">\(p_{{\theta}}(x_0|x_1)\)</span> 的方差，
而是假设它的方差是一个常量，暂时记作 <span class="math notranslate nohighlight">\(\Sigma\)</span>
。即模型输入 <span class="math notranslate nohighlight">\(x_1\)</span> 和 <span class="math notranslate nohighlight">\(t=1\)</span>，输出 <span class="math notranslate nohighlight">\(\mu_{\theta}(x_1,t=1)\)</span>。
根据高斯分布的概率密度函数，对 <span class="math notranslate nohighlight">\(\ln p(x_0|x_1)\)</span> 进行展开</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-035-1">
<span class="eqno">(2.1.56)<a class="headerlink" href="#equation-eq-ddpm-035-1" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\ln p(x_0|x_1) &amp;=\ln \frac{1}{(2\pi)^{n/2} |\Sigma |^{1/2}}exp\{-\frac{1}{2}(x_0 - \mu_{\theta}(x_1,t=1))^{T}\Sigma^{-1}(x - \mu_{\theta}(x_1,t=1))\}\\&amp; \propto -\frac{1}{2}(x_0 - \mu_{\theta}(x_1,t=1))^{T}(x_0 - \mu_{\theta}(x_1,t=1))\}\\&amp; = -\frac{1}{2} \left\lVert x_0 - \mu_{\theta}(x_1,t=1) \right\rVert_2^2\end{aligned}\end{align} \]</div>
<p>从中可以看出极大化 <span class="math notranslate nohighlight">\(\ln p(x_0|x_1)\)</span> 就等价于极小化模型输出 <span class="math notranslate nohighlight">\(\mu_{\theta}(x_1,t=1)\)</span>
和 <span class="math notranslate nohighlight">\(x_0\)</span> 的平方误差，模型输出其实就相当于在预测 <span class="math notranslate nohighlight">\(x_0\)</span> 的值，因此我们可以把模型输出
<span class="math notranslate nohighlight">\(\mu_{\theta}(x_1,t=1)\)</span> 用符号 <span class="math notranslate nohighlight">\(\hat x_{0}(x_1,t=1)\)</span> 来表示。</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-035-2">
<span class="eqno">(2.1.57)<a class="headerlink" href="#equation-eq-ddpm-035-2" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}&amp; {\operatorname{\arg\max}}_{\theta}
\mathbb{E}_{q(x_{1}|x_0)}\left[\ln p_{{\theta}}(x_0|x_1)\right]\\&amp;\Leftrightarrow  {\operatorname{\arg\max}}_{\theta} \  \mathbb{E}_{q(x_{1}|x_0)}
 \left [ -\frac{1}{2} \left\lVert x_0 - \hat x_{0}(x_1,t=1) \right\rVert_2^2 \right ]\\&amp;\Leftrightarrow   {\operatorname{\arg\min}}_{\theta} \  \mathbb{E}_{q(x_{1}|x_0)}
\left [ \left\lVert x_0 - \hat x_{0}(x_1,t=1) \right\rVert_2^2 \right ]\end{aligned}\end{align} \]</div>
<p>可以看到，这一项其实就相当于，我们用一个模型去预测样本 <span class="math notranslate nohighlight">\(x_0\)</span>，损失函数等价于 <strong>均方误差</strong>。
至于其中的期望项 <span class="math notranslate nohighlight">\(\mathbb{E}_{q(x_{1}|x_0)}\)</span> ，是由于模型的输入 <span class="math notranslate nohighlight">\(x_1\)</span> 是一个随机变量，
并不是一个数值变量，所以需要对随机变量 <span class="math notranslate nohighlight">\(x_1\)</span> 求期望，而求期望时随机变量 <span class="math notranslate nohighlight">\(x_1\)</span> 的概率分布用的是条件概率
分布 <span class="math notranslate nohighlight">\(q(x_{1}|x_0)\)</span> ，这个概率分布用 <a class="reference internal" href="#equation-eq-ddpm-023">公式(2.1.47)</a> 即可求得。
当然按照 VAE 相似的套路，我们可以通过采样法近似求解期望。
这里可以暂时先不关注这一项的具体实现，接着看后面的内容，在最后的表达式中，这一项其实可以和第三项合并在一起。</p>
<p><strong>prior matching term</strong></p>
<p>第二项是先验匹配项（prior matching term），它是一个 KL 散度，<span class="math notranslate nohighlight">\(q(x_T|x_0)\)</span> 是前向扩散过程生成的 <span class="math notranslate nohighlight">\(x_T\)</span>
的后验分布（有了观察样本 <span class="math notranslate nohighlight">\(x_0\)</span> 之后，所以是后验）；
<span class="math notranslate nohighlight">\(p(x_T)\)</span> 是逆向生成过程中 <span class="math notranslate nohighlight">\(x_T\)</span> 的先验分布。这一项就是约束 <span class="math notranslate nohighlight">\(x_T\)</span> 的后验分布要和先验分布尽量接近，
VAE 中也有类似的一项，起作用相当于正则项。不过在这里 <span class="math notranslate nohighlight">\(q(x_T|x_0)\)</span> 和 <span class="math notranslate nohighlight">\(p(x_T)\)</span>
都没有包含未知参数，所以在极大化 ELBO 时，这一项相当于常数项，可以忽略。</p>
<p><strong>denoising matching term</strong></p>
<p>这一项也是一个 KL 散度，KL 散度的两项都是高斯分布，
只要能找到这两个高斯分布的均值和方差，就可以解析计算它们的 KL 散度了。
我们先来推导下真实分布 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span>，
在 <a class="reference internal" href="#equation-eq-ddpm-033">公式(2.1.54)</a> 的基础上继续推导展开，注意 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span> 是条件概率分布，
其中 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> <strong>是未知量（分布的目标变量）</strong>，
<span class="math notranslate nohighlight">\(x_{t},x_0\)</span> 是 <strong>条件变量</strong>。</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-036">
<span class="eqno">(2.1.58)<a class="headerlink" href="#equation-eq-ddpm-036" title="此公式的永久链接"></a></span>\[\begin{split}\begin{align}
{q(x_{t-1}|x_t, x_0)}
&amp;= {\frac{q(x_t | x_{t-1}, x_0)q(x_{t-1}|x_0)}{q(x_{t}|x_0)}}\\
&amp;= {\frac{\mathcal{N}(x_{t} ; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t)\textit{I})\mathcal{N}(x_{t-1} ; \sqrt{\bar\alpha_{t-1}}x_0, (1 - \bar\alpha_{t-1}) \textit{I})}{\mathcal{N}(x_{t} ; \sqrt{\bar\alpha_{t}}x_0, (1 - \bar\alpha_{t})\textit{I})}}\\
&amp;\propto {\text{exp}\left\{-\left[\frac{(x_{t} - \sqrt{\alpha_t} x_{t-1})^2}{2(1 - \alpha_t)} + \frac{(x_{t-1} - \sqrt{\bar\alpha_{t-1}} x_0)^2}{2(1 - \bar\alpha_{t-1})} - \frac{(x_{t} - \sqrt{\bar\alpha_t} x_{0})^2}{2(1 - \bar\alpha_t)} \right]\right\}}\\
&amp;= {\text{exp}\left\{-\frac{1}{2}\left[\frac{(x_{t} - \sqrt{\alpha_t} x_{t-1})^2}{1 - \alpha_t} + \frac{(x_{t-1} - \sqrt{\bar\alpha_{t-1}} x_0)^2}{1 - \bar\alpha_{t-1}} - \frac{(x_{t} - \sqrt{\bar\alpha_t} x_{0})^2}{1 - \bar\alpha_t} \right]\right\}}\\
&amp;= {\text{exp}\left\{-\frac{1}{2}\left[\frac{(-2\sqrt{\alpha_t} x_{t}x_{t-1} + \alpha_t x_{t-1}^2)}{1 - \alpha_t} + \frac{(x_{t-1}^2 - 2\sqrt{\bar\alpha_{t-1}}x_{t-1} x_0)}{1 - \bar\alpha_{t-1}} + C(x_t, x_0)\right]\right\}} \\
&amp;\propto {\text{exp}\left\{-\frac{1}{2}\left[- \frac{2\sqrt{\alpha_t} x_{t}x_{t-1}}{1 - \alpha_t} + \frac{\alpha_t x_{t-1}^2}{1 - \alpha_t} + \frac{x_{t-1}^2}{1 - \bar\alpha_{t-1}} - \frac{2\sqrt{\bar\alpha_{t-1}}x_{t-1} x_0}{1 - \bar\alpha_{t-1}}\right]\right\}}\\
&amp;= {\text{exp}\left\{-\frac{1}{2}\left[(\frac{\alpha_t}{1 - \alpha_t} + \frac{1}{1 - \bar\alpha_{t-1}})x_{t-1}^2 - 2\left(\frac{\sqrt{\alpha_t}x_{t}}{1 - \alpha_t} + \frac{\sqrt{\bar\alpha_{t-1}}x_0}{1 - \bar\alpha_{t-1}}\right)x_{t-1}\right]\right\}}\\
&amp;= {\text{exp}\left\{-\frac{1}{2}\left[\frac{\alpha_t(1-\bar\alpha_{t-1}) + 1 - \alpha_t}{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}x_{t-1}^2 - 2\left(\frac{\sqrt{\alpha_t}x_{t}}{1 - \alpha_t} + \frac{\sqrt{\bar\alpha_{t-1}}x_0}{1 - \bar\alpha_{t-1}}\right)x_{t-1}\right]\right\}}\\
&amp;= {\text{exp}\left\{-\frac{1}{2}\left[\frac{\alpha_t-\bar\alpha_{t} + 1 - \alpha_t}{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}x_{t-1}^2 - 2\left(\frac{\sqrt{\alpha_t}x_{t}}{1 - \alpha_t} + \frac{\sqrt{\bar\alpha_{t-1}}x_0}{1 - \bar\alpha_{t-1}}\right)x_{t-1}\right]\right\}}\\
&amp;= {\text{exp}\left\{-\frac{1}{2}\left[\frac{1 -\bar\alpha_{t}}{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}x_{t-1}^2 - 2\left(\frac{\sqrt{\alpha_t}x_{t}}{1 - \alpha_t} + \frac{\sqrt{\bar\alpha_{t-1}}x_0}{1 - \bar\alpha_{t-1}}\right)x_{t-1}\right]\right\}}\\
&amp;= {\text{exp}\left\{-\frac{1}{2}\left(\frac{1 -\bar\alpha_{t}}{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}\right)\left[x_{t-1}^2 - 2\frac{\left(\frac{\sqrt{\alpha_t}x_{t}}{1 - \alpha_t} + \frac{\sqrt{\bar\alpha_{t-1}}x_0}{1 - \bar\alpha_{t-1}}\right)}{\frac{1 -\bar\alpha_{t}}{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}}x_{t-1}\right]\right\}}\\
&amp;= {\text{exp}\left\{-\frac{1}{2}\left(\frac{1 -\bar\alpha_{t}}{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}\right)\left[x_{t-1}^2 - 2\frac{\left(\frac{\sqrt{\alpha_t}x_{t}}{1 - \alpha_t} + \frac{\sqrt{\bar\alpha_{t-1}}x_0}{1 - \bar\alpha_{t-1}}\right)(1 - \alpha_t)(1 - \bar\alpha_{t-1})}{1 -\bar\alpha_{t}}x_{t-1}\right]\right\}}\\
&amp;= {\text{exp}\left\{-\frac{1}{2}\left(\frac{1}{\frac{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}{1 -\bar\alpha_{t}}}\right)\left[x_{t-1}^2 - 2\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}x_{t-1}\right]\right\}}\\
&amp;\propto {\mathcal{N}(x_{t-1} ;} \underbrace{{\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}}}_{\mu_q(x_t, x_0)}, \underbrace{{\frac{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}{1 -\bar\alpha_{t}}\textit{I}}}_{{\Sigma}_q(t)})
\end{align}\end{split}\]</div>
<p>上面推导过程中的 <span class="math notranslate nohighlight">\(C(x_t, x_0)\)</span> 是与目标变量 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> 无关的项，它仅仅与 <span class="math notranslate nohighlight">\(x_0,x_{t},\alpha\)</span> 有关，
所以可以把它看做是常量，可以不关注它。
总之最后的结果是 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span> 同样是一个高斯分布，它均值记作 <span class="math notranslate nohighlight">\(\mu_q(x_t, x_0)\)</span>，是一个关于 <span class="math notranslate nohighlight">\(x_t, x_0\)</span> 的函数。
方差记作 <span class="math notranslate nohighlight">\(\Sigma_q(t)\)</span>，仅与 <span class="math notranslate nohighlight">\(\alpha\)</span> 有关，如果把 <span class="math notranslate nohighlight">\(\alpha\)</span> 当做一个超参数，那么显然方差是可以直接计算的。</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-037">
<span class="eqno">(2.1.59)<a class="headerlink" href="#equation-eq-ddpm-037" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mu_q(x_t, x_0) &amp;= { \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}}\\\Sigma_q(t) &amp;= \frac{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}{ 1 -\bar\alpha_{t}}  \textit{I} = \sigma_q^2(t)   \textit{I}\end{aligned}\end{align} \]</div>
<p>我们的参数化模型分布 <span class="math notranslate nohighlight">\(p_\theta(x_{t-1}|x_t)\)</span>，需要尽可能的接近真实高斯分布 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span>
，显然 <span class="math notranslate nohighlight">\(p_\theta(x_{t-1}|x_t)\)</span> 也需要是一个高斯分布才行，我们用 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span> 表示它的均值，
<span class="math notranslate nohighlight">\(\Sigma_\theta\)</span> 表示的方差。
真实分布 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span> 的方差 <span class="math notranslate nohighlight">\(\Sigma_q(t)\)</span> 是一个仅与 <span class="math notranslate nohighlight">\(\alpha\)</span> 有关的量，
<strong>在本论文中把它看做一个不需要模型学习的常量</strong>，直接假设 <span class="math notranslate nohighlight">\(\Sigma_\theta=\Sigma_q(t)\)</span> 即可。
在本论文中，作者认为这个方差学习与否，最终效果差异不大，所以就没有用模型去学习。
然而后来，有其他学者发布新的论文，提出了学习方差的改进方法，认为学习方差效果会更好。
后面的章节会详细介绍这个改进方法，这里先假设方差是不需要模型学习的常量即可。</p>
<div class="math notranslate nohighlight" id="equation-aigc-5">
<span class="eqno">(2.1.60)<a class="headerlink" href="#equation-aigc-5" title="此公式的永久链接"></a></span>\[p_{\theta} (x_{t-1}|x_t) \sim \mathcal{N}(x_{t-1} ;\mu_{\theta},\Sigma_q(t))\]</div>
<p>现在，真实分布 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span> 和参数化模型学习分布 <span class="math notranslate nohighlight">\(p_\theta(x_{t-1}|x_t)\)</span> 各自的均值和方差表达式都确定了，
又已知两个高斯分布的 KL 散度计算公式如下：</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-038">
<span class="eqno">(2.1.61)<a class="headerlink" href="#equation-eq-ddpm-038" title="此公式的永久链接"></a></span>\[\begin{align}
\KL{\mathcal{N}(x; {\mu}_x,{\Sigma}_x)}{\mathcal{N}(y; {\mu}_y,{\Sigma}_y)}
&amp;=\frac{1}{2}\left[\log\frac{|{\Sigma}_y|}{|{\Sigma}_x|} - d + \text{tr}({\Sigma}_y^{-1}{\Sigma}_x)
+ ({\mu}_y-{\mu}_x)^T {\Sigma}_y^{-1} ({\mu}_y-{\mu}_x)\right]
\end{align}\]</div>
<p>根据这个公式，可以得到真实分布 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span> 和参数化学习分布 <span class="math notranslate nohighlight">\(p_\theta(x_{t-1}|x_t)\)</span>
的 KL 散度为</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-039">
<span class="eqno">(2.1.62)<a class="headerlink" href="#equation-eq-ddpm-039" title="此公式的永久链接"></a></span>\[\begin{split}\begin{align}
&amp; \quad \,\KL{q(x_{t-1}|x_t, x_0)}{p_{{\theta}}(x_{t-1}|x_t)} \\
&amp;= \KL{\mathcal{N}(x_{t-1}; {\mu}_q,{\Sigma}_q(t))}{\mathcal{N}(x_{t-1}; {\mu}_{{\theta}},{\Sigma}_q(t))}\\
&amp;=\frac{1}{2}\left[\log\frac{|{\Sigma}_q(t)|}{|{\Sigma}_q(t)|} - d + \text{tr}({\Sigma}_q(t)^{-1}{\Sigma}_q(t))
+ ({\mu}_{{\theta}}-{\mu}_q)^T {\Sigma}_q(t)^{-1} ({\mu}_{{\theta}}-{\mu}_q)\right]\\
&amp;=\frac{1}{2}\left[\log1 - d + d + ({\mu}_{{\theta}}-{\mu}_q)^T {\Sigma}_q(t)^{-1} ({\mu}_{{\theta}}-{\mu}_q)\right]\\
&amp;=\frac{1}{2}\left[({\mu}_{{\theta}}-{\mu}_q)^T {\Sigma}_q(t)^{-1} ({\mu}_{{\theta}}-{\mu}_q)\right]\\
&amp;=\frac{1}{2}\left[({\mu}_{{\theta}}-{\mu}_q)^T \left(\sigma_q^2(t)\textbf{I}\right)^{-1} ({\mu}_{{\theta}}-{\mu}_q)\right]\\
&amp;=\frac{1}{2\sigma_q^2(t)}\left[\left\lVert{\mu}_{{\theta}}-{\mu}_q\right\rVert_2^2\right]
\end{align}\end{split}\]</div>
<p>极大化 ELBO 函数，等同于极小化 <span class="math notranslate nohighlight">\(\KL{q(x_{t-1}|x_t, x_0)}{p_{{\theta}}(x_{t-1}|x_t)}\)</span>
，又相当于极小化 <span class="math notranslate nohighlight">\(\left[\left\lVert{\mu}_{{\theta}}-{\mu}_q\right\rVert_2^2\right]\)</span>
，又是一个均方误差。 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span> 是参数化模型学习的分布 <span class="math notranslate nohighlight">\(p_\theta(x_{t-1}|x_t)\)</span> 的均值，
也是我们的模型需要预测（输出）的值，
显然 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span> 是和 <span class="math notranslate nohighlight">\(x_t\)</span> 相关的，即它肯定是 <span class="math notranslate nohighlight">\(x_t\)</span> 的一个函数，
也就是说我们的参数化模型输入 <span class="math notranslate nohighlight">\(x_t\)</span> ，并且输出 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span>。
前面我们已经推到出 <span class="math notranslate nohighlight">\({\mu}_q\)</span> 为</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-040">
<span class="eqno">(2.1.63)<a class="headerlink" href="#equation-eq-ddpm-040" title="此公式的永久链接"></a></span>\[\mu_q(x_t, x_0) = { \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}}\]</div>
<p>从中可以看出 <span class="math notranslate nohighlight">\({\mu}_q\)</span> 本身也是一个关于 <span class="math notranslate nohighlight">\(x_t\)</span> 的函数，
我们的目标是使得 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span> 和 <span class="math notranslate nohighlight">\({\mu}_q\)</span> 尽量接近，
那么我们完全可以把 <span class="math notranslate nohighlight">\({\mu}_{\theta}\)</span> <strong>人为定义成类似</strong>  <span class="math notranslate nohighlight">\({\mu}_q\)</span> （<a class="reference internal" href="#equation-eq-ddpm-040">公式(2.1.63)</a>）
的计算方式，即</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-041">
<span class="eqno">(2.1.64)<a class="headerlink" href="#equation-eq-ddpm-041" title="此公式的永久链接"></a></span>\[\mu_{\theta}={\mu}_{{\theta}}(x_t, t) = \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\hat x_{{\theta}}(x_t, t)}{1 -\bar\alpha_{t}}\]</div>
<p>模型不再直接预测 <span class="math notranslate nohighlight">\({\mu}_q\)</span> ，而是预测输出 <span class="math notranslate nohighlight">\(\hat x_{{\theta}}(x_t, t)\)</span> ，
然后利用公式 <a class="reference internal" href="#equation-eq-ddpm-041">公式(2.1.64)</a> 计算得到 <span class="math notranslate nohighlight">\({\mu}_q\)</span> 的预测值，
这样一来，<span class="math notranslate nohighlight">\(\left\lVert{\mu}_{{\theta}}-{\mu}_q\right\rVert_2^2\)</span>
可以简化成</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-042">
<span class="eqno">(2.1.65)<a class="headerlink" href="#equation-eq-ddpm-042" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}&amp; \quad \left\lVert{\mu}_{{\theta}}-{\mu}_q \right\rVert_2^2\\&amp;= \left\lVert  \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\hat x_{{\theta}}(x_t, t)}{1 -\bar\alpha_{t}}
-    { \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}}
\right\rVert_2^2\\&amp;= \left\lVert  \frac{ \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\hat x_{{\theta}}(x_t, t)}{1 -\bar\alpha_{t}}
-    { \frac{\sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}}
\right\rVert_2^2\\&amp;= \left\lVert  \frac{ \sqrt{\bar\alpha_{t-1}}(1-\alpha_t) }  {1 -\bar\alpha_{t}}
 ( \hat x_{{\theta}}(x_t, t)   -    x_0 )
\right\rVert_2^2\\&amp;=  \frac{ \sqrt{\bar\alpha_{t-1}}(1-\alpha_t) }  {1 -\bar\alpha_{t}}
\left\lVert  ( \hat x_{{\theta}}(x_t, t) -  x_0 ) \right\rVert_2^2\end{aligned}\end{align} \]</div>
<p>对于 <span class="math notranslate nohighlight">\(2\le t \le T\)</span> 的每一时刻 <span class="math notranslate nohighlight">\(t\)</span>，
我们的参数化模型输入为 <span class="math notranslate nohighlight">\(x_t, t\)</span> ，
预测输出 <span class="math notranslate nohighlight">\(\hat x_{{\theta}}(x_t, t)\)</span>，
我们要最小化这个输出值和 <span class="math notranslate nohighlight">\(x_0\)</span> 的平方误差。
极大化 ELBO 函数（ <a class="reference internal" href="#equation-eq-ddpm-035">公式(2.1.55)</a> ）中的第3项的负的 KL 散度，
就等价于极小化模型输出和 <span class="math notranslate nohighlight">\(x_0\)</span> 的平方误差。</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-043">
<span class="eqno">(2.1.66)<a class="headerlink" href="#equation-eq-ddpm-043" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}&amp; {\operatorname{\arg\max}}_{\theta} \left[ -\sum_{t=2}^{T}
    \mathbb{E}_{q(x_{t}|x_0)} \left[\KL{q(x_{t-1}|x_t, x_0)}{p_{{\theta}}(x_{t-1}|x_t)}\right]
    \right ]\\&amp; \Leftrightarrow {\operatorname{\arg\min}}_{\theta}  \left [
   \sum_{t=2}^{T}
    \frac{1}{2\sigma_q^2(t)}  \frac{ \sqrt{\bar\alpha_{t-1}}(1-\alpha_t) }  {1 -\bar\alpha_{t}}
    \mathbb{E}_{q(x_{t}|x_0)}  \left [ \left\lVert  ( \hat x_{{\theta}}(x_t, t) -  x_0 ) \right\rVert_2^2 \right ] \right ]\\
&amp; \Leftrightarrow {\operatorname{\arg\min}}_{\theta}  \left [
   \sum_{t=2}^{T}
    \mathbb{E}_{q(x_{t}|x_0)}  \left [ \left\lVert  ( \hat x_{{\theta}}(x_t, t) -  x_0 ) \right\rVert_2^2 \right ] \right ]\end{aligned}\end{align} \]</div>
<p><strong>最终的目标函数</strong></p>
<p>最后我们整合一下 <a class="reference internal" href="#equation-eq-ddpm-035-2">公式(2.1.57)</a> 和 <a class="reference internal" href="#equation-eq-ddpm-043">公式(2.1.66)</a>
就得到了最终的目标函数，可以看到最终的目标函数其实非常简洁，
对任意 <span class="math notranslate nohighlight">\(t \in [1,T]\)</span>，
参数化模型输入 <span class="math notranslate nohighlight">\(x_t\)</span> 和 <span class="math notranslate nohighlight">\(t\)</span>，输出 <span class="math notranslate nohighlight">\(\hat x_{{\theta}}(x_t, t)\)</span>
，模型的目标是优化预测值 <span class="math notranslate nohighlight">\(\hat x_{{\theta}}(x_t, t)\)</span> 和真实值 <span class="math notranslate nohighlight">\(x_0\)</span> 之间的平方误差。
公式中期望 <span class="math notranslate nohighlight">\(\mathbb{E}_{q(x_{t}|x_0)}\)</span> 的解决方法和 VAE 中一样，随机采样 <span class="math notranslate nohighlight">\(x_t\)</span>
即可，采样次数记为 <span class="math notranslate nohighlight">\(L\)</span> 次，<span class="math notranslate nohighlight">\(L\)</span> 可以设为一个超参数，从经验上看 <span class="math notranslate nohighlight">\(L=1\)</span>
其实就可以了。至于如何采样 <span class="math notranslate nohighlight">\(x_t\)</span> 的值，显然在前向过程中通过 <a class="reference internal" href="#equation-eq-ddpm-025">公式(2.1.49)</a>
可以直接计算得到。</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-044">
<span class="eqno">(2.1.67)<a class="headerlink" href="#equation-eq-ddpm-044" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}    &amp; {\operatorname{\arg\max}}_{\theta} \quad \text{ELBO}\\    &amp; \Leftrightarrow {\operatorname{\arg\max}}_{\theta} \left [
          \mathbb{E}_{q(x_{1}|x_0)}\left[\ln p_{{\theta}}(x_0|x_1)\right]
        - \sum_{t=2}^{T} \mathbb{E}_{q(x_{t}|x_0)}\left[\KL{q(x_{t-1}|x_t, x_0)}{p_{{\theta}}(x_{t-1}|x_t)}\right]
        \right]\\    &amp;\Leftrightarrow  {\operatorname{\arg\min}}_{\theta}
    \left[
        \mathbb{E}_{q(x_{1}|x_0)} \left [ \left\lVert x_0 - \hat x_{0}(x_1,t=1) \right\rVert_2^2 \right ]
    \right ]
    + \left [
    \sum_{t=2}^{T}
    \mathbb{E}_{q(x_{t}|x_0)}   \left [ \left\lVert( \hat x_{{\theta}}(x_t, t) - x_0 )  \right\rVert_2^2 \right ]
    \right ]\\    &amp;  \Leftrightarrow  {\operatorname{\arg\min}}_{\theta}
     \sum_{t=1}^{T} \mathbb{E}_{q(x_{t}|x_0)}   \left [ \left\lVert( \hat x_{{\theta}}(x_t, t) - x_0 )  \right\rVert_2^2 \right ]\end{aligned}\end{align} \]</div>
<p>在这个变分扩散模型中，前向过程不需要学习，模型只学习解码分布 <span class="math notranslate nohighlight">\(p_{\theta}(x_{t-1}|x_t)\)</span>。
在模训练阶段前向过程和逆向过程都要计算，而在模型推理（应用）阶段，不需要前向过程，仅有逆向即可。
在推理时，输入一个随机高斯噪声数据，然后迭代（循环）执行 <span class="math notranslate nohighlight">\(p_{\theta}(x_{t-1}|x_t)\)</span>
最终还原成（生成）一个新的有效图片。</p>
</section>
<section id="id14">
<h3><span class="section-number">2.1.5. </span>图片生成（采样）过程<a class="headerlink" href="#id14" title="此标题的永久链接"></a></h3>
<p>训练完成的神经网络模型为 <span class="math notranslate nohighlight">\(\hat{x}_{\theta}(x_t, t)\)</span>
，它的输入 <span class="math notranslate nohighlight">\(x_t\)</span> 和 <span class="math notranslate nohighlight">\(t\)</span>
，输出是对 <span class="math notranslate nohighlight">\(x_0\)</span> 的预测。有个它之后，就可以得到逆过程中的条件概率分布 <span class="math notranslate nohighlight">\(p(x_{t-1}|x_{t})\)</span>
的一个近似表示。</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-045">
<span class="eqno">(2.1.68)<a class="headerlink" href="#equation-eq-ddpm-045" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}p(x_{t-1}|x_{t})
&amp;\approx p_{\theta} (x_{t-1}|x_{t})\\&amp;\approx  q(x_{t-1}|x_t, x_0)\\&amp;\sim  \mathcal{N} (x_{t-1} ; \mu_q(x_t, x_0) , \Sigma_q(t) )\end{aligned}\end{align} \]</div>
<p>其中，均值 <span class="math notranslate nohighlight">\(\mu_q(x_t, x_0)\)</span> 和 方差 <span class="math notranslate nohighlight">\(\Sigma_q(t)\)</span> 分别为</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-046">
<span class="eqno">(2.1.69)<a class="headerlink" href="#equation-eq-ddpm-046" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mu_q(x_t, x_0) &amp;= { \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t) \hat{x}_{\theta}(x_t, t) }{1 -\bar\alpha_{t}}}\\\Sigma_q(t) &amp;= \frac{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}{ 1 -\bar\alpha_{t}}  \textit{I} = \sigma_q^2(t)   \textit{I}\end{aligned}\end{align} \]</div>
<p>逆向生成（采样）过程可以简述为</p>
<ol class="arabic simple">
<li><p>设定 <span class="math notranslate nohighlight">\(T=1000\)</span>。</p></li>
<li><p>从一个标准高斯分布中采样得到 <span class="math notranslate nohighlight">\(x_T\)</span>。</p></li>
<li><p>利用神经网络模型计算 <span class="math notranslate nohighlight">\(\hat{x}_{\theta}(x_t, t)\)</span></p></li>
<li><p>计算出 <span class="math notranslate nohighlight">\(p(x_{t-1}|x_{t})\)</span> 的均值 <span class="math notranslate nohighlight">\(\mu_q(x_t, x_0)\)</span> 和方差 <span class="math notranslate nohighlight">\(\Sigma_q(t)\)</span>。</p></li>
<li><p>从 <span class="math notranslate nohighlight">\(p(x_{t-1}|x_{t})\)</span> 随机采样的到 <span class="math notranslate nohighlight">\(x_{t-1}\)</span></p></li>
<li><p>重复步骤3~5，直到 <span class="math notranslate nohighlight">\(t=1\)</span> 。</p></li>
</ol>
<p>在每一个时刻 <span class="math notranslate nohighlight">\(t\)</span> 模型预测都是原始的 <span class="math notranslate nohighlight">\(x_0\)</span>
，从直觉上讲，感觉就不是很合理。当 <span class="math notranslate nohighlight">\(t\)</span> 比较大时，<span class="math notranslate nohighlight">\(x_t\)</span> 距离 <span class="math notranslate nohighlight">\(x_0\)</span> 应该是比较远的，
虽然表示时刻的 <span class="math notranslate nohighlight">\(t\)</span> 也作为模型的输入，但同样的模型参数要同时解决不同的 <span class="math notranslate nohighlight">\(t\)</span>，其实是比较困难的。
事实也证明了 DPM 效果确实不是很靠谱，生成的图像质量通常并不高，
在论文<a class="footnote-reference brackets" href="#footcite-ho2020denoising" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> 中做了相应的对比实现，证明了这个这个结论。
所以虽然 DPM 在 2015年就提出了，但一直没有引发热度，
直到2020年，Ho et al 等人发表了论文 DPM 的改进模型 “Denoising Diffusion Probabilistic Models (DDPMs)”
，进一步 OPEN AI  发布了基于 DDPM 的 DALL-E ，DALL-E 的效果是非常显著的，这才是得扩散模型得到足够的关注。
下一节讨论 DDPM 做了哪些关键的改进。</p>
</section>
</section>
<section id="denoising-diffusion-probabilistic-model-ddpm">
<h2><span class="section-number">2.2. </span>降噪扩散概率模型（Denoising diffusion probabilistic model,DDPM）<a class="headerlink" href="#denoising-diffusion-probabilistic-model-ddpm" title="此标题的永久链接"></a></h2>
<p>在 2020年 Ho et al 等人发表了论文 “Denoising Diffusion Probabilistic Models (DDPMs)”
，DDPM 对 DPM 做了关键的改进和优化，解决了 DPM 的一些不足，使得扩散模型生成的图像质量得到了大幅提升，
这才使得扩散模型在图像生成领域大放异彩。
DDPM 做的关键改进就是参数化模型预测的内容做了调整：</p>
<ul class="simple">
<li><p>不再是预测原始的 <span class="math notranslate nohighlight">\(x_0\)</span>，而是预测每一个时刻添加的噪声，降低了模型的学习难度。</p></li>
</ul>
<p>回顾下 <a class="reference internal" href="#equation-eq-ddpm-025">公式(2.1.49)</a>，在前向过程中，我们得到 <span class="math notranslate nohighlight">\(q(x_t|x_0)\)</span>
的分布为：</p>
<div class="math notranslate nohighlight" id="equation-aigc-6">
<span class="eqno">(2.2.56)<a class="headerlink" href="#equation-aigc-6" title="此公式的永久链接"></a></span>\[q(x_t|x_0) \sim \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t } \ x_0,  (1- \bar{ \alpha}_t)    \textit{I})\]</div>
<p><span class="math notranslate nohighlight">\(x_t\)</span> 的采样过程为</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-051">
<span class="eqno">(2.2.57)<a class="headerlink" href="#equation-eq-ddpm-051" title="此公式的永久链接"></a></span>\[x_t = \sqrt{\bar{\alpha}_t } \ x_0 + \sqrt{1- \bar{ \alpha}_t }  \ \epsilon_t  \ \ \ ,
\bar{\alpha}_t = \prod_{i=1}^t \alpha_i ,\ \ \epsilon_t \sim \mathcal{N}(0,\textit{I})\]</div>
<p><a class="reference internal" href="#equation-eq-ddpm-051">公式(2.2.57)</a> 表达了从 <span class="math notranslate nohighlight">\(x_0\)</span> 计算 <span class="math notranslate nohighlight">\(x_t\)</span> 的关系，
现在可以把它反过来，用 <span class="math notranslate nohighlight">\(x_t\)</span> 得到 <span class="math notranslate nohighlight">\(x_0\)</span>。</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-052">
<span class="eqno">(2.2.58)<a class="headerlink" href="#equation-eq-ddpm-052" title="此公式的永久链接"></a></span>\[x_0 =  \frac{x_t -\sqrt{1- \bar{ \alpha}_t }  \ \epsilon_t }{ \sqrt{\bar{\alpha}_t }  }
,\ \ \epsilon_t \sim \mathcal{N}(0,\textit{I})\]</div>
<p>我们在 <a class="reference internal" href="#equation-eq-ddpm-039">公式(2.1.62)</a> 的基础上，重新推导下 ELBO 中第三项 KL 散度，
为方便阅读，把 <a class="reference internal" href="#equation-eq-ddpm-039">公式(2.1.62)</a> 复制到这里。</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-053">
<span class="eqno">(2.2.59)<a class="headerlink" href="#equation-eq-ddpm-053" title="此公式的永久链接"></a></span>\[\KL{q(x_{t-1}|x_t, x_0)}{p_{{\theta}}(x_{t-1}|x_t)}
=\frac{1}{2\sigma_q^2(t)}\left[\left\lVert{\mu}_{{\theta}}-{\mu}_q\right\rVert_2^2\right]\]</div>
<p>其中 <span class="math notranslate nohighlight">\({\mu}_q\)</span> 代表逆过程真实分布 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span> 的均值，
<span class="math notranslate nohighlight">\(\mu_{\theta}\)</span> 代表需要学习的近似逆过程分布 <span class="math notranslate nohighlight">\(p_{{\theta}}(x_{t-1}|x_t)\)</span>
的均值，同时也是我们的参数化模型的预测输出值。
假定 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span> 与 <span class="math notranslate nohighlight">\(p_{{\theta}}(x_{t-1}|x_t)\)</span>
的方差是相同的，并且都是 <span class="math notranslate nohighlight">\(\sigma_q^2(t)\)</span>。
这次利用 <a class="reference internal" href="#equation-eq-ddpm-052">公式(2.2.58)</a> 对 <span class="math notranslate nohighlight">\({\mu}_q\)</span> 重新推导</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-054">
<span class="eqno">(2.2.60)<a class="headerlink" href="#equation-eq-ddpm-054" title="此公式的永久链接"></a></span>\[\begin{split}\begin{align}
{\mu}_q &amp;= {\mu}_q(x_t, x_0) = \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}\\
&amp;= \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\frac{x_t - \sqrt{1 - \bar\alpha_t}\epsilon }{\sqrt{\bar\alpha_t}}}{1 -\bar\alpha_{t}}\\
&amp;= \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + (1-\alpha_t)\frac{x_t - \sqrt{1 - \bar\alpha_t}\epsilon }{\sqrt{\alpha_t}}}{1 -\bar\alpha_{t}}\\
&amp;= \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t}}{1 - \bar\alpha_t} + \frac{(1-\alpha_t)x_t}{(1-\bar\alpha_t)\sqrt{\alpha_t}} - \frac{(1 - \alpha_t)\sqrt{1 - \bar\alpha_t}\epsilon }{(1-\bar\alpha_t)\sqrt{\alpha_t}}\\
&amp;= \left(\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1 - \bar\alpha_t} + \frac{1-\alpha_t}{(1-\bar\alpha_t)\sqrt{\alpha_t}}\right)x_t - \frac{(1 - \alpha_t)\sqrt{1 - \bar\alpha_t}}{(1-\bar\alpha_t)\sqrt{\alpha_t}}\epsilon \\
&amp;= \left(\frac{\alpha_t(1-\bar\alpha_{t-1})}{(1 - \bar\alpha_t)\sqrt{\alpha_t}} + \frac{1-\alpha_t}{(1-\bar\alpha_t)\sqrt{\alpha_t}}\right)x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}\sqrt{\alpha_t}}\epsilon \\
&amp;= \frac{\alpha_t-\bar\alpha_{t} + 1-\alpha_t}{(1 - \bar\alpha_t)\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}\sqrt{\alpha_t}}\epsilon \\
&amp;= \frac{1-\bar\alpha_t}{(1 - \bar\alpha_t)\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}\sqrt{\alpha_t}}\epsilon \\
&amp;= \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}\sqrt{\alpha_t}}\epsilon
\end{align}\end{split}\]</div>
<p>接下，把模型的预测输出 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span>
，也按照这个形式进行参数化</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-055">
<span class="eqno">(2.2.61)<a class="headerlink" href="#equation-eq-ddpm-055" title="此公式的永久链接"></a></span>\[\mu_{\theta}={\mu}_{{\theta}}(x_t, t) = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}\sqrt{\alpha_t}} {\hat\epsilon}_{ {\theta}}(x_t, t)\]</div>
<p>令 <span class="math notranslate nohighlight">\({\hat\epsilon}_{ {\theta}}(x_t, t)\)</span> 表示模型的预测输出，然后通过 <a class="reference internal" href="#equation-eq-ddpm-055">公式(2.2.61)</a>
计算得到 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span>。
把 <a class="reference internal" href="#equation-eq-ddpm-054">公式(2.2.60)</a> 和 <a class="reference internal" href="#equation-eq-ddpm-055">公式(2.2.61)</a> 一起带入 <a class="reference internal" href="#equation-eq-ddpm-053">公式(2.2.59)</a></p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-056">
<span class="eqno">(2.2.62)<a class="headerlink" href="#equation-eq-ddpm-056" title="此公式的永久链接"></a></span>\[\begin{split}\begin{align}
&amp;  \KL{q(x_{t-1}|x_t, x_0)}{p_{{\theta}}(x_{t-1}|x_t)} \\
&amp;=\frac{1}{2\sigma_q^2(t)}\left[\left\lVert{\mu}_{{\theta}}-{\mu}_q\right\rVert_2^2\right]\\
&amp;= \frac{1}{2\sigma_q^2(t)}\left[\left\lVert\frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}\sqrt{\alpha_t}}{\hat\epsilon}_{{\theta}}(x_t, t) -
\frac{1}{\sqrt{\alpha_t}}x_t + \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}\sqrt{\alpha_t}}\epsilon \right\rVert_2^2\right]\\
&amp;= \frac{1}{2\sigma_q^2(t)}\left[\left\lVert \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}\sqrt{\alpha_t}}\epsilon  - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}\sqrt{\alpha_t}}{\hat\epsilon}_{{\theta}}(x_t, t)\right\rVert_2^2\right]\\
&amp;= \frac{1}{2\sigma_q^2(t)}\left[\left\lVert \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}\sqrt{\alpha_t}}(\epsilon  - {\hat\epsilon}_{{\theta}}(x_t, t))\right\rVert_2^2\right]\\
&amp;= \frac{1}{2\sigma_q^2(t)}\frac{(1 - \alpha_t)^2}{(1 - \bar\alpha_t)\alpha_t}\left[\left\lVert\epsilon  - {\hat\epsilon}_{{\theta}}(x_t, t)\right\rVert_2^2\right]
\end{align}\end{split}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\epsilon\)</span> 表示从时刻 <span class="math notranslate nohighlight">\(t-1\)</span> 到时刻 <span class="math notranslate nohighlight">\(t\)</span> 的前向过程中，
按照公式 <a class="reference internal" href="#equation-eq-ddpm-024">公式(2.1.48)</a> 添加到 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> 的随机高斯噪声。</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-057">
<span class="eqno">(2.2.63)<a class="headerlink" href="#equation-eq-ddpm-057" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}x_t &amp;=\sqrt{\alpha_t} \ x_{t-1} +  \sqrt{1- \alpha_t } \ \epsilon\\&amp;= \sqrt{\bar{\alpha}_t } \ x_0 + \sqrt{1- \bar{ \alpha}_t }  \ \epsilon  \ \ \ ,
\bar{\alpha}_t = \prod_{i=1}^t \alpha_i ,\ \ \epsilon \sim \mathcal{N}(0,\textit{I})\\&amp;\sim \mathcal{N}(\sqrt{\bar{\alpha_t} } \ x_0,  (1- \bar{ \alpha_t})    \textit{I})\end{aligned}\end{align} \]</div>
<p>对于每一个时刻 <span class="math notranslate nohighlight">\(t\)</span>:</p>
<ol class="arabic simple">
<li><p>从标准高斯分布 <span class="math notranslate nohighlight">\(\epsilon\)</span> 中随机采样一个噪声，
注意每一个时刻都是独立重新采样的随机高斯噪声，所以不同时刻 <span class="math notranslate nohighlight">\(t\)</span>，<span class="math notranslate nohighlight">\(\epsilon\)</span> 是不一样的值。</p></li>
<li><p>代入上式中计算得到 <span class="math notranslate nohighlight">\(x_t\)</span>。</p></li>
<li><p>把 <span class="math notranslate nohighlight">\(x_t\)</span> 和 <span class="math notranslate nohighlight">\(t\)</span> 输入参数神经网络模型，模型输出值 <span class="math notranslate nohighlight">\(\hat \epsilon_{{\theta}}(x_t, t)\)</span> 作为预测噪声 。</p></li>
<li><p>最小化 <span class="math notranslate nohighlight">\(\hat \epsilon_{{\theta}}(x_t, t)\)</span> 与 <span class="math notranslate nohighlight">\(\epsilon\)</span> 之间的平方误差。</p></li>
</ol>
<p>参数化模型直接输出值是 <span class="math notranslate nohighlight">\({\hat\epsilon}_{ {\theta}}(x_t, t)\)</span>
，这是就相当于 <strong>模型预测是每一步从</strong>  <span class="math notranslate nohighlight">\(x_{t-1}\)</span> 到 <span class="math notranslate nohighlight">\(t\)</span>  <strong>添加的噪声</strong>。
由于预测噪声之后，再通过 <a class="reference internal" href="#equation-eq-ddpm-055">公式(2.2.61)</a> 就得到了逆向过程 <span class="math notranslate nohighlight">\(p_{{\theta}}(x_{t-1}|x_t)\)</span>
的期望。</p>
<p>从直觉上讲，模型预测每一步的噪声比直接预测 <span class="math notranslate nohighlight">\(x_0\)</span> 要容易很多，
从 DDPM 的论文中也证明了一点。
相比原来的 DPM，DDPM 能生成更高质量的图像数据。</p>
<p>论文中给出的训练过程伪代码：</p>
<figure class="align-center" id="id31">
<span id="fg-dm-010"></span><a class="reference internal image-reference" href="../_images/diffusion_training.webp"><img alt="../_images/diffusion_training.webp" src="../_images/diffusion_training.webp" style="width: 654.75px; height: 180.25px;" /></a>
<figcaption>
<p><span class="caption-number">图 2.2.1 </span><span class="caption-text">DDPM训练过程伪代码（来自 <a class="footnote-reference brackets" href="#footcite-ho2020denoising" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>）</span><a class="headerlink" href="#id31" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>论文中给出的采样（图像生成）过程伪代码如下：</p>
<figure class="align-center" id="id32">
<span id="fg-dm-011"></span><a class="reference internal image-reference" href="../_images/diffusion_sampling.webp"><img alt="../_images/diffusion_sampling.webp" src="../_images/diffusion_sampling.webp" style="width: 629.5500000000001px; height: 222.3px;" /></a>
<figcaption>
<p><span class="caption-number">图 2.2.2 </span><span class="caption-text">DDPM采样过程伪代码（来自 <a class="footnote-reference brackets" href="#footcite-ho2020denoising" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>）</span><a class="headerlink" href="#id32" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>其中 <span class="math notranslate nohighlight">\(\beta_t = 1 - \alpha_t\)</span>
，<span class="math notranslate nohighlight">\(\sigma_{t}\)</span>  是逆过程分布 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span> 的标准差，
参照 <a class="reference internal" href="#equation-eq-ddpm-036">公式(2.1.58)</a>。</p>
<p>在采样（图像生成）过程有一个细节，最后生成的图像（输出值）<span class="math notranslate nohighlight">\(\tilde{x}_0\)</span>
并不是通过 <span class="math notranslate nohighlight">\(x_{t-1}\)</span> 计算得到的，二者计算方式不一样。
逆过程中，<span class="math notranslate nohighlight">\(x_{t-1}\)</span> 的分布是 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0)\)</span>，
它是一个高斯分布的随机变量，它需要通过随机采样得到，
所以上图伪代码中，是先计算出其均值 <span class="math notranslate nohighlight">\(\tilde{\mu}\)</span>，
然后再加上一项随机高斯噪声 <span class="math notranslate nohighlight">\(\sigma_t z\)</span> 。</p>
<p>而最终生成的图像 <span class="math notranslate nohighlight">\(x_0\)</span> 是根据 <a class="reference internal" href="#equation-eq-ddpm-057">公式(2.2.63)</a> 的逆运算得到的。
其实当 <span class="math notranslate nohighlight">\(t=1\)</span> 时， <span class="math notranslate nohighlight">\(\tilde{x_0}\)</span> 和
<span class="math notranslate nohighlight">\(\tilde{\mu}\)</span> 是相同的。
当 <span class="math notranslate nohighlight">\(t=1\)</span> 时，有 <span class="math notranslate nohighlight">\(\bar{\alpha}_1 = \alpha_1\)</span>
，此时</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-058">
<span class="eqno">(2.2.64)<a class="headerlink" href="#equation-eq-ddpm-058" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\tilde{\mu} &amp;= \frac{1} {\sqrt{\alpha_t}} (x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha_t}} } \epsilon_{\theta}(x_t,t) )\\&amp;=  \frac{1} {\sqrt{\alpha_1}} (x_1 - \frac{1-\alpha_1}{\sqrt{1-\bar{\alpha_1}} } \bar{\epsilon} )\\&amp;=  \frac{1} {\sqrt{\alpha_1}} (x_1 - \frac{1-\alpha_1}{\sqrt{1-\bar{\alpha_1}} } \bar{\epsilon} )\\&amp;=  \frac{1} {\sqrt{\alpha_1}} (x_1 - \frac{1-\alpha_1}{\sqrt{1-\alpha_1} } \bar{\epsilon} )\\&amp;=  \frac{1} {\sqrt{\alpha_1}} (x_1 -  \sqrt{1-\alpha_1}  \bar{\epsilon} )\\&amp;=  \tilde{x}_0\end{aligned}\end{align} \]</div>
<p>也就是说我们最终输出的 <span class="math notranslate nohighlight">\(\tilde{x}_0\)</span> 是 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0,t=1)\)</span> 的期望，
而不是  <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t, x_0,t=1)\)</span> 的采样值，这和我们的 ELBO 函数的第一项是一致的。
事实上，所有的回归模型最终输出的都是期望值。</p>
</section>
<section id="score-based-ddpm">
<span id="ch-ddpm-score-based"></span><h2><span class="section-number">2.3. </span>基于分数的解释（Score-based DDPM）<a class="headerlink" href="#score-based-ddpm" title="此标题的永久链接"></a></h2>
<p>在2019年，宋旸 <span id="id18">[<a class="reference internal" href="dalle2.html#id14" title="Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. 2019. arXiv:1907.05600.">SE19</a>]</span> 提出了一种基于分数的扩散模型 <a class="footnote-reference brackets" href="#footcite-song2019generative" id="id19" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> ，
实际上这种基于分数的扩散模型和本章基于马尔科夫的扩散模型是等价的，
论文 <span id="id20">[<a class="reference internal" href="dalle2.html#id19" title="Calvin Luo. Understanding diffusion models: a unified perspective. 2022. arXiv:2208.11970.">Luo22</a>]</span> <a class="footnote-reference brackets" href="#footcite-luo2022understanding" id="id21" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> 给出了这种等价说明。
后续的章节我们再单独详细的介绍宋旸论文中基于分数扩散模型的原理，
这里我们暂且按照论文 <span id="id22">[<a class="reference internal" href="dalle2.html#id19" title="Calvin Luo. Understanding diffusion models: a unified perspective. 2022. arXiv:2208.11970.">Luo22</a>]</span> 的内容给出和 DDPM 等价的分数模型。</p>
<p>首先需要一个关于 <cite>Tweedie</cite> 公式（Tweedie’s Formula）的背景知识。
在统计学中，经常需要根据观测样本估计一个概率分布的未知参数，估计算法有很多，比如最大似然估计、贝叶斯估计等等。
<cite>Tweedie</cite> 公式是一种估计指数族分布均值参数的方法，如果你不知道什么是指数族，可以查看本博客中广义线性模型的内容。
高斯分布就是指数族分布中的一种，所以可以 <cite>Tweedie</cite> 公式估计高斯分布的均值参数。
这里省略 <cite>Tweedie</cite> 公式的证明过程，只关注怎么使用 <cite>Tweedie</cite> 公式。</p>
<p>假设有一个（多元）高斯随机变量 <span class="math notranslate nohighlight">\(z \sim (\mathcal{N}{z;\mu_z,\Sigma_z})\)</span>
， <cite>Tweedie</cite> 公式可以写为</p>
<div class="math notranslate nohighlight" id="equation-aigc-7">
<span class="eqno">(2.3.17)<a class="headerlink" href="#equation-aigc-7" title="此公式的永久链接"></a></span>\[\mathbb{E}[\mu_z | z] = z + \Sigma_z \underbrace{ \nabla_z \log p(z)}_{\text{score}}\]</div>
<p>其中等式左边 <span class="math notranslate nohighlight">\(\mathbb{E}[\mu_z | z]\)</span> 表示：有 <span class="math notranslate nohighlight">\(z\)</span> 的样本条件下，
<span class="math notranslate nohighlight">\(\mu_z\)</span> 的期望，直观地理解，就是用 <span class="math notranslate nohighlight">\(z\)</span> 的样本估计 <span class="math notranslate nohighlight">\(\mu_z\)</span> 的值。
等式右边就是具体计算方式，在这里只需要 <span class="math notranslate nohighlight">\(z\)</span> 的一条样本就可以。
<span class="math notranslate nohighlight">\(\log p(z)\)</span> 就是观测样本的对数似然，
<span class="math notranslate nohighlight">\(\nabla_z \log p(z)\)</span> 就是观测样本的对数似然的一阶导数，又名梯度，
也叫 <cite>Stein</cite> 分数（Stein score），简称分数(score)。
注意，还有另外一个分数， <cite>fisher score</cite> ，二者不太一样，不要搞混了。
<cite>fisher score</cite> 是对数似然关于 <strong>参数</strong> 的梯度 <span class="math notranslate nohighlight">\(\nabla_{\theta} \log p(z;\theta)\)</span>
; <cite>Stein score</cite> 是对数似然关于 <strong>观测变量（样本）</strong>  的梯度 <span class="math notranslate nohighlight">\(\nabla_z \log p(z)\)</span>。</p>
<p>回到 <a class="reference internal" href="#equation-eq-ddpm-025">公式(2.1.49)</a>，在前向过程中，我们得到 <span class="math notranslate nohighlight">\(q(x_t|x_0)\)</span></p>
<div class="math notranslate nohighlight" id="equation-aigc-8">
<span class="eqno">(2.3.18)<a class="headerlink" href="#equation-aigc-8" title="此公式的永久链接"></a></span>\[q(x_t|x_0) = \mathcal{N}(x_{t} ; \sqrt{\bar\alpha_t} x_0, \left(1 - \bar\alpha_t\right)\textit{I})\]</div>
<p><span class="math notranslate nohighlight">\(q(x_t|x_0)\)</span> 是一个高斯分布，虽然已知它的均值是 <span class="math notranslate nohighlight">\(\sqrt{\bar\alpha_t} x_0\)</span>，
但我们仍可以利用 <cite>Tweedie</cite> 公式估计它的均值参数</p>
<div class="math notranslate nohighlight" id="equation-aigc-9">
<span class="eqno">(2.3.19)<a class="headerlink" href="#equation-aigc-9" title="此公式的永久链接"></a></span>\[\sqrt{\bar\alpha_t} x_0 = \mathbb{E}\left[{\mu}_{x_t}|x_t\right] = x_t + (1 - \bar\alpha_t)\nabla_{x_t}\log p(x_t)\]</div>
<p>对上式进行移项可以得到一个新的关系式</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-065">
<span class="eqno">(2.3.20)<a class="headerlink" href="#equation-eq-ddpm-065" title="此公式的永久链接"></a></span>\[x_0 = \frac{x_t + (1 - \bar\alpha_t)\nabla\log p(x_t)}{\sqrt{\bar\alpha_t}}\]</div>
<p>这个关系式是一个分数表达 <span class="math notranslate nohighlight">\(x_0\)</span> 的等式。
现在回到逆过程中的真实分布 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t,x_0)\)</span>
，它的均值 <span class="math notranslate nohighlight">\({\mu}_q(x_t, x_0)\)</span> 在 <a class="reference internal" href="#equation-eq-ddpm-040">公式(2.1.63)</a>
中已经给出，这里把 <a class="reference internal" href="#equation-eq-ddpm-065">公式(2.3.20)</a> 带入进去</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-066">
<span class="eqno">(2.3.21)<a class="headerlink" href="#equation-eq-ddpm-066" title="此公式的永久链接"></a></span>\[\begin{split}\begin{align}
{\mu}_q(x_t, x_0) &amp;= \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}\\
&amp;= \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\frac{x_t + (1 - \bar\alpha_t)\nabla\log p(x_t)}{\sqrt{\bar\alpha_t}}}{1 -\bar\alpha_{t}}\\
&amp;= \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + (1-\alpha_t)\frac{x_t + (1 - \bar\alpha_t)\nabla\log p(x_t)}{\sqrt{\alpha_t}}}{1 -\bar\alpha_{t}}\\
&amp;= \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t}}{1 - \bar\alpha_t} + \frac{(1-\alpha_t)x_t}{(1-\bar\alpha_t)\sqrt{\alpha_t}} + \frac{(1 - \alpha_t)(1 - \bar\alpha_t)\nabla\log p(x_t)}{(1-\bar\alpha_t)\sqrt{\alpha_t}}\\
&amp;= \left(\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1 - \bar\alpha_t} + \frac{1-\alpha_t}{(1-\bar\alpha_t)\sqrt{\alpha_t}}\right)x_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}}\nabla\log p(x_t)\\
&amp;= \left(\frac{\alpha_t(1-\bar\alpha_{t-1})}{(1 - \bar\alpha_t)\sqrt{\alpha_t}} + \frac{1-\alpha_t}{(1-\bar\alpha_t)\sqrt{\alpha_t}}\right)x_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}}\nabla\log p(x_t)\\
&amp;= \frac{\alpha_t-\bar\alpha_{t} + 1-\alpha_t}{(1 - \bar\alpha_t)\sqrt{\alpha_t}}x_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}}\nabla\log p(x_t)\\
&amp;= \frac{1-\bar\alpha_t}{(1 - \bar\alpha_t)\sqrt{\alpha_t}}x_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}}\nabla\log p(x_t)\\
&amp;= \frac{1}{\sqrt{\alpha_t}}x_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}} \nabla\log p(x_t)
\end{align}\end{split}\]</div>
<p>重新按照个形式设定参数化分布 <span class="math notranslate nohighlight">\(p_{\theta}(x_{t-1}|x_t)\)</span> 均值 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span>
的表达式</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-067">
<span class="eqno">(2.3.22)<a class="headerlink" href="#equation-eq-ddpm-067" title="此公式的永久链接"></a></span>\[\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_t}}x_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}}s_{{\theta}}(x_t, t)\]</div>
<p>现在 <span class="math notranslate nohighlight">\(s_{{\theta}}(x_t, t)\)</span> 是模型学习预测的内容，
相当于模型直接预测的是分数 <span class="math notranslate nohighlight">\(\nabla_{x_t} \log p(x_t)\)</span>，
最后我们推导下目标函数 ELBO 中相关的 KL 散度。</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-068">
<span class="eqno">(2.3.23)<a class="headerlink" href="#equation-eq-ddpm-068" title="此公式的永久链接"></a></span>\[\begin{split}\begin{align}
&amp; \quad \,\argmin_{{\theta}} \KL{q(x_{t-1}|x_t, x_0)}{p_{{\theta}}(x_{t-1}|x_t)} \nonumber \\
&amp;= \argmin_{{\theta}}\KL{\mathcal{N}\left(x_{t-1}; {\mu}_q,{\Sigma}_q\left(t\right)\right)}{\mathcal{N}\left(x_{t-1}; {\mu}_{{\theta}},{\Sigma}_q\left(t\right)\right)}\\
&amp;=\argmin_{{\theta}}\frac{1}{2\sigma_q^2(t)}\left[\left\lVert\frac{1}{\sqrt{\alpha_t}}x_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}}s_{{\theta}}(x_t, t) -
\frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{\alpha_t}}\nabla\log p(x_t)\right\rVert_2^2\right]\\
&amp;=\argmin_{{\theta}}\frac{1}{2\sigma_q^2(t)}\left[\left\lVert \frac{1 - \alpha_t}{\sqrt{\alpha_t}}s_{{\theta}}(x_t, t) - \frac{1 - \alpha_t}{\sqrt{\alpha_t}}\nabla\log p(x_t)\right\rVert_2^2\right]\\
&amp;=\argmin_{{\theta}}\frac{1}{2\sigma_q^2(t)}\left[\left\lVert \frac{1 - \alpha_t}{\sqrt{\alpha_t}}(s_{{\theta}}(x_t, t) - \nabla\log p(x_t))\right\rVert_2^2\right]\\
&amp;=\argmin_{{\theta}}\frac{1}{2\sigma_q^2(t)}\frac{(1 - \alpha_t)^2}{\alpha_t}\left[\left\lVert s_{{\theta}}(x_t, t) - \nabla\log p(x_t)\right\rVert_2^2\right] \label{eq:123}
\end{align}\end{split}\]</div>
<p>最后的目标函数仍然是均方误差的形式，和原始扩散模型以及基于噪声的扩散模型是高度一致的。
公式中的 <span class="math notranslate nohighlight">\(\nabla\log p(x_t)\)</span> 是梯度的真实值，这个可以直接计算得到，<cite>pytorch</cite> 中可以计算指定变量的梯度。
<span class="math notranslate nohighlight">\(s_{{\theta}}(x_t, t)\)</span> 代表一个神经网络模型，它去学习和预测 <span class="math notranslate nohighlight">\(\nabla\log p(x_t)\)</span>
。</p>
<p>接下来看下梯度和噪声的关系，结合 <a class="reference internal" href="#equation-eq-ddpm-065">公式(2.3.20)</a> 和 <a class="reference internal" href="#equation-eq-ddpm-052">公式(2.2.58)</a> 可得</p>
<div class="math notranslate nohighlight" id="equation-aigc-10">
<span class="eqno">(2.3.24)<a class="headerlink" href="#equation-aigc-10" title="此公式的永久链接"></a></span>\[\begin{split}\begin{align}
x_0 = \frac{x_t + (1 - \bar\alpha_t)\nabla\log p(x_t)}{\sqrt{\bar\alpha_t}} &amp;= \frac{x_t - \sqrt{1 - \bar\alpha_t}{\epsilon}_t}{\sqrt{\bar\alpha_t}}\\
\therefore (1 - \bar\alpha_t)\nabla\log p(x_t) &amp;= -\sqrt{1 - \bar\alpha_t}{\epsilon}_t\\
\nabla\log p(x_t) &amp;= -\frac{1}{\sqrt{1 - \bar\alpha_t}}{\epsilon}_t
\end{align}\end{split}\]</div>
<p>我们知道梯度的方向，是函数极值的方向。
梯度 <span class="math notranslate nohighlight">\(\nabla\log p(x_t)\)</span> 指向的是对数概率函数 <span class="math notranslate nohighlight">\(\log p(x_t)\)</span> 概率密度最大的方向，
它和添加的噪声相差一个负号，正好是相反的，梯度的方向是噪声的反方向，这十分符合直觉。</p>
</section>
<section id="id23">
<h2><span class="section-number">2.4. </span>扩散模型的三种等价表示<a class="headerlink" href="#id23" title="此标题的永久链接"></a></h2>
<p>到这里我们一共介绍了扩撒模型的三种不同形式，1）直接预测初始样本；2）预测噪声；3）预测分数。
这三种形式理论是等价的，然后对于模型的学习难度以及灵活性的却是不同的。
这里我们把这三种形式进行总结对比一下，方便理解和复习。</p>
<p>逆向过程中真实的（后验）分布为 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t,x_0)\)</span>
，它是一个条件高斯分布，均值记为 <span class="math notranslate nohighlight">\(\mu_q\)</span>，
方差记为 <span class="math notranslate nohighlight">\(\Sigma_{q(t)}\)</span>，</p>
<div class="math notranslate nohighlight" id="equation-aigc-11">
<span class="eqno">(2.4.23)<a class="headerlink" href="#equation-aigc-11" title="此公式的永久链接"></a></span>\[q(x_{t-1}|x_t,x_0) \sim \mathcal{N}(x_{t-1},\mu_q,\Sigma_{q(t)})\]</div>
<p>方差 <span class="math notranslate nohighlight">\(\Sigma_{q(t)}\)</span> 的表达式为</p>
<div class="math notranslate nohighlight" id="equation-aigc-12">
<span class="eqno">(2.4.24)<a class="headerlink" href="#equation-aigc-12" title="此公式的永久链接"></a></span>\[\Sigma_q(t) = \frac{(1 - \alpha_t)(1 - \bar\alpha_{t-1})}{ 1 -\bar\alpha_{t}}  \textit{I} = \sigma_q^2(t)   \textit{I}\]</div>
<p>方差看做一个已知的常量，默认模型不去学习它。
我们的参数化模型学习的分布记为 <span class="math notranslate nohighlight">\(p_{\theta}(x_{t-1}|x_t)\)</span>,
我们要令 <span class="math notranslate nohighlight">\(p_{\theta}(x_{t-1}|x_t)\)</span> 尽量近似真实分布 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t,x_0)\)</span>。
实际实现时，模型学习拟合的是真实分布 <span class="math notranslate nohighlight">\(q(x_{t-1}|x_t,x_0)\)</span> 的均值 <span class="math notranslate nohighlight">\(\mu_q\)</span>，
然而它可以推导出三种不同等价的形式，因此也就有三种不同的学习方式。</p>
<p><strong>第一种形式：学习初始样本</strong></p>
<p>均值 <span class="math notranslate nohighlight">\(\mu_q\)</span> 的第一种表达式为</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-083">
<span class="eqno">(2.4.25)<a class="headerlink" href="#equation-eq-ddpm-083" title="此公式的永久链接"></a></span>\[\mu_q(x_t, x_0) = { \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1 -\bar\alpha_{t}}}\]</div>
<p>按照个形式设定参数化分布 <span class="math notranslate nohighlight">\(p_{\theta}(x_{t-1}|x_t)\)</span> 的均值 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-084">
<span class="eqno">(2.4.26)<a class="headerlink" href="#equation-eq-ddpm-084" title="此公式的永久链接"></a></span>\[\mu_{\theta}={\mu}_{{\theta}}(x_t, t) = \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})x_{t} + \sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\hat x_{{\theta}}(x_t, t)}{1 -\bar\alpha_{t}}\]</div>
<p>此时模型实际学习的是初始样本 <span class="math notranslate nohighlight">\(x_0\)</span>，
显然这种形式对模型来说难度比较大，所以效果并不突出。</p>
<p><strong>第二种形式：学习噪声</strong></p>
<p>均值 <span class="math notranslate nohighlight">\(\mu_q\)</span> 的第二种表达式为</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-085">
<span class="eqno">(2.4.27)<a class="headerlink" href="#equation-eq-ddpm-085" title="此公式的永久链接"></a></span>\[\mu_q(x_t, x_0) = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}\sqrt{\alpha_t}}\ \epsilon\]</div>
<p>同理，按照个形式设定参数化分布 <span class="math notranslate nohighlight">\(p_{\theta}(x_{t-1}|x_t)\)</span> 的均值 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-086">
<span class="eqno">(2.4.28)<a class="headerlink" href="#equation-eq-ddpm-086" title="此公式的永久链接"></a></span>\[\mu_{\theta}={\mu}_{{\theta}}(x_t, t) = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}\sqrt{\alpha_t}} {\hat\epsilon}_{ {\theta}}(x_t, t)\]</div>
<p>此时模型学习预测的噪声 <span class="math notranslate nohighlight">\(\epsilon\)</span>
，相比直接学习初始样本 <span class="math notranslate nohighlight">\(x_0\)</span>，每一步学习噪声简单了许多，模型的效果提升很大。</p>
<p><strong>第三种形式：学习分数</strong></p>
<p>依据 <cite>Tweedie</cite> 公式，均值 <span class="math notranslate nohighlight">\(\mu_q\)</span> 的表达式可以写成基于分数的形式</p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-087">
<span class="eqno">(2.4.29)<a class="headerlink" href="#equation-eq-ddpm-087" title="此公式的永久链接"></a></span>\[{\mu}_q(x_t, x_0) =  \frac{1}{\sqrt{\alpha_t}}x_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}}\nabla\log p(x_t)\]</div>
<p>同样，按照个形式设定参数化分布 <span class="math notranslate nohighlight">\(p_{\theta}(x_{t-1}|x_t)\)</span> 的均值 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-ddpm-088">
<span class="eqno">(2.4.30)<a class="headerlink" href="#equation-eq-ddpm-088" title="此公式的永久链接"></a></span>\[{\mu}_q(x_t, x_0) =  \frac{1}{\sqrt{\alpha_t}}x_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}} s_{\theta}(x_t,t)\]</div>
<p>此时模型学习预测的是分数（梯度） <span class="math notranslate nohighlight">\(\nabla\log p(x_t)\)</span>。
相比学习噪声，学习分数有个好处，他可以令我在逆过程采样图片时采用基于分数的采样算法，
而基于分数的采样算法种类很多，这极大的增加了算法的灵活性。</p>
</section>
<section id="improved-denoising-diffusion-probabilistic-models-iddpm">
<h2><span class="section-number">2.5. </span>改进降噪扩散概率模型（Improved Denoising Diffusion Probabilistic Models,IDDPM）<a class="headerlink" href="#improved-denoising-diffusion-probabilistic-models-iddpm" title="此标题的永久链接"></a></h2>
<ol class="arabic simple">
<li><p>学习方差</p></li>
<li><p>线性的schedule 改成余弦schedule</p></li>
</ol>
<p>待补充</p>
<p>源码 <a class="reference external" href="https://github.com/openai/improved-diffusion">https://github.com/openai/improved-diffusion</a></p>
</section>
<section id="id24">
<h2><span class="section-number">2.6. </span>参考文献<a class="headerlink" href="#id24" title="此标题的永久链接"></a></h2>
<div class="docutils container" id="id25">
<aside class="footnote brackets" id="footcite-sohldickstein2015deep" role="note">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>,<a role="doc-backlink" href="#id4">3</a>)</span>
<p>Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. 2015. <a class="reference external" href="https://arxiv.org/abs/1503.03585">arXiv:1503.03585</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-luo2022understanding" role="note">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id5">2</a>,<a role="doc-backlink" href="#id7">3</a>,<a role="doc-backlink" href="#id11">4</a>,<a role="doc-backlink" href="#id12">5</a>,<a role="doc-backlink" href="#id13">6</a>,<a role="doc-backlink" href="#id21">7</a>)</span>
<p>Calvin Luo. Understanding diffusion models: a unified perspective. 2022. <a class="reference external" href="https://arxiv.org/abs/2208.11970">arXiv:2208.11970</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-ho2020denoising" role="note">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id9">1</a>,<a role="doc-backlink" href="#id15">2</a>,<a role="doc-backlink" href="#id16">3</a>,<a role="doc-backlink" href="#id17">4</a>)</span>
<p>Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. 2020. <a class="reference external" href="https://arxiv.org/abs/2006.11239">arXiv:2006.11239</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-kingma2022variational" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">4</a><span class="fn-bracket">]</span></span>
<p>Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. 2022. <a class="reference external" href="https://arxiv.org/abs/2107.00630">arXiv:2107.00630</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-song2019generative" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">5</a><span class="fn-bracket">]</span></span>
<p>Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. 2019. <a class="reference external" href="https://arxiv.org/abs/1907.05600">arXiv:1907.05600</a>.</p>
</aside>
</aside>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="页脚">
        <a href="%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html" class="btn btn-neutral float-left" title="1. 变分自编码器（Variational Autoencoder）" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="ddim.html" class="btn btn-neutral float-right" title="3. 去噪扩散隐式模型（Denoising Diffusion Implicit Models,DDIM）" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2018, zhangzhenhu(acmtiger@outlook.com) 禁止一切形式的转载！.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用的 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a> 开发.
  


    <script src="https://utteranc.es/client.js"
            repo="zhangzhenhu/blog"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>



</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>