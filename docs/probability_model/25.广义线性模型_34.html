<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>19. 广义线性模型 &mdash; 张振虎的博客 张振虎 文档</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
    <link rel="canonical" href="https://www.zhangzhenhu.com/probability_model/25.广义线性模型_34.html" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/translations.js"></script>
        <script>window.MathJax = {"tex": {"macros": {"RR": "{\\bf R}", "bold": ["{\\bf #1}", 1], "KL": ["{D_\\textrm{KL}\\left ( #1 \\| #2 \\right )}", 2], "EE": ["{\\mathbb{E}_{#1} \\left [ #2 \\right ]}", 2, ""], "scalemath": ["{\\scalebox{#1}{\\mbox{\\ensuremath{\\displaystyle #2}}}}", 2], "argmin": ["{\\operatorname*{\\arg\\min}}"], "ind": ["{\\perp\\!\\!\\!\\!\\perp}"]}}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="20. 混合模型" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html" />
    <link rel="prev" title="18. 分类模型" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            张振虎的博客
          </a>
              <div class="version">
                acmtiger@outlook.com
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="搜索文档" aria-label="搜索文档" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="导航菜单">
              <p class="caption" role="heading"><span class="caption-text">目录:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../glm/source/index.html">广义线性模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id2">1.1. 概率模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id3">1.1.1. 概率律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id4">1.1.2. 离散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id5">1.1.3. 连续模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id6">1.2. 条件概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id7">1.3. 联合概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id8">1.4. 全概率与贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id9">1.5. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id10">1.6. 随机变量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id11">1.6.1. 离散随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id12">1.6.2. 连续随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id13">1.6.3. 累积分布函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id14">1.6.4. 随机变量的函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id15">1.6.5. 期望与方差</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id16">1.7. 边缘化</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id17">1.8. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#ch-basic-bernoulli">1.8.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id19">1.8.2. 二项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id20">1.8.3. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id21">1.8.4. 多项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id22">1.8.5. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id23">1.8.6. 卡方分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#t">1.8.7. t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#f">1.8.8. F分布</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html">2. 最大似然估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-liklihood">2.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id3">2.2. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id4">2.3. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-2-gaussian-ml">2.4. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id6">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html">3. 推断与检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id2">3.1. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution">3.2. 抽样分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution-normal">3.2.1. 正态分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#t">3.2.2. 学生t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id5">3.2.3. 卡方分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id6">3.3. 极限理论</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id7">3.3.1. 马尔可夫和切比雪夫不等式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id8">3.3.2. 弱大数定律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id9">3.3.3. 依概率收敛</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-clt">3.3.4. 中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id11">3.3.5. 强大数定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id12">3.4. 似然估计量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id13">3.4.1. 估计量的偏差与方差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-fisher-information">3.4.2. 信息量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-mle-estimator">3.4.3. 最大似然估计的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-interval">3.5. 置信区间</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#z">3.5.1. 均值参数的 Z 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id18">3.5.2. 均值参数的 T 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id19">3.5.3. 方差参数的区间估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-test">3.6. 简单假设检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id22">3.6.1. Z检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id23">3.6.2. T检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id24">3.6.3. 卡方检验</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html">4. 贝叶斯估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id2">4.1. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id3">4.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id4">4.1.2. 类别分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id5">4.2. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id6">4.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id7">4.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id8">4.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html">5. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-1">5.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id3">5.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id4">5.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id5">5.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id6">5.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id7">5.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-moments">5.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id9">5.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%97%8F/content.html#kl">5.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html">6. 线性回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id2">6.1. 最小二乘</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id3">6.1.1. 最小误差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id4">6.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id5">6.2. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id6">6.2.1. 高斯假设</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id7">6.2.2. 参数估计</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html">7. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id2">7.1. 指数族分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id3">7.1.1. 自然指数族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id4">7.1.2. 示例：高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id5">7.1.3. 示例：伯努利分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id6">7.2. 广义线性模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id7">7.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html">8. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate">8.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id3">8.2. 泰勒级数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id4">8.3. 梯度下降法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id6">8.4. 牛顿法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id7">8.4.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id8">8.4.2. 标准连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id9">8.4.3. 迭代初始值的设定</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#irls">8.5. 迭代重加权最小二乘(IRLS)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id10">8.5.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id11">8.5.2. 算法过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id12">8.6. 估计量的标准误差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate-phi">8.7. 分散参数的估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html">9. 模型评估</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id2">9.1. 拟合优度</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id3">9.1.1. 嵌套模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#likelihood-ratio">9.1.2. 对数似然比(Likelihood ratio)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance">9.1.3. 偏差(deviance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#r-2">9.1.4. 决定系数 <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#ch-glm-gof-chi">9.1.5. 广义皮尔逊卡方统计量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#residual-analysis">9.2. 残差分析(Residual analysis)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#response-residuals">9.2.1. Response residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#working-residuals">9.2.2. Working residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#partial-residuals">9.2.3. Partial residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#pearson-residuals">9.2.4. Pearson residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance-residuals">9.2.5. Deviance residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#score-residuals">9.2.6. Score residuals</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#model-selection">9.3. 模型选择(model selection)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#aic">9.3.1. AIC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#bic">9.3.2. BIC</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html">10. 模型检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id2">10.1. 拉格朗日乘子检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id3">10.1.1. 得分统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id4">10.1.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#wald">10.2. wald 检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id5">10.2.1. 参数估计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id6">10.2.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id7">10.3. 似然比检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id8">10.3.1. 抽样分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id9">10.3.2. 模型比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id10">10.3.3. 偏差统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#f">10.3.4. F 检验</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id11">10.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">11. 高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">11.1. 传统线性回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">11.2. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">11.3. 高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">11.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">11.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">11.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">11.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id10">11.5. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">12. 逆高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">12.1. 逆高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">12.2. 逆高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">12.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">12.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">12.3.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">12.3.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">12.4. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">13. 二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">13.1. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">13.2. 逻辑回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">13.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">13.2.2. 参数估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#odds-logit">13.2.3. odds 与 logit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">13.3. 二项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">13.4. 二项式回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">13.4.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">13.4.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">13.5. 其它连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">13.5.1. 恒等连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#probit">13.5.2. probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#log-log-clog-log">13.5.3. log-log 和 clog-log</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">13.6. 分组数据与比例数据</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html">14. 泊松模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#poisson">14.1. 泊松(Poisson)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id2">14.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id3">14.1.2. 泊松分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id5">14.2. 泊松回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id6">14.3. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id7">14.4. 拟合统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id8">14.5. 频率模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id9">14.6. 泊松模型的局限性</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html">15. 指数模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#exponential">15.1. 指数(exponential)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id2">15.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id3">15.1.2. 分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id6">15.2. 指数回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id7">15.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id8">15.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id9">15.3.2. 拟合优度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#irls">15.3.3. IRLS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html">16. Gamma 模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id1">16.1. Gamma 函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id2">16.2. Gamma 分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id3">16.3. Gamma 回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id4">16.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id5">16.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#irls">16.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id6">16.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id7">16.5. 其他连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#id8">16.5.1. 对数 Gamma 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/gamma%E6%A8%A1%E5%9E%8B/content.html#identity-gamma">16.5.2. 恒等(identity) Gamma 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html">17. 过度分散</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id2">17.1. 什么是过度分散</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id3">17.2. 过度分散的检测</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id4">17.3. 过度分散的影响</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id5">17.4. 标准误差的修正</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">18. 负二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">18.1. 负二项式分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">18.1.1. 从二项式分布推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">18.1.2. 泊松-伽马混合分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#alpha">18.1.3. 辅助参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的影响</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">18.2. 负二项回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">18.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#irls">18.3.1. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">18.3.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">18.4. 负二项式模型扩展</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">18.4.1. 对数连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">18.4.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">18.4.3. 几何模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">18.4.4. 广义负二项式模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html">19. 零计数问题</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id2">19.1. 零截断模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id3">19.1.1. 零截断泊松模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id4">19.1.2. 零截断负二项式模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id5">19.2. 零膨胀模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#hurdle">19.2.1. Hurdle 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#zero-inflate">19.2.2. Zero-inflate 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">20. 多项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">20.1. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#softmax">20.2. softmax 回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">20.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">20.3. 多项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id6">20.4. 多项式回归模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">21. 有序离散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">21.1. 有序逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">21.2. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">21.3. 连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#logit">21.3.1. logit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#probit">21.3.2. probit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#clog-log">21.3.3. clog-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#log-log">21.3.4. log-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#cauchit">21.3.5. cauchit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">21.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html">附录</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id2">标准正态累积分布表</a></li>
<li class="toctree-l3"><a class="reference internal" href="../glm/source/%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id3">卡方分布临界值表</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../glm/source/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/content.html">参考文献</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index_html.html">概率图</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id2">1.1. 概率分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id3">1.2. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#marginalization">1.3. 边缘化(marginalization)</a></li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id6">1.4. 贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id7">1.5. 期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id8">1.6. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id9">1.6.1. 离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id10">1.6.2. 连续变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id11">1.6.3. 计数变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id12">1.7. 大数定律</a><ul>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id13">1.7.1. 独立同分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id14">1.7.2. 中心极限定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id15">1.8. 信息论基础</a><ul>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id16">1.8.1. 信息熵</a></li>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#kl">1.8.2. KL散度</a></li>
<li class="toctree-l4"><a class="reference internal" href="1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id18">1.8.3. 互信息</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html">2. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-liklihood">2.1. 极大似然估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id3">2.1.1. 二值离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id4">2.1.2. 一般离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-gaussian-ml">2.1.3. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id6">2.1.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-bayesian-estimation">2.2. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id8">2.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id9">2.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id10">2.3. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id11">2.3.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id12">2.3.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id13">2.4. 最大似然估计与贝叶斯估计的对比</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id14">2.5. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#fisher-information">2.6. Fisher Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id15">2.7. 估计量的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id16">2.7.1. 估计量的方差与偏差</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id17">2.7.2. 大数定律和中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-mle-estimator">2.7.3. 最大似然估计的特性</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html">3. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-1">3.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id3">3.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id4">3.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id5">3.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id6">3.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id7">3.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-moments">3.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id9">3.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#kl">3.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="19.%E5%A4%9A%E7%BB%B4%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_26.html">4. 多维高斯分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html">5. 有向图(Directed Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id1">5.1. 有向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id2">5.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id3">5.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html">6. 无向图(Undirected Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id1">6.1. 无向图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id2">6.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id3">6.3. 图的分解</a></li>
<li class="toctree-l3"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#vs">6.4. 有向图 vs 无向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id4">6.5. 树</a></li>
<li class="toctree-l3"><a class="reference internal" href="5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id5">6.6. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html">7. 因子图</a><ul>
<li class="toctree-l3"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id2">7.1. 因子图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id3">7.2. 图模型之间的转换</a><ul>
<li class="toctree-l4"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id4">7.2.1. 转换为因子图</a></li>
<li class="toctree-l4"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id5">7.2.2. 因子图转换为有向图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id6">7.3. 图模型的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#i-map">7.3.1. I-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#d-map">7.3.2. D-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#p-map">7.3.3. P-map</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html">8. 模型推断：消元法</a><ul>
<li class="toctree-l3"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#id2">8.1. 什么是模型的推断</a></li>
<li class="toctree-l3"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#id3">8.2. 消元法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#id4">8.2.1. 有向图消元算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#ch-condition-margin">8.2.2. 条件概率和边缘概率</a></li>
<li class="toctree-l4"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#id6">8.2.3. 无向图的消元法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#id7">8.3. 图消除</a></li>
<li class="toctree-l3"><a class="reference internal" href="8.%E6%B6%88%E5%85%83%E6%B3%95.html#id9">8.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html">9. 加和乘积算法(sum-product algorithm)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id1">9.1. 树结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id2">9.2. 从消元法到信息传播</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id3">9.3. 树模型的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id4">9.4. 因子图的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id5">9.5. 类树结构图模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#polytrees">9.6. 多重树(polytrees)</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id6">9.7. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html">10. 最大后验估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id2">10.1. 最大后验概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id3">10.2. 最大化后验的状态</a></li>
<li class="toctree-l3"><a class="reference internal" href="11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id4">10.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html">11. 完整观测的参数学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id2">11.1. 有向图的参数学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id3">11.2. 无向图的参数学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id4">11.2.1. 成对二值变量模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id5">11.2.2. 一般二值变量模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html">12. 不完整观测的学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#id2">12.1. 隐变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#em">12.2. 期望最大化算法(EM)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="14.%E5%9B%BE%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E5%AD%A6%E4%B9%A0_lecture_23.html">13. 有向图结构学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="16.%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD_lecture_17.html">14. 变分推断</a></li>
<li class="toctree-l2"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html">15. 马尔科夫蒙特卡洛</a><ul>
<li class="toctree-l3"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#why-sampling">15.1. Why sampling？</a></li>
<li class="toctree-l3"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#monte-carlo">15.2. 蒙特卡罗(Monte Carlo)</a></li>
<li class="toctree-l3"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain">15.3. 马尔科夫链(Markov Chain)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id2">15.3.1. 一个例子</a></li>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#time-reversibility">15.3.2. 时间可逆性(Time Reversibility)</a></li>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id3">15.3.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain-monte-carlo">15.4. Markov Chain Monte Carlo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#metropolis-hastings">15.4.1. Metropolis-Hastings</a></li>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id4">15.4.2. 例子：正态分布的采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id5">15.4.3. 多变量采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#gibbs">15.4.4. Gibbs 采样</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#mixing-time">15.5. Mixing Time</a></li>
<li class="toctree-l3"><a class="reference internal" href="17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#approximate-map-and-partitioning">15.6. Approximate MAP and Partitioning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html">16. 贝叶斯分类器</a><ul>
<li class="toctree-l3"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id2">16.1. 朴素贝叶斯模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id3">16.1.1. 模型表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id4">16.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id5">16.2. 高斯判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id6">16.2.1. 一元高斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id7">16.2.2. 多元高斯模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id8">16.3. 逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id9">16.4. 生成模型和判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id10">16.5. 多分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id11">16.6. 其它扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html">17. 回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id2">17.1. 机器学习的概率解释</a></li>
<li class="toctree-l3"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id3">17.2. 经典线性回归</a><ul>
<li class="toctree-l4"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id4">17.2.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id5">17.3. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id6">17.3.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id7">17.4. 凸函数最优化问题</a></li>
<li class="toctree-l3"><a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id8">17.5. 岭回归</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html">18. 分类模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id2">18.1. 生成模型与判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id3">18.2. 线性回归与线性分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id4">18.3. 生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id5">18.3.1. 高斯判别模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id6">18.3.2. 朴素贝叶斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id7">18.3.3. 指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id8">18.4. 判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id9">18.4.1. 逻辑回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id10">18.4.2. 多分类</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id11">18.4.3. 最大熵模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#probit">18.4.4. Probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#noisy-or">18.4.5. Noisy-OR 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id12">18.4.6. 其它指数模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">19. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">19.1. 定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">19.1.1. 指数族分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">19.1.2. 链接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">19.1.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id6">19.2. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id7">19.2.1. 梯度下降法</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">19.2.2. 牛顿法</a></li>
<li class="toctree-l4"><a class="reference internal" href="#irls">19.2.3. 迭代重加权最小二乘(IRLS)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#goodness-of-fit">19.3. goodness of fit</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">19.4. 连续值响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id10">19.4.1. 高斯族</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gamma">19.4.2. Gamma族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id12">19.5. 二项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id13">19.6. 多项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id14">19.7. 计数响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id15">19.7.1. 泊松分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id16">19.8. GLM扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html">20. 混合模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id2">20.1. 一般混合模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id3">20.1.1. 模型的有向图表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id5">20.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id8">20.2. 高斯混合模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id9">20.2.1. 模型的表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id10">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#k-means">20.3. K-means</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="32.%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90_42.html">21. 因子分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E4%BA%8C%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B.html">22. 二变量模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="33.LDA_43.html">23. 主题模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="33.LDA_43.html#plsa">23.1. PLSA</a></li>
<li class="toctree-l3"><a class="reference internal" href="33.LDA_43.html#lda">23.2. LDA</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html">24. 隐马尔可夫模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id2">24.1. 隐马尔可夫模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id3">24.1.1. 马尔可夫模型和朴素贝叶斯模型的关系</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id4">24.2. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="27.%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA_37.html">25. 条件随机场</a></li>
<li class="toctree-l2"><a class="reference internal" href="28.%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8_38.html">26. 卡尔曼滤波器</a></li>
<li class="toctree-l2"><a class="reference internal" href="40.%E9%A1%B9%E7%9B%AE%E5%8F%8D%E5%BA%94%E7%90%86%E8%AE%BA_50.html">27. 项目反应理论</a></li>
<li class="toctree-l2"><a class="reference internal" href="41.%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA_51.html">28. 贝叶斯知识追踪</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE.html">29. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../aigc/index.html">AI内容生成（ai-gc）</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html">1. 变分自编码器（Variational Autoencoder）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#evidence-lower-bound-elbo">1.1. 证据下界(Evidence Lower Bound,ELBO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id2">1.2. 编码-解码</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id3">1.3. 总结</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#em">1.3.1. 和EM算法的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#variational">1.3.2. 为什么叫变分（variational）？</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#vq-vae">1.4. VQ-VAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id4">1.5. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html">2. 扩散概率模型（diffusion probabilistic models）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#diffusion-probabilistic-model">2.1. 扩散概率模型（diffusion probabilistic model）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#markovian-hierarchical-variational-autoencoder-mhvae">2.1.1. 马尔科夫分层自编码器（Markovian Hierarchical Variational Autoencoder,MHVAE)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id6">2.1.2. 扩散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id8">2.1.3. 前向-后向</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#elbo">2.1.4. 目标函数（ELBO）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id14">2.1.5. 图片生成（采样）过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#denoising-diffusion-probabilistic-model-ddpm">2.2. 降噪扩散概率模型（Denoising diffusion probabilistic model,DDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#score-based-ddpm">2.3. 基于分数的解释（Score-based DDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id23">2.4. 扩散模型的三种等价表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#improved-denoising-diffusion-probabilistic-models-iddpm">2.5. 改进降噪扩散概率模型（Improved Denoising Diffusion Probabilistic Models,IDDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id24">2.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../aigc/ddim.html">3. 去噪扩散隐式模型（Denoising Diffusion Implicit Models,DDIM）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../aigc/ddim.html#id1">3.1. 扩散模型的回顾</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/ddim.html#id2">3.2. 非马尔科夫前向过程</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/ddim.html#id4">3.3. 加速采样</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/ddim.html#id5">3.4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../aigc/Score-Based_Generative_Models.html">4. 基于分数的生成模型（Score-based generative models）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../aigc/Score-Based_Generative_Models.html#id4">4.1. 基于分数的生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../aigc/Score-Based_Generative_Models.html#score-matching">4.1.1. 分数匹配算法（Score Matching）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/Score-Based_Generative_Models.html#id8">4.1.2. 基于分数的生成模型面临的困难</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/Score-Based_Generative_Models.html#id12">4.1.3. 通过加噪的方法估计分布的近似分数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/Score-Based_Generative_Models.html#id13">4.1.4. 基于分数的改进采样算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/Score-Based_Generative_Models.html#id15">4.1.5. 改进的分数生成模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/Score-Based_Generative_Models.html#id17">4.2. 随机微分方程</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../aigc/Score-Based_Generative_Models.html#id18">4.2.1. 微分方程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/Score-Based_Generative_Models.html#id19">4.2.2. 随机微分方程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/Score-Based_Generative_Models.html#id20">4.2.3. 基于随机微分方程的生成模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/Score-Based_Generative_Models.html#id21">4.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../aigc/Guidance.html">5. 条件控制扩散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../aigc/Guidance.html#classifier-guidance">5.1. classifier guidance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/Guidance.html#classifier-free-guidance">5.2. Classifier-free guidance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/Guidance.html#clip-guidance">5.3. CLIP Guidance</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../aigc/Guidance.html#id12">5.3.1. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../aigc/dalle2.html">6. DALL·E 2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../aigc/dalle2.html#glide">6.1. GLIDE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/dalle2.html#unclip">6.2. unCLIP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/dalle2.html#id3">6.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html">7. 稳定扩散模型（Stable diffusion model）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#latent-diffusion-model-ldm">7.1. 潜在扩散模型（Latent diffusion model,LDM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#stable-diffusion-sd">7.2. 稳定扩散模型（Stable diffusion,SD）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#id3">7.2.1. 推理过程代码</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#id4">7.2.2. 训练过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#id5">7.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../aigc/controlnet.html">8. 条件控制之ControlNet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../aigc/controlnet.html#id3">8.1. 算法原理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/controlnet.html#id6">8.2. 代码实现</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/controlnet.html#id7">8.3. 最后的总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/controlnet.html#id8">8.4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../aigc/dreamBooth.html">9. 条件控制之DreamBooth</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../aigc/dreamBooth.html#id2">9.1. DreamBooth 技术</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/dreamBooth.html#id3">9.2. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../aigc/imagen.html">10. Imagen</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../aigc/imagen.html#id2">10.1. 代码实现解读</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../aigc/imagen.html#id3">10.1.1. 第一阶段</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/imagen.html#id4">10.1.2. 第二阶段</a></li>
<li class="toctree-l4"><a class="reference internal" href="../aigc/imagen.html#id5">10.1.3. 第三阶段</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/imagen.html#id6">10.2. Imagen 总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../aigc/imagen.html#id7">10.3. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../deepspeed/index.html">deepspeed 详解-源码分析</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html">1. deepspeed - 预备知识</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html#torch-distribute">1.1. torch.distribute</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html#amp">1.2. 自动混合精度AMP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html#cuda-stream-and-event">1.3. cuda Stream and Event</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html#pin-memory">1.4. pin_memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html">2. deepspeed - 总入口</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#id1">2.1. 优化器的初始化</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#id2">2.1.1. 基础优化器</a></li>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#zero">2.1.2. 创建 ZeRO 优化器</a></li>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#f16">2.1.3. 创建 f16 半精度优化器</a></li>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#bf16">2.1.4. 创建 bf16 半精度优化器</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html">3. Stage3 - 参数分割</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#deepspeedzerooptimizer-stage3">3.1. DeepSpeedZeroOptimizer_Stage3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#deepspeedzerooffload">3.2. DeepSpeedZeRoOffload</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#init">3.3. Init 模块</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#convert-to-deepspeed-param">3.3.1. _convert_to_deepspeed_param</a></li>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#partition">3.3.2. partition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#partition-param-sec">3.3.3. partition_param_sec</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../deepspeed/stage3-hook.html">4. Stage3 - hook 注册</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html">5. Stage3 - 前后向过程</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html#id1">5.1. 参数还原</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html#all-gather-params">5.1.1. __all_gather_params</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html#id2">5.2. 参数重新分割</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html#release-param">5.2.1. release_param</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../deepspeed/index.html#id2">参考文献</a></li>
<li class="toctree-l1"><a class="reference internal" href="../audio/index.html">语音技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../audio/feature.html">1. 音频特征</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id2">1.1. 认识声音</a></li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id3">1.2. 认识声波</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id4">1.2.1. 物体的振动以及简谐振动</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id7">1.2.2. 什么是声波</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id8">1.2.3. 纯音和复合音</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#spectrum">1.2.4. 频谱 Spectrum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id10">1.2.5. 名词</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id13">1.3. 语音学</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id14">1.3.1. 发声原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id15">1.3.2. 听觉感应</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id16">1.4. 数字信号处理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id17">1.4.1. 模数转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#wav">1.4.2. 音频文件–WAV</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id18">1.5. 分帧与加窗</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id19">1.5.1. 预加重处理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id20">1.5.2. 分帧与加窗处理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id21">1.6. 声音的感官度量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#sound-pressure-level-spl">1.6.1. 声压与声压级(Sound Pressure Level,SPL)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#intensity-level-il">1.6.2. 声强与声强级(Intensity Level,IL）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id22">1.6.3. 声压与声强的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id23">1.6.4. 响度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id24">1.6.5. 音量计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id25">1.6.6. 频率与音高</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id26">1.7. 时域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id27">1.7.1. 短时能量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id28">1.7.2. 短时平均幅度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id29">1.7.3. 短时过零率</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id31">1.8. 频域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#spectrum-spectrogram">1.8.1. 声谱(spectrum)和时频谱(spectrogram)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#short-time-fourier-transform-stft">1.8.2. 短时傅里叶变换 Short-time Fourier transform (STFT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id33">1.8.3. 倒频谱</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id34">1.8.4. 色谱图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id35">1.9. 小波域特征</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id36">1.9.1. 离散小波域变换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id37">1.9.2. 小波域过零率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id38">1.9.3. 小波域质心</a></li>
<li class="toctree-l4"><a class="reference internal" href="../audio/feature.html#id39">1.9.4. 小波域子带能量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#mfcc">1.10. 语音识别的音频特征–MFCC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../audio/feature.html#id40">1.11. 参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../edm/index.html">教育领域数据挖掘</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../edm/bkt.html">1. 贝叶斯知识追踪(Bayesian Knowledge Tracing,BKT)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id1">1.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#hidden-markov-model-hmm">1.2. 隐马尔科夫模型(Hidden Markov Model,HMM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#bayesian-knowledge-tracing">1.3. 贝叶斯知识追踪(Bayesian Knowledge Tracing)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#bkt">1.3.1. BKT的参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#item-response-theory-irt">1.4. 项目反映理论(Item Response Theory,IRT)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#bktirt">1.5. BKT结合IRT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id5">1.6. 实验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id6">1.6.1. 数据集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id7">1.6.2. 实验方法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id8">1.6.3. 实验结果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id9">1.6.4. 项目代码</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id10">1.7. 未来工作</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id11">1.7.1. 题目难度的计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#irt">1.7.2. 多参数IRT模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edm/bkt.html#id12">1.7.3. 参数估计算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edm/bkt.html#id13">1.8. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../nlp/index.html">自然语言处理</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html">1. 文本去重</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id2">1.1. 背景</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id3">1.2. 技术思路</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id4">1.3. 相似（距离）算法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#euclidean-distance">1.3.1. 欧氏距离（Euclidean Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minkowski-distance">1.3.2. 闵科夫斯基距离（Minkowski Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#manhattan-distance">1.3.3. 曼哈顿距离（Manhattan Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#chebyshev-distance">1.3.4. 切比雪夫距离（Chebyshev Distance ）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#mahalanobis-distance">1.3.5. 马氏距离(Mahalanobis Distance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#cosine-similarity">1.3.6. 余弦夹角相似度(Cosine Similarity)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#hamming-distance">1.3.7. 汉明距离（Hamming Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#jaccard">1.3.8. Jaccard 系数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id5">1.3.9. 编辑距离</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id6">1.3.10. 最长公共字串</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id7">1.3.11. 最长公共子序列</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id8">1.4. 文本去重</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#kshingle">1.4.1. KShingle算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minhash">1.4.2. Minhash算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#simhash">1.4.3. simhash</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#ksentence">1.4.4. KSentence算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id9">1.5. 话术去重</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../nlp/bert/content.html">2. Attention&amp;Transformer&amp;Bert 简介</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#transformer">2.1. Transformer 从宏观到微观</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#seq2seq">2.1.1. seq2seq</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id1">2.1.2. 模型的输入</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#self-attention">2.2. Self-Attention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id2">2.2.1. 什么是注意力？</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id3">2.2.2. 加权求和</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#id4">2.2.3. 位置编码</a></li>
<li class="toctree-l4"><a class="reference internal" href="../nlp/bert/content.html#multi-head">2.2.4. 多头注意力（Multi-head）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#attention">2.3. Attention 机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nlp/bert/content.html#id5">2.4. 其它参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../rst_tutorial/latex.html">latex demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../rst_tutorial/latex.html#latex">latex</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../rst_tutorial/latex.html#how-to-write-an-m-x-n-matrix-in-latex">How to write an m x n matrix in LaTeX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-big-parentheses">With big parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-parentheses">With parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-brackets">With brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#latex-matrix-with-no-bracket">LateX matrix with no bracket</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-vertical-bar-brackets">With vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-curly-brackets">with curly brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#with-double-vertical-bar-brackets">with double vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#small-inline-matrix">small inline matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../rst_tutorial/latex.html#examples-matrix-2-x-2-in-latex">Examples matrix 2 x 2 in LaTeX</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../rst_tutorial/graphviz.html">graphviz demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../rst_tutorial/graphviz.html#id1">布局</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rst_tutorial/graphviz.html#id2">其它</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">读书笔记</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">《统计因果推断推理入门》读书笔记</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html">1. 第三章 干预的效果</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id2">1.1. 第3.1节 干预</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id3">1.2. 第3.2节 校正公式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id8">1.3. 第3.3节 后门准则</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html">《深度学习推荐系统》读书笔记</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id2">重点</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id3">冷启动</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id4">探索与利用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id5">召回层的主要策略</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#embedding">协同过滤 &amp; Embedding 向量</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="移动版导航菜单" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">张振虎的博客</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="页面导航">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index_html.html">概率图</a></li>
      <li class="breadcrumb-item active"><span class="section-number">19. </span>广义线性模型</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/probability_model/25.广义线性模型_34.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1><span class="section-number">19. </span>广义线性模型<a class="headerlink" href="#id1" title="此标题的永久链接"></a></h1>
<p>我们已经学习讨论了回归模型和分类模型，而分类模型又可以分为生成式模型和判别式模型两大类。
在这些模型中，我们都是在寻找条件概率分布 <span class="math notranslate nohighlight">\(p(Y|X)\)</span> ，
其中生成式分类模型把输入变量 <span class="math notranslate nohighlight">\(X\)</span> 和输出变量 <span class="math notranslate nohighlight">\(Y\)</span> 都看做是随机变量，
然后通过贝叶斯定理的方法求得（后验）条件概率 <span class="math notranslate nohighlight">\(p(Y|X)\)</span> 。
然而，回归模型(线性回归)和判别式分类模型(比如逻辑回归)
直接为条件概率 <span class="math notranslate nohighlight">\(p(Y|X)\)</span> 选取一个参数化的概率分布。
本章我们讨论经典线性回归和逻辑回归模型的扩展，在正式讨论前，
我们首先回顾一下线性回顾模型和逻辑回归模型。</p>
<p><strong>线性回归</strong></p>
<p>在线性回归模型中，输出变量 <span class="math notranslate nohighlight">\(Y\)</span> 的值域是实数值，并且假设其服从高斯分布。
此时输出变量 <span class="math notranslate nohighlight">\(Y\)</span> 的边缘概率分布可以写成：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-0">
<span class="eqno">(19.1)<a class="headerlink" href="#equation-probability-model-25-34-0" title="此公式的永久链接"></a></span>\[p(Y) \sim \mathcal{N}(\mu,\sigma^2)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mu\)</span> 是均值(期望)参数，表示变量 <span class="math notranslate nohighlight">\(Y\)</span> 的期望 <span class="math notranslate nohighlight">\(\mu=\mathbb{E}[Y]\)</span> ，
<span class="math notranslate nohighlight">\(\sigma^2\)</span> 是方差参数，反映变量的波动性。</p>
<p>在回归问题中我们需要对条件概率建模 <span class="math notranslate nohighlight">\(p(Y|X)\)</span> 进行建模，所谓的条件概率，
就是在变量 <span class="math notranslate nohighlight">\(X\)</span> 取某个值时(条件下)变量 <span class="math notranslate nohighlight">\(Y\)</span> 服从什么样的概率分布。
也就是变量 <span class="math notranslate nohighlight">\(X\)</span> 取不同值时，变量 <span class="math notranslate nohighlight">\(Y\)</span> 服从”不同的分布”，
这里的”不同分布”通常是不同参数的同种分布。</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>如果在 <span class="math notranslate nohighlight">\(X\)</span> 取不同值时，变量 <span class="math notranslate nohighlight">\(Y\)</span> 服从同一个分布(参数也相同)，
那么说明变量 <span class="math notranslate nohighlight">\(X\)</span> 和变量 <span class="math notranslate nohighlight">\(Y\)</span> 是相互独立的，此时有 <span class="math notranslate nohighlight">\(p(Y)=P(Y|X)\)</span> 。
如果某个输入变量 <span class="math notranslate nohighlight">\(X\)</span> 与输出变量 <span class="math notranslate nohighlight">\(Y\)</span> 是相互独立的，
那么这个输入变量就不能作为我们的特征变量，因为它对确定 <span class="math notranslate nohighlight">\(Y\)</span> 没有任何贡献(影响)。
所以无论是在分类问题还是回归问题中，作为特征的输入变量 <span class="math notranslate nohighlight">\(X\)</span> 都不能独立于输出变量 <span class="math notranslate nohighlight">\(Y\)</span> 。</p>
</div>
<p>线性回顾模型是通过在边缘概率分布 <span class="math notranslate nohighlight">\(p(Y)\)</span> 的基础上”引入”变量 <span class="math notranslate nohighlight">\(X\)</span> 进而得到条件概率分布 <span class="math notranslate nohighlight">\(p(Y|X)\)</span> 。
我们令变量 <span class="math notranslate nohighlight">\(Y\)</span> 的期望(均值)值 <span class="math notranslate nohighlight">\(\mu\)</span> 等于变量 <span class="math notranslate nohighlight">\(X\)</span> 的线性组合，即：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-1">
<span class="eqno">(19.2)<a class="headerlink" href="#equation-probability-model-25-34-1" title="此公式的永久链接"></a></span>\[\mu = \beta^T x\]</div>
<p>也就是说在不同的 <span class="math notranslate nohighlight">\(x\)</span> 值下，变量 <span class="math notranslate nohighlight">\(Y\)</span> 的均值不同，进而我们就得到了条件概率分布 p(Y|X)：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-2">
<span class="eqno">(19.3)<a class="headerlink" href="#equation-probability-model-25-34-2" title="此公式的永久链接"></a></span>\[p(Y|X)  \sim \mathcal{N}(\beta^T x,\sigma^2)\]</div>
<p>通过参数化 <span class="math notranslate nohighlight">\(\mathbb{E}[Y]\)</span> 将变量 <span class="math notranslate nohighlight">\(X\)</span> 引入到边缘概率分布 <span class="math notranslate nohighlight">\(p(Y)\)</span> 进而得到条件概率分布 <span class="math notranslate nohighlight">\(p(Y|X)\)</span>
。注意，在经典线性回归模型中，方差 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 被认为是已知的，并且是和均值独立的，
只有 <span class="math notranslate nohighlight">\(\beta\)</span> 是需要学习的未知参数。</p>
<p><strong>逻辑回归</strong></p>
<p>现在我们回顾一下逻辑回归模型，在逻辑回归模型中，输出变量 <span class="math notranslate nohighlight">\(Y\)</span> 是伯努利变量，服从伯努利分布：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-3">
<span class="eqno">(19.4)<a class="headerlink" href="#equation-probability-model-25-34-3" title="此公式的永久链接"></a></span>\[p(y) = \mu^y(1-\mu)^{1-y}\]</div>
<p>在逻辑回归模型中，我们假设均值参数 <span class="math notranslate nohighlight">\(\mu\)</span> 是一个关于线性组合的 <span class="math notranslate nohighlight">\(\beta^Tx\)</span> 的sigmoid函数：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-4">
<span class="eqno">(19.5)<a class="headerlink" href="#equation-probability-model-25-34-4" title="此公式的永久链接"></a></span>\[\mu=\mu(x)=\frac{1}{1+e^{-\beta^T x}}\]</div>
<p>条件概率分布 <span class="math notranslate nohighlight">\(p(Y|X)\)</span> 就是：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-5">
<span class="eqno">(19.6)<a class="headerlink" href="#equation-probability-model-25-34-5" title="此公式的永久链接"></a></span>\[p(y|x) =\mu(x)^y(1-\mu(x))^{1-y}\]</div>
<p>我们发现线性回归模型和逻辑回归模型都是通过参数化 <span class="math notranslate nohighlight">\(Y\)</span> 的期望 <span class="math notranslate nohighlight">\(\mu=\mathbb{E}[Y]\)</span> ，
进而得到条件概率分布 <span class="math notranslate nohighlight">\(p(Y|X)\)</span> ，事实上，我们将这种方式进行扩展，
就得到了广义线性模型(generalized linear model,GLM)。</p>
<section id="id2">
<h2><span class="section-number">19.1. </span>定义<a class="headerlink" href="#id2" title="此标题的永久链接"></a></h2>
<figure class="align-center" id="id17">
<span id="fg-34-1"></span><a class="reference internal image-reference" href="../_images/34_11.jpg"><img alt="../_images/34_11.jpg" src="../_images/34_11.jpg" style="width: 599.9px; height: 187.6px;" /></a>
<figcaption>
<p><span class="caption-number">图 19.1.1 </span><span class="caption-text">广义线性模型变量之间的关系</span><a class="headerlink" href="#id17" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>广义线性模型是对经典线性模型的扩展，将输出变量 <span class="math notranslate nohighlight">\(Y\)</span> 的条件概率分布扩展到指数族分布(指数族的一个子集，不是全部)，
<a class="reference internal" href="#fg-34-1"><span class="std std-numref">图 19.1.1</span></a> 展示了广义线性模型框架下各个变量之间的关系。</p>
<ul class="simple">
<li><p>输入变量 <span class="math notranslate nohighlight">\(X\)</span> 和系数 <span class="math notranslate nohighlight">\(\beta\)</span> 组成一个线性关系， <span class="math notranslate nohighlight">\(\eta=\beta^T x\)</span> ，
<span class="math notranslate nohighlight">\(\eta\)</span> 被称为线性预测器(linear predictor)，<span class="math notranslate nohighlight">\(\beta\)</span> 是定义的未知参数。</p></li>
<li><p>通过一个链接函数(link function)将 <span class="math notranslate nohighlight">\(\eta\)</span> 和变量 <span class="math notranslate nohighlight">\(Y\)</span> 的均值 <span class="math notranslate nohighlight">\(\mu\)</span> 关联起来，<span class="math notranslate nohighlight">\(\eta=g(\mu)\)</span> ，
函数 <span class="math notranslate nohighlight">\(g\)</span> 称为链接函数。 <span class="math notranslate nohighlight">\(g\)</span> 的反函数就是激活函数(active function)，<span class="math notranslate nohighlight">\(\mu=g^{-1}(\eta)\)</span>
，有的资料中也称为响应函数(response function)、均值函数(mean function)。
激活函数可以是线性的，也可以是非线性的，
比如，经典线性回归模型的激活函数为 <span class="math notranslate nohighlight">\(\mu=f(\eta)=\eta\)</span> ，逻辑回归模型的激活函数为 <span class="math notranslate nohighlight">\(\mu=f(\eta)=sigmoid(\eta)\)</span> 。</p></li>
<li><p>在广义线性模型的框架下，响应变量 <span class="math notranslate nohighlight">\(Y\)</span> 被看做是随机变量，并且其概率分布是指数族分布的一种，<span class="math notranslate nohighlight">\(\theta\)</span> 是分布的参数。
<span class="math notranslate nohighlight">\(\theta\)</span> 和 <span class="math notranslate nohighlight">\(\mu\)</span> 存在一一映射关系，我们用函数 <span class="math notranslate nohighlight">\(\psi\)</span> 表示这种关系。</p></li>
</ul>
<p>显然，一个广义线性模型有三个关键组件：</p>
<ol class="arabic simple">
<li><p>一个线性预测器 <span class="math notranslate nohighlight">\(\eta=\beta^T x\)</span> ，被称为系统组件(systematic component)。</p></li>
<li><p>一个链接函数 <span class="math notranslate nohighlight">\(g\)</span> 使得 <span class="math notranslate nohighlight">\(\mathbb{E}[Y|X]=\mu=g^{-1}(\eta)\)</span> ，链接函数描述系统组件和随机组件之间的关系。</p></li>
<li><p>一个指数族分布作为响应变量 <span class="math notranslate nohighlight">\(Y\)</span> 概率分布 <span class="math notranslate nohighlight">\(p(Y;\theta)\)</span> ，被称为随机组件(random component)。</p></li>
</ol>
<section id="id3">
<h3><span class="section-number">19.1.1. </span>指数族分布<a class="headerlink" href="#id3" title="此标题的永久链接"></a></h3>
<p>我们回顾一下 <a class="reference internal" href="18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-1"><span class="std std-numref">节 3.1</span></a> ，指数族的概率分布的概率密度(质量)函数的一般形式为：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-08">
<span class="eqno">(19.1.10)<a class="headerlink" href="#equation-eq-34-08" title="此公式的永久链接"></a></span>\[p(y|\theta) = \exp \{theta^T T(y) - A(\theta) + S(y)\}\]</div>
<p>然而在GLM中，通常并不使用上述形式指数族，而是指数族的一个特殊子集，叫做自然指数族(Natural Exponential Family,NEF)，
满足条件 <span class="math notranslate nohighlight">\(T(y)=y\)</span> 的指数族被称为自然指数族。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-09">
<span class="eqno">(19.1.11)<a class="headerlink" href="#equation-eq-34-09" title="此公式的永久链接"></a></span>\[p(y|\theta) = \exp \{\theta^T y - A(\theta) + S(y)\}\]</div>
<p>指数族的这个形式被称为自然形式(natural form)或者规范形式(canonical form)，其中参数 <span class="math notranslate nohighlight">\(\theta\)</span>
称为自然参数(natural parameter)或者规范参数(canonical parameter)。
自然指数族相对于一般指数族的关键变化就是要求 <span class="math notranslate nohighlight">\(T(y)=y\)</span> ，因为只有满足这个条件时
<span class="math notranslate nohighlight">\(A(\theta)\)</span> 的一阶导数才等于 <span class="math notranslate nohighlight">\(Y\)</span> 的期望 <span class="math notranslate nohighlight">\(\mathbb{E}[Y]\)</span> 。
在GLM的定义中，我们并不直接使用 <a class="reference internal" href="#equation-eq-34-09">公式(19.1.11)</a> 形式，而是在这基础上引入一个额外的参数 <span class="math notranslate nohighlight">\(\phi\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-edf">
<span class="eqno">(19.1.12)<a class="headerlink" href="#equation-eq-34-edf" title="此公式的永久链接"></a></span>\[p(y|\theta) = \exp \{\frac{\theta y - b(\theta)}{a(\phi)} + c(y,\phi)\}\]</div>
<p>这种形式的指数族通常被称为指数分散族(exponential dispersion family,EDF)，
参数 <span class="math notranslate nohighlight">\(\phi\)</span> 称为分散参数(dispersion parameter)，
<span class="math notranslate nohighlight">\(a(\phi)\)</span> 称为分散函数(dispersion function)，
分散参数 <span class="math notranslate nohighlight">\(\phi\)</span> 和分布的方差有关。
并不是所有的指数族分布都存在 <span class="math notranslate nohighlight">\(\phi\)</span>
，比如对于泊松分布、二项式分布来说，<span class="math notranslate nohighlight">\(\phi=1\)</span>
。</p>
<p>在EDF中，<span class="math notranslate nohighlight">\(\theta\)</span> 不再是向量，而是一个标量，仍然称为 canonical parameter，
<strong>参数</strong> <span class="math notranslate nohighlight">\(\theta\)</span> <strong>是和</strong> <span class="math notranslate nohighlight">\(Y\)</span> <strong>的均值</strong> <span class="math notranslate nohighlight">\(\mu\)</span> <strong>相关的，</strong>
<span class="math notranslate nohighlight">\(\theta\)</span> 和 <span class="math notranslate nohighlight">\(\mu\)</span> 之间存在一一映射的函数关系 <span class="math notranslate nohighlight">\(\theta=\psi(\mu)\)</span> 。
换句话说，<span class="math notranslate nohighlight">\(\theta\)</span> 和 <span class="math notranslate nohighlight">\(\mu\)</span> 可以互相转化。
通常在GLM中，只有参数 <span class="math notranslate nohighlight">\(\theta\)</span> 作为模型的未知参数，此时称为单参数模。
单参数模型指的是模型中只有 <span class="math notranslate nohighlight">\(\theta\)</span> 是未知参数，而 <span class="math notranslate nohighlight">\(\phi\)</span> 是已知的，</p>
<p><strong>分散参数(dispersion parameter)</strong></p>
<p>在最初的GLM论文中(Nelder and Wedderburn, 1972)把 <span class="math notranslate nohighlight">\(a(\phi)\)</span>
称为比例因子(scale factor)， 并且没有给参数 <span class="math notranslate nohighlight">\(\phi\)</span>
单独命名。后来在1974年 Royal Statistical Society 发布了首个
GLM的软件工具包(Generalized Linear Interactive Modelling,GLIM)，
在GLIM中把 <span class="math notranslate nohighlight">\(a(\phi)\)</span> 定义成：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-6">
<span class="eqno">(19.1.13)<a class="headerlink" href="#equation-probability-model-25-34-6" title="此公式的永久链接"></a></span>\[a(\phi) = \frac{\phi}{w}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(w\)</span> 是先验权重(prior weight)，并且把 <span class="math notranslate nohighlight">\(\phi\)</span>
称为尺度参数(scale parameter)，
这就是导致了对 <span class="math notranslate nohighlight">\(\phi\)</span> 命名产生了歧义。
因为”scale”这个词在统计学还有其它用法，容易产生混淆，
所以在1980s(McCullagh and Nelder)初版的GLM书籍中，
把 <span class="math notranslate nohighlight">\(\phi\)</span> 命名为”dispersion parameter”，
后来也就沿用了这种叫法。
但是由于GLIM流行了很久，导致”scale”的叫法还存在很多资料中。</p>
<p>在很多GLM的工具包中，都会把 <span class="math notranslate nohighlight">\(a(\phi)\)</span> 定义成如下形式：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-7">
<span class="eqno">(19.1.14)<a class="headerlink" href="#equation-probability-model-25-34-7" title="此公式的永久链接"></a></span>\[a(\phi)_n = \frac{\phi}{w_n}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(w_n\)</span> 是观测样本的权重，不同的样本可以拥有不同的权重值，
比如在利用ML进行参数估计时，对于某些样本设置成 <span class="math notranslate nohighlight">\(w_n=0\)</span>
，这就相当于抛弃了这些样本。</p>
<p><strong>累积函数(cumulant function)</strong></p>
<p>我们知道在 <a class="reference internal" href="#equation-eq-34-09">公式(19.1.11)</a> 的指数族形式中 <span class="math notranslate nohighlight">\(A(\theta)\)</span> 称为累积函数(cumulant function)，
可以用 <span class="math notranslate nohighlight">\(A(\theta)\)</span> 的导数求出分布的矩，一阶导数是分布的期望，
二阶导数是分布的方差。
然而在GLM中我们使用的是 <a class="reference internal" href="#equation-eq-34-edf">公式(19.1.12)</a> 的形式，
其中 <span class="math notranslate nohighlight">\(b(\theta)\)</span> 本质上就是 <span class="math notranslate nohighlight">\(A(\theta)\)</span>
，二者关系是：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-8">
<span class="eqno">(19.1.15)<a class="headerlink" href="#equation-probability-model-25-34-8" title="此公式的永久链接"></a></span>\[A(\theta) = \frac{ b(\theta) }{a(\phi)}\]</div>
<p>所以我们同样把 <span class="math notranslate nohighlight">\(b(\theta)\)</span> 被称为累积函数(cumulant function)，
并且它同样和分布的矩(moments)有关。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-9">
<span class="eqno">(19.1.16)<a class="headerlink" href="#equation-probability-model-25-34-9" title="此公式的永久链接"></a></span>\[\mathbb{E}[Y] = b'(\theta)=\mu\]</div>
<div class="math notranslate nohighlight" id="equation-eq-34-20">
<span class="eqno">(19.1.17)<a class="headerlink" href="#equation-eq-34-20" title="此公式的永久链接"></a></span>\[Var(Y) = A''(\theta)=a(\phi)b''(\theta)\]</div>
<p><strong>方差结构</strong></p>
<p>在EDF(指数分散族，Exponential Dispersion Family)中，
分布的方差可以表示成两部分的乘积（ <a class="reference internal" href="#equation-eq-34-20">公式(19.1.17)</a> ），
一部分是分散函数 <span class="math notranslate nohighlight">\(a(\phi)\)</span>
，另一部分是累计函数的二阶导数 <span class="math notranslate nohighlight">\(b''(\theta)\)</span>
实际上，<span class="math notranslate nohighlight">\(b''(\theta)\)</span> 是一个关于 <span class="math notranslate nohighlight">\(\mu\)</span> 函数，
首先 <span class="math notranslate nohighlight">\(b(\theta)\)</span> 是一个关于 <span class="math notranslate nohighlight">\(\theta\)</span> 的函数，
其二阶导数自然也是一个关于 <span class="math notranslate nohighlight">\(\theta\)</span> 的函数(或者是一个常数)。
然而自然参数 <span class="math notranslate nohighlight">\(\theta\)</span> 和均值参数 <span class="math notranslate nohighlight">\(\mu\)</span>
存在一一对应关系，所以一定可以把 <span class="math notranslate nohighlight">\(\theta\)</span> 替换成 <span class="math notranslate nohighlight">\(\mu\)</span>
，所以 <span class="math notranslate nohighlight">\(b''(\theta)\)</span> 一定可以写成一个关于 <span class="math notranslate nohighlight">\(\mu\)</span> 的函数，
因此我们定义 <span class="math notranslate nohighlight">\(\nu(\mu)=b''(\theta)\)</span>
。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-10">
<span class="eqno">(19.1.18)<a class="headerlink" href="#equation-probability-model-25-34-10" title="此公式的永久链接"></a></span>\[Var(Y) =\sigma^2 = b''(\theta) a(\phi) = \nu(\mu)a(\phi)\]</div>
<p>函数 <span class="math notranslate nohighlight">\(\nu(\mu)=b''(\theta)\)</span> 称为方差函数(variance function)，其将分布的均值参数 <span class="math notranslate nohighlight">\(\mu\)</span>
和分布的方差关联在一起。
如果其值一个常数值，说明均值和方差是独立无关的；反之，如果其最终是 <span class="math notranslate nohighlight">\(\mu\)</span> 的函数，说明均值和方差是相关联的。
在统计中，方差函数是一个平滑函数，它把随机量的方差描述成其均值的函数。
在高斯分布中， <span class="math notranslate nohighlight">\(b''(\theta)=1\)</span> ，所以方差和均值是相互独立的，
对于其他分布，这是不成立的，这使得高斯分布是特例。
分散参数
<span class="math notranslate nohighlight">\(\phi\)</span>
也影响着分布的方差，
参数 <span class="math notranslate nohighlight">\(\theta\)</span> 和 <span class="math notranslate nohighlight">\(\phi\)</span> 本质上是位置(locate)和比例(scale)参数，
位置参数反映数据的均值，比例参数反映数据方差。
当然对于很多分布，
<span class="math notranslate nohighlight">\(a(\phi)=1\)</span> 。</p>
<p>在经典线性回归模型中，输入特征数据 <span class="math notranslate nohighlight">\(x\)</span> 通过线性组合 <span class="math notranslate nohighlight">\(\eta=\beta^T x\)</span>
影响着响应变量 <span class="math notranslate nohighlight">\(Y\)</span> (高斯分布) 的均值 <span class="math notranslate nohighlight">\(\mu=\eta=\beta^T x\)</span> ，
所有的观测样本共用参数 <span class="math notranslate nohighlight">\(\beta\)</span> (对于任意 <span class="math notranslate nohighlight">\(x\)</span> ，都是同样的 <span class="math notranslate nohighlight">\(\beta\)</span> 值)，
当 <span class="math notranslate nohighlight">\(x\)</span> 不同时， 高斯变量 <span class="math notranslate nohighlight">\(Y\)</span> 拥有不同的均值 <span class="math notranslate nohighlight">\(\mu\)</span> ，
通过这种方式实现了条件概率分布 <span class="math notranslate nohighlight">\(p(Y|X)\)</span> 的表达。
但是对于高斯变量 <span class="math notranslate nohighlight">\(Y\)</span> 的方差参数 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 并没有假设为未知参数，而是假设其为已知的值，
并且对于任意的观测样本 <span class="math notranslate nohighlight">\(x\)</span> 都是一样的值。
然而，在GLM的框架下，是可以允许不同观测样本有不同的方差，
而这是通过 <span class="math notranslate nohighlight">\(a(\phi)\)</span> 实现的。
此时函数 <span class="math notranslate nohighlight">\(a(\phi)\)</span> 通常被定义成如下的形式：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-11">
<span class="eqno">(19.1.19)<a class="headerlink" href="#equation-probability-model-25-34-11" title="此公式的永久链接"></a></span>\[a(\phi) = \frac{\phi}{w_n}\]</div>
<p>通常对于所有的观测样本来说，<span class="math notranslate nohighlight">\(\phi\)</span> 是一个相同的，而 <span class="math notranslate nohighlight">\(w\)</span> 可以根据不同的观测样本取不同的值，
下标 <span class="math notranslate nohighlight">\(n\)</span> 表示样本编号。
<span class="math notranslate nohighlight">\(w\)</span> 被称为先验权重(prior weight)，通常是根据额外的先验信息确定的。
如果所有观测样本具有相同的方差假设，那么 <span class="math notranslate nohighlight">\(w\)</span> 值通常就是1；反之，<span class="math notranslate nohighlight">\(w\)</span> 可以是和样本相关的，
不同的样本采用不同的值。</p>
<table class="docutils align-default" id="id18">
<caption><span class="caption-number">表 19.1.1 </span><span class="caption-text">常见分布的方差函数</span><a class="headerlink" href="#id18" title="此表格的永久链接"></a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>分布</p></th>
<th class="head"><p>方差函数</p></th>
<th class="head"><p>约束</p></th>
<th class="head"><p>导数 <span class="math notranslate nohighlight">\(\partial \nu(\mu) / \partial\mu\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Gaussian</p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \mu \in \mathcal{R} \\ y \in \mathcal{R} \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Bernoulli</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu(1-\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} 0&lt;\mu&lt;1 \\ 0 \le y \le 1 \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1-2\mu\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Binomial(k)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu(1-\mu/k)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} 0&lt;\mu&lt;k \\ 0 \le y \le k \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1-2\mu/k\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Poisson</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \mu &gt;0 \\  y \ge 0 \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Gamma</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \mu &gt;0 \\  y &gt; 0 \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(2\mu\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Inverse Gaussian</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu^3\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \mu &gt;0 \\  y &gt; 0 \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(3\mu^2\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Negative binomial(<span class="math notranslate nohighlight">\(\alpha\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu+\alpha\mu^3\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \mu &gt;0 \\  y \ge 0 \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1+2\alpha\mu\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Power(<span class="math notranslate nohighlight">\(k\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu^k\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \mu &gt;0 \\  k \ne 0,1,2  \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(k\mu^{k-1}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Quasi</p></td>
<td><p><span class="math notranslate nohighlight">\(\nu(\mu)\)</span></p></td>
<td></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\partial \nu(\mu)}{\partial \mu}\)</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="id4">
<h3><span class="section-number">19.1.2. </span>链接函数<a class="headerlink" href="#id4" title="此标题的永久链接"></a></h3>
<p>链接函数 <span class="math notranslate nohighlight">\(g\)</span> 是用来链接线性预测器 <span class="math notranslate nohighlight">\(\eta\)</span> 和均值 <span class="math notranslate nohighlight">\(\mu\)</span> 的，要求 <strong>必须是连续可微，并且可逆的。</strong>
原则上，任何单调的连续可微函数都可以，但是对于标准GLM，有一些方便且通用的选择。</p>
<p>在高斯线性模型中，链接函数是恒等函数 <span class="math notranslate nohighlight">\(\eta=g(\mu)=\mu\)</span> 。
在泊松分布中，均值 <span class="math notranslate nohighlight">\(\mu\)</span> 必须是正的，所以 <span class="math notranslate nohighlight">\(\eta=\mu\)</span> 不再适用，因为 <span class="math notranslate nohighlight">\(\eta=\beta^Tx\)</span>
的取值范围值整个实数域。对于泊松分布，标准的链接函数选择是对数函数 <span class="math notranslate nohighlight">\(\eta=log \mu\)</span> ，此时 <span class="math notranslate nohighlight">\(\mu=e^{\eta}\)</span>
确保了 <span class="math notranslate nohighlight">\(\mu\)</span> 为正数。
<strong>链接函数本质上，就是把实数域范围的</strong> <span class="math notranslate nohighlight">\(\eta\)</span> <strong>转换到特定分布合法的</strong> <span class="math notranslate nohighlight">\(\mu\)</span> <strong>值空间上。</strong></p>
<aside class="topic">
<p class="topic-title">规范链接(canonical link)</p>
<p>当链接函数使得 <span class="math notranslate nohighlight">\(\eta=\theta\)</span> 时，称为规范链接(canonical link)函数。
实际上规范链接函数满足 <span class="math notranslate nohighlight">\(\eta=g(\mu)=\psi(\mu)=\theta\)</span> ，
换句话说，对于一个特定的指数族分布，其规范链接函数为 <span class="math notranslate nohighlight">\(g=\psi\)</span> 。
使用规范链接函数可以带来很多统计属性，最直接的就是可以简化参数估计的计算过程。</p>
</aside>
<table class="docutils align-default" id="id19">
<caption><span class="caption-number">表 19.1.2 </span><span class="caption-text">常见链接函数</span><a class="headerlink" href="#id19" title="此表格的永久链接"></a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>名称</p></th>
<th class="head"><p>链接函数</p></th>
<th class="head"><p>激活函数(反链接)</p></th>
<th class="head"><blockquote>
<div><p><span class="math notranslate nohighlight">\(\mu\)</span> 的空间</p>
</div></blockquote>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Identity</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\eta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu \in \mathcal{R}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Logit</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\ln\{\mu/(1-\mu)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=e^\eta/(1+e^\eta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu \in (0,1)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Log</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\ln(\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=e^\eta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu &gt;0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Negative binomial( <span class="math notranslate nohighlight">\(\alpha\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\ln\{\mu/(\mu+1/\alpha)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=e^\eta/\{ \alpha(1-e^\eta)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu &gt;0\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Log-complement</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\ln(1-\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=1-e^\eta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu &lt;1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Log-log</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=-ln \{- \ln(\mu)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\exp\{-\exp(-\eta)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu \in (0,1)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Complementary log-log</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=ln \{- \ln(1-\mu)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=1-\exp\{-\exp(\eta)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu \in (0,1)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Probit</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\Phi^{-1}(\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\Phi(\eta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu \in (0,1)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Reciprocal</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=1/\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=1/\eta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu \in \mathcal{R}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Power(<span class="math notranslate nohighlight">\(\alpha=-2\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=1/\mu^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=1/\sqrt{\eta}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu &gt;0\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Power(<span class="math notranslate nohighlight">\(\alpha\)</span>) <span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr}\alpha \ne 0\\ \alpha=0 \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\left\{  \begin{array}{lr}\mu^\alpha \\ \ln(\mu) \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\left\{  \begin{array}{lr}\eta^{1/\alpha} \\ \exp(\eta) \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu \in \mathcal{R}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Odds power(<span class="math notranslate nohighlight">\(\alpha\)</span>) <span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr}\alpha \ne 0\\ \alpha=0 \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\left\{  \begin{array}{lr} \frac{\mu/(1-\mu)^\alpha-1}{\alpha} \\ \ln \left( \frac{\mu}{1-\mu} \right) \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\left\{  \begin{array}{lr} \frac{(1+\alpha\eta)^{1/\alpha}}{1+(1+\alpha\eta)^{1/\alpha}} \\ \frac{e^\eta}{1+e^\eta} \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu \in (0,1)\)</span></p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default" id="id20">
<caption><span class="caption-number">表 19.1.3 </span><span class="caption-text">链接函数的导数</span><a class="headerlink" href="#id20" title="此表格的永久链接"></a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>名称</p></th>
<th class="head"><p>链接函数</p></th>
<th class="head"><p>一阶导数 <span class="math notranslate nohighlight">\(\triangle=\partial \eta /\partial \mu\)</span></p></th>
<th class="head"><p>二阶导数</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Identity</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Logit</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\ln\{\mu/(1-\mu)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1/\{\mu(1-\mu)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((2\mu-1)\triangle^2\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Log</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\ln(\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1/\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\triangle^2\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Negative binomial( <span class="math notranslate nohighlight">\(\alpha\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\ln\{\alpha\mu/(1+\alpha\mu)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1/(\mu+\alpha\mu^2)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\triangle^2(1+2\alpha\mu)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Log-complement</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\ln(1-\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-1/(1-\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\triangle^2\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Log-log</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=-ln \{- \ln(\mu)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-1/\{\mu\ln(\mu)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\{1+\ln(\mu)\}\triangle^2\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Complementary log-log</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=ln \{- \ln(1-\mu)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\{(\mu-1)\ln(1-\mu)\}^{-1}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\{1+\ln(1-\mu)\}\triangle^2\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Probit</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\Phi^{-1}(\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1/\phi\{\Phi^{-1}(\mu)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\eta\triangle^2\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Reciprocal</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=1/\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-1/\mu^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-2\triangle / \mu\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Power(<span class="math notranslate nohighlight">\(\alpha=-2\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=1/\mu^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-2/\mu^3\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-3\triangle / \mu\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Power(<span class="math notranslate nohighlight">\(\alpha\)</span>) <span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr}\alpha \ne 0\\ \alpha=0 \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\left\{  \begin{array}{lr}\mu^\alpha \\ \ln(\mu) \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \alpha \mu ^{\alpha-1} \\ 1/\mu \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} (\alpha-1)\triangle/\alpha \\ -\triangle^2  \end{array} \right .\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Odds power(<span class="math notranslate nohighlight">\(\alpha\)</span>) <span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr}\alpha \ne 0\\ \alpha=0 \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\left\{  \begin{array}{lr} \frac{\mu/(1-\mu)^\alpha-1}{\alpha} \\ \ln \left( \frac{\mu}{1-\mu} \right) \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \frac{\mu^{\alpha-1}}{(1-\mu)^{\alpha+1}} \\ \frac{1}{\mu(1-\mu)} \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \triangle\left(\frac{1-1/\alpha}{1-\mu} +\alpha+1\right) \\ \mu\triangle^2  \end{array} \right .\)</span></p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default" id="id21">
<caption><span class="caption-number">表 19.1.4 </span><span class="caption-text">激活函数的导数</span><a class="headerlink" href="#id21" title="此表格的永久链接"></a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>链接函数名称</p></th>
<th class="head"><p>激活函数(反链接)</p></th>
<th class="head"><p>一阶导数 <span class="math notranslate nohighlight">\(\triangle=\partial \mu /\partial \eta\)</span></p></th>
<th class="head"><p>二阶导数</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Identity</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\eta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Logit</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=e^\eta/(1+e^\eta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu(1-\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\triangle(1-2\mu)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Log</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=e^\eta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\triangle\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Negative binomial( <span class="math notranslate nohighlight">\(\alpha\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=e^\eta/\{ \alpha(1-e^\eta)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu+\alpha\mu^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\triangle(1+2\alpha\mu)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Log-complement</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=1-e^\eta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu-1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\triangle\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Log-log</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\exp\{-\exp(-\eta)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\mu\ln(\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\triangle\{ 1+\ln(\mu)\}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Complementary log-log</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=1-\exp\{-\exp(\eta)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((\mu-1)\ln(1-\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\triangle\{1+\ln(1-\mu)\}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Probit</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\Phi(\eta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\phi(\eta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\triangle \eta\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Reciprocal</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=1/\eta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\mu^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-2\triangle\mu\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Power(<span class="math notranslate nohighlight">\(\alpha=-2\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=1/\sqrt{\eta}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\mu^3/2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(3\triangle^2 / \mu\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Power(<span class="math notranslate nohighlight">\(\alpha\)</span>) <span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr}\alpha \ne 0\\ \alpha=0 \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\left\{  \begin{array}{lr}\eta^{1/\alpha} \\ \exp(\eta) \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \frac{1}{\alpha} \mu^{1-\alpha} \\  \mu \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \triangle(1/\alpha -1)/\mu^\alpha \\ \triangle  \end{array} \right .\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Odds power(<span class="math notranslate nohighlight">\(\alpha\)</span>) <span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr}\alpha \ne 0\\ \alpha=0 \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\left\{  \begin{array}{lr} \frac{(1+\alpha\eta)^{1/\alpha}}{1+(1+\alpha\eta)^{1/\alpha}} \\ \frac{e^\eta}{1+e^\eta} \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \frac{\mu(1-\mu)}{1+\alpha\eta} \\ \mu(1-\mu)  \end{array} \right .\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{  \begin{array}{lr} \triangle \left ( 1-2\mu-\frac{\alpha}{1+\alpha\eta}   \right) \\  \triangle(1-2\mu) \end{array} \right .\)</span></p></td>
</tr>
</tbody>
</table>
<p>当无法合理地假设数据是正态分布的或者响应变量的结果集有限集时，传统的线性回归模型是不合适的。
此外，在许多情况下，传统线性回归模型的同方差假设是站不住脚的，此时传统线性回归模型也是不合适的。
GLM允许对传统线性回归模型进行扩展，以突破这些限制。
GLM框架通过链接函数把线性预测变量 <span class="math notranslate nohighlight">\(\eta=\beta^Tx\)</span> 映射到服从指数族分布的响应变量的均值参数 <span class="math notranslate nohighlight">\(\mu\)</span>
。并且我们能够开发一种适用于所以GLM框架下模型的参数估计算法，
所以我们可以支持对诸如logit，probit和Poisson之类的有用统计模型的参数估计。</p>
</section>
<section id="id5">
<h3><span class="section-number">19.1.3. </span>例子<a class="headerlink" href="#id5" title="此标题的永久链接"></a></h3>
<p><strong>高斯分布</strong></p>
<p>高斯分布的概率密度函数为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-12">
<span class="eqno">(19.1.20)<a class="headerlink" href="#equation-probability-model-25-34-12" title="此公式的永久链接"></a></span>\[f(y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \{ -\frac{1}{2}\frac{(y-\mu)^2}{\sigma^2} \}\]</div>
<p>把其改写成指数分散族的形式：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-13">
<span class="eqno">(19.1.21)<a class="headerlink" href="#equation-probability-model-25-34-13" title="此公式的永久链接"></a></span>\[f(y) = \exp \{ \frac{y\mu-\frac{1}{2}\mu^2}{\sigma^2} - \frac{y^2}{2\sigma^2}
- \frac{1}{2} \ln (2\pi\sigma^2) \}\]</div>
<p>和 <span class="math notranslate nohighlight">\(eq_34_EDF\)</span> 进行对比，各个标准项为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-14">
<span class="eqno">(19.1.22)<a class="headerlink" href="#equation-probability-model-25-34-14" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\theta &amp;=\mu\\b(\theta) &amp;= \frac{1}{2}\mu^2\\a(\phi) &amp;= \sigma^2\end{aligned}\end{align} \]</div>
<p>高斯分布的均值和方差为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-15">
<span class="eqno">(19.1.23)<a class="headerlink" href="#equation-probability-model-25-34-15" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mathbb{E}[Y]=b'(\theta) = \mu\\Var(Y) = b''(\theta)a(\phi) = \sigma^2\end{aligned}\end{align} \]</div>
<p>对于高斯分布来说，方差和均值是独立无关的。</p>
<p><strong>伯努利分布</strong></p>
<table class="docutils align-default" id="id22">
<caption><span class="caption-number">表 19.1.5 </span><span class="caption-text">常见GLM表(1)</span><a class="headerlink" href="#id22" title="此表格的永久链接"></a></caption>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>Normal(Gaussian) <span class="math notranslate nohighlight">\(N(\mu,\sigma^2)\)</span></p></th>
<th class="head"><p>Bernoulli <span class="math notranslate nohighlight">\(B(\mu)\)</span></p></th>
<th class="head"><p>Binomial <span class="math notranslate nohighlight">\(B(N,\mu)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Range of y</p></td>
<td><p>real: <span class="math notranslate nohighlight">\((-\infty,+\infty)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\{0,1\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\{0,\dots,N\}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>f(y)</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{\sqrt{2\pi\sigma^2}}\exp \{ -\frac{(y-\mu)^2}{2\sigma^2} \}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu^y(1-\mu)^{1-y}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\binom{N}{y}\mu^y(1-\mu)^{N-y}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>EDF</p></td>
<td><p><span class="math notranslate nohighlight">\(\exp\{\frac{\mu y-\frac{\mu^2}{2}}{\sigma^2}-\frac{y^2}{2\sigma^2}-\frac{\ln 2\pi\sigma^2}{2}\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\exp\{y \ln \frac{\mu}{1-\mu} + \ln(1-\mu)\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\exp\bigg[ \frac{y \ln(\frac{\mu}{1-\mu}) + \ln(1-\mu)}{1/N} + \ln({N \choose y})\bigg]\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\theta=\psi(\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\theta=\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\theta=\ln \left ( \frac{\mu}{1-\mu} \right )=logit(\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\theta=\ln \left ( \frac{\mu}{1-\mu} \right )\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mu=\psi^{-1}(\theta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\theta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\frac{1}{1+e^{-\theta}}=sigmoid(\theta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\frac{1}{1+e^{-\theta}}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(b(\theta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\theta^2}{2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\ln(1+e^{\theta})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\ln(1+e^{\theta})\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(b(\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\mu^2}{2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\ln(1-\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\ln(1-\mu)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Link name</p></td>
<td><p>Identity</p></td>
<td><p>Logit</p></td>
<td><p>Logit</p></td>
</tr>
<tr class="row-even"><td><p>Link function</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\mu\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\ln \left( \frac{\mu}{1-\mu} \right)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\eta=\ln \left( \frac{\mu}{1-\mu} \right)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Mean function</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\eta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\frac{1}{1+e^{-\eta}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=\frac{N}{1+e^{-\eta}}\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\nu(\mu)=b''(\theta)\)</span></p></td>
<td><p>1</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu(1-\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu(1-\mu)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(a(\phi)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma^2\)</span></p></td>
<td><p>1</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{N}\)</span></p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default" id="id23">
<caption><span class="caption-number">表 19.1.6 </span><span class="caption-text">常见GLM表(2)</span><a class="headerlink" href="#id23" title="此表格的永久链接"></a></caption>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>Categorical <span class="math notranslate nohighlight">\(Cat(K,\mu)\)</span></p></th>
<th class="head"><p>Poisson <span class="math notranslate nohighlight">\(Poisson(\mu)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Range of y</p></td>
<td><p><span class="math notranslate nohighlight">\(\{1,\dots,K\}\)</span></p></td>
<td><p>integer <span class="math notranslate nohighlight">\(0,1,2,\dots\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>f(y)</p></td>
<td><p><span class="math notranslate nohighlight">\(\prod_{k}\mu_k^{y_k}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\exp\{y\ln \mu - \ln\mu\}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>EDF</p></td>
<td><p><span class="math notranslate nohighlight">\(\exp \left \{ \sum_{k=1}^{K-1} x_k \ln \left ( \frac{\mu_k}{ \mu_K} \right )+ \ln \left  (1-\sum_{k=1}^{K-1} \mu_k \right ) \right \}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\exp\{y\ln \mu - \ln\mu\}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\theta=\psi(\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\theta_k=\ln \left ( \frac{\mu_k}{\mu_K} \right )\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\theta=\ln \mu\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mu=\psi^{-1}(\theta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu_k = \frac{e^{\theta_k}}{\sum_{j=1}^K e^{\theta_j}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu=e^{\theta}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(b(\theta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\ln \left (  \sum_{k=1}^K e^{\theta_k}  \right )\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(e^{\theta}\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(b(\mu)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(- \ln \left  (1-\sum_{k=1}^{K-1} \mu_k \right )\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\ln\mu\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Link name</p></td>
<td><p>Logit</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Link function</p></td>
<td><p><span class="math notranslate nohighlight">\(\eta_k=\ln \left( \frac{\mu_k}{\mu_K} \right)\)</span></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Mean function</p></td>
<td><p><span class="math notranslate nohighlight">\(\mu_k=\frac{e^{\eta_k}}{\sum_k e^{\eta_k}}\)</span></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\nu(\mu)=b''(\theta)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mu_k(1-\mu_k)\)</span></p></td>
<td><p>mu</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(a(\phi)\)</span></p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="id6">
<h2><span class="section-number">19.2. </span>参数估计<a class="headerlink" href="#id6" title="此标题的永久链接"></a></h2>
<p>本节我们介绍两种GLM模型的参数估计算法，
我们将统一的以指数族的形式展现算法过程，这样适用于指数族中的所有具体分布，
我们的目标是让大家对GLM的基础理论有个全面的了解，同时我们会着重强调算法成立的假设及其一些限制。
在本章的后几节我们将说明该理论在特定指数族成员上的运用。</p>
<p>传统上，对于单参数的指数族分布可以运用梯度下降法和牛顿法进行参数估计，
梯度下降法的优点是算法实现简单，缺点是收敛速度不如牛顿法。
梯度下降法和牛顿法在形式上是非常相似的，二者都是沿着目标函数的负梯度方向寻找最优解，
不同的是传统梯度下降法利用一阶导数，而牛顿法利用二阶导数，牛顿法相对于梯度下降法收敛速度会更快，
但是由于二阶导数的引入也使得牛顿法的计算复杂度增加很对，甚至很多时候无法计算。
所以这里我们同时会介绍牛顿法的一个变种算法，迭代重加权最小平方法(iteratively reweighted least squares,IRLS)。
GLM框架下的模型都可以以统计的形式运用IRLS算法进行参数估计，这是GLM非常有吸引力的一点。</p>
<p>在之后的参数估计算法讨论中，我们都是假设在一个满足独立同分布(IID)的观测样本集上进行参数估计，
并且这个样本集是指数族分布的观测样本集。所有样本的联合概率通常可以被称为样本集的似然函数，
独立同分布的样本集的联合概率函数可以写成：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-21">
<span class="eqno">(19.2.15)<a class="headerlink" href="#equation-eq-34-21" title="此公式的永久链接"></a></span>\[f(y;\theta,\phi)=\prod_{n=1}^N f(y_n;\theta,\phi)\]</div>
<p>似然函数可以理解为：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-22">
<span class="eqno">(19.2.16)<a class="headerlink" href="#equation-eq-34-22" title="此公式的永久链接"></a></span>\[L(\theta,\phi;y)=\prod_{n=1}^N f(\theta,\phi;y_n)\]</div>
<p><a class="reference internal" href="#equation-eq-34-21">公式(19.2.15)</a> 和 <a class="reference internal" href="#equation-eq-34-22">公式(19.2.16)</a> 的区别在于，
前者是一个概率密度(质量)函数，其是在给定 <span class="math notranslate nohighlight">\(\theta,\phi\)</span> 的条件下关于未知量 <span class="math notranslate nohighlight">\(y\)</span> 的函数；
后者是一个似然函数，其表达是在给定数据 <span class="math notranslate nohighlight">\(y\)</span> 的条件下关于未知参数 <span class="math notranslate nohighlight">\(\theta,\phi\)</span> 的函数。</p>
<p>因为联合似然函数是一个函数的连乘形式，所以通常我们会对其进行一个对数转换(log-transform)进而得到一个连加的形式，
连加的形式更方便进行计算。加了对数的似然函数被称为对数似然函数，<span class="math notranslate nohighlight">\(\ell\)</span>
，这个函数也是最大似然估计(ML)的核心，我们从指数族的对数似然函数开始。</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>连乘变成连加有两个好处，(1)更容易求导和极大化操作；(2)似然函数是概率连乘，而概率都是小于1的，大量小于1的数字连乘产生更小的数字，
甚至趋近于0，而计算机的浮点数精度是通常无法处理这么小的数字的，所以加对数更方便计算机进行数值处理。</p>
</div>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-16">
<span class="eqno">(19.2.17)<a class="headerlink" href="#equation-probability-model-25-34-16" title="此公式的永久链接"></a></span>\[\ell=\sum_{n=1}^N \left \{   \frac{y_n\theta_n - b(\theta_n)}{a(\phi)}   + c(y_i,\phi)   \right \}\]</div>
<p><span class="math notranslate nohighlight">\(\theta\)</span> 是规范参数(canonical parameter )；
<span class="math notranslate nohighlight">\(b(\theta)\)</span> 是累积量，它描述的是分布的矩(moment)；
<span class="math notranslate nohighlight">\(\phi\)</span> 是分散参数(dispersion parameter)，它是比例或辅助参数；
<span class="math notranslate nohighlight">\(c(\cdot)\)</span> 是归一化项。
归一化项不是 <span class="math notranslate nohighlight">\(\theta\)</span> 的函数，而是简单地缩放基础密度函数的范围使得整个函数的积分（或求和）为1。
在上一节我们讨论过，指数族分布的期望与方差可以通过 <span class="math notranslate nohighlight">\(b(\theta)\)</span> 的导数求得。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-17">
<span class="eqno">(19.2.18)<a class="headerlink" href="#equation-probability-model-25-34-17" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mu &amp;= \mathbb{E}[Y] = b'(\theta)\\Var(Y) &amp;= b''(\theta) a(\phi)\end{aligned}\end{align} \]</div>
<p>并且我们知道，均值参数 <span class="math notranslate nohighlight">\(\mu\)</span> 和规范参数 <span class="math notranslate nohighlight">\(\theta\)</span> 是存在一个可逆的函数关系的，也就是说
<span class="math notranslate nohighlight">\(\mu\)</span> 可以看做是关于 <span class="math notranslate nohighlight">\(\theta\)</span> 的一个函数，反之，<span class="math notranslate nohighlight">\(\theta\)</span> 也可看做是一个关于
<span class="math notranslate nohighlight">\(\mu\)</span> 的函数。
基于这个事实，我们可以把 <span class="math notranslate nohighlight">\(b''(\theta)\)</span> 看做是一个关于 <span class="math notranslate nohighlight">\(\mu\)</span> 的函数，记作
<span class="math notranslate nohighlight">\(\nu(\mu)\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-18">
<span class="eqno">(19.2.19)<a class="headerlink" href="#equation-probability-model-25-34-18" title="此公式的永久链接"></a></span>\[b''(\theta) \triangleq \nu(\mu)\]</div>
<p>因此，方差 <span class="math notranslate nohighlight">\(Var(Y)\)</span> 就可以被看成是函数 <span class="math notranslate nohighlight">\(\nu(\mu)\)</span>
和分散函数 <span class="math notranslate nohighlight">\(a(\phi)\)</span> 的乘积，通常我们把 <span class="math notranslate nohighlight">\(\nu(\mu)=b''(\theta)\)</span> 称为方差函数(variance function)，
<strong>注意：虽然叫方差函数，但方差函数的值不是方差本身。</strong>
有时 <span class="math notranslate nohighlight">\(b''(\theta)\)</span> 会是一个常数量(constant)，比如高斯分布，此时分布的方差为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-19">
<span class="eqno">(19.2.20)<a class="headerlink" href="#equation-probability-model-25-34-19" title="此公式的永久链接"></a></span>\[Var(Y)=constant \times a(\phi)\]</div>
<p>这时，分布的方差就不会受到均值的影响了。
另外方差函数 <span class="math notranslate nohighlight">\(\nu(\mu)\)</span> 可以通过简单方式求得。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-20">
<span class="eqno">(19.2.21)<a class="headerlink" href="#equation-probability-model-25-34-20" title="此公式的永久链接"></a></span>\[\nu(\mu) = b''(\theta) =(b'(\theta))'= (\mu(\theta))' = \frac{\partial \mu}{\partial \theta}\]</div>
<p>显然，当 <span class="math notranslate nohighlight">\(\mu\)</span> 与 <span class="math notranslate nohighlight">\(\theta\)</span> 之间的映射函数是线性函数时，一阶偏导 <span class="math notranslate nohighlight">\(\frac{\partial \mu}{\partial \theta}\)</span>
就是一个常数值。另外，我们知道反函数的导数就等于原函数导数的倒数，所以有：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-21">
<span class="eqno">(19.2.22)<a class="headerlink" href="#equation-probability-model-25-34-21" title="此公式的永久链接"></a></span>\[\frac{\partial \theta}{\partial \mu} = \frac{1}{\nu(\mu)}\]</div>
<p>在GLM框架下，输入变量 <span class="math notranslate nohighlight">\(X\)</span> 和其系数 <span class="math notranslate nohighlight">\(\beta\)</span>
组成一个线性预测器 <span class="math notranslate nohighlight">\(\eta=\beta^Tx\)</span> 。
<span class="math notranslate nohighlight">\(\eta\)</span> 和分布的均值(期望)通过链接函数(已知的)连接在一起 。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-22">
<span class="eqno">(19.2.23)<a class="headerlink" href="#equation-probability-model-25-34-22" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\eta &amp;=\beta^Tx = g(\mu)\\\mu &amp;= g^{-1}(\eta)\end{aligned}\end{align} \]</div>
<p>其中 <span class="math notranslate nohighlight">\(\beta\)</span> 和 <span class="math notranslate nohighlight">\(x\)</span> 都是一个向量，<span class="math notranslate nohighlight">\(\beta^Tx = \sum_j \beta_j x_j\)</span> ，
因此有：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-23">
<span class="eqno">(19.2.24)<a class="headerlink" href="#equation-probability-model-25-34-23" title="此公式的永久链接"></a></span>\[\frac{\partial \eta}{\partial \beta_j} = x_j\]</div>
<p>线性预测器 <span class="math notranslate nohighlight">\(\eta\)</span> 的值空间并没有特别的限定，
其值空间可以是整个实数域 <span class="math notranslate nohighlight">\(\eta \in R\)</span> 。
<strong>因此，链接函数的一个目的就是将线性预测器的值映射到输出变量的范围。</strong>
映射到输出变量的范围可以确保和指数族特定成员分布相关。
一个特殊的链接函数是恒等函数， <span class="math notranslate nohighlight">\(\eta=\theta\)</span> ，
恒等函数的链接函数被称为规范链接(canonical link)或者自然链接(natural link)。</p>
<p>现在让我们回到似然估计，似然估计的一个关键过程就是要对数似然函数的求导。
经过以上的铺垫，我们利用求导的链式法则对GLM模型的对数似然函数进行求导。
注意，我们的参数 <span class="math notranslate nohighlight">\(\beta\)</span> 是一个向量参数，我们只需要对其中的一项进行求导即可。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-jac">
<span class="eqno">(19.2.25)<a class="headerlink" href="#equation-eq-34-jac" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\frac{ \partial \ell}{\beta_j} &amp;= \sum_{n=1}^N \left ( \frac{\partial \ell_n}{\partial \theta_n} \right )
\left ( \frac{\partial \theta_n}{\partial \mu_n} \right )
\left ( \frac{\partial \mu_n}{\partial \eta_n} \right )
\left ( \frac{\partial \eta_n}{\partial \beta_j} \right )\\&amp;= \sum_{n=1}^N \left \{ \frac{y_n-b'(\beta_n)}{a(\phi)}   \right \}
\left \{ \frac{1}{\nu(\mu_n)} \right \} \left ( \frac{\partial \mu}{\partial \eta} \right )_n x_{jn}\\&amp;= \sum_{n=1}^N \frac{y_n-\mu_n}{a(\phi) \nu(\mu_n) } \left ( \frac{\partial \mu}{\partial \eta} \right )_n x_{jn}\end{aligned}\end{align} \]</div>
<p>其中 <span class="math notranslate nohighlight">\(n\)</span> 是观测样本的编号，<span class="math notranslate nohighlight">\(x_{jn}\)</span> 表示第 <span class="math notranslate nohighlight">\(n\)</span> 条观测样本的第 <span class="math notranslate nohighlight">\(j\)</span> 列特征值，其值已知。
<span class="math notranslate nohighlight">\(y_n\)</span> 是当前观测样本的标签值，也是已知的。
<span class="math notranslate nohighlight">\(\mu_n=g^{-1}(\eta_n)=g^{-1}(\beta^Tx_n)\)</span> 是链接函数的反函数的值，代入样本值 <span class="math notranslate nohighlight">\(x_n\)</span>
和当前的参数值 <span class="math notranslate nohighlight">\(\beta\)</span> 后可以直接算出，方差函数 <span class="math notranslate nohighlight">\(\nu(\mu_n)\)</span> 是关于 <span class="math notranslate nohighlight">\(\mu_n\)</span> 的函数，因此也可以算出。
<span class="math notranslate nohighlight">\(a(\phi)\)</span> 是已知的， <span class="math notranslate nohighlight">\(\frac{\partial \mu}{\partial \eta} ` 是函数 :math:`g^{-1}(\eta_n)\)</span> 关于 <span class="math notranslate nohighlight">\(\eta_n\)</span> 的导数，
在确定了链接函数的形式后也是可以算出的。
<a class="reference internal" href="#equation-eq-34-jac">公式(19.2.25)</a> 是GLM标准指数族形式下对数似然函数的一阶偏导数，GLM框架下的任意模型都可以按照此公式计算偏导数，
只需要按照特定的分布和链接函数替换相应组件即可。</p>
<p>回顾一下线性回归的章节，要想求得参数的估计值，我们可以令对数似然函数的一阶导数为0，得到正规方程(normal equations)，
然后解正规方程的方式得到解析解。
然而，正规方程并不是一定能求解的，需要满足一些限制条件才行，
比如链接函数必须是规范链接函数(canonical link)以及满足矩阵可逆(如果不是很理解可以回顾一下线性回归的章节- <a class="reference internal" href="21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#ch-29"><span class="std std-numref">节 17</span></a> )。
所以解析解的方式并不具备通用性，我们需要采用更一般的方法，迭代法。</p>
<p>迭代法又可以简单分为一阶导(梯度下降法系列)和二阶导(牛顿法系列)，实际这两种都可以通过泰勒展开(Taylor explanation)公式进行证明。
这里我们简单回顾一下泰勒展开定理。
注意，本书讨论的迭代求解算法默认目标函数都是凸函数，也就是函数有唯一的极值点。
关于非凸函数以及带约束的优化问题，请读者参考其它资料。</p>
<aside class="topic">
<p class="topic-title">泰勒展开</p>
<p>设 <span class="math notranslate nohighlight">\(n\)</span> 是一个正整数。如果定义在一个包含 <span class="math notranslate nohighlight">\(x_0\)</span> 的区间上的函数f在 <span class="math notranslate nohighlight">\(x_0\)</span>
处 <span class="math notranslate nohighlight">\(n+1\)</span> 次可导，那么对于这个区间上的任意x，都有</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-24">
<span class="eqno">(19.2.26)<a class="headerlink" href="#equation-probability-model-25-34-24" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}f(x)_{Taylor}  &amp;= \sum_{n=0}^{\infty} \frac{f^{(n)}(x_0)}{n!} \times (x - x_0)^n\\ &amp;= f(x_0) + \frac{f'(x_0)}{1!}(x-x_0) + \frac{f^{(2)}(x_0)}{2!}(x-x_0)^2+ \cdots + \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n + R_n(x)\end{aligned}\end{align} \]</div>
</aside>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>泰勒公式(Taylor formula)、泰勒级数(Taylor series)、泰勒展开(Taylor explanation)、泰勒定理(Taylor theory)是一回事。</p>
</div>
<section id="id7">
<h3><span class="section-number">19.2.1. </span>梯度下降法<a class="headerlink" href="#id7" title="此标题的永久链接"></a></h3>
<p>我们把对数似然函数按照泰勒公式进行展开，但是我们只展开到一阶导数，把更高阶导数的和看做一个常数量constant，
则对数似然函数通过泰勒展开可以简化成：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-25">
<span class="eqno">(19.2.27)<a class="headerlink" href="#equation-probability-model-25-34-25" title="此公式的永久链接"></a></span>\[f(x)_{Taylor} = f(x_0) + f'(x_0)(x-x_0) +constant\]</div>
<p>现在我们把对数似然函数按照泰勒展开公式进行操作：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-30">
<span class="eqno">(19.2.28)<a class="headerlink" href="#equation-eq-34-30" title="此公式的永久链接"></a></span>\[\ell(\beta^{(t+1)}) = \ell(\beta^t) + \ell'(\beta^t)(\beta^{(t+1)} - \beta^t) + constant\]</div>
<p>其中，我们把 <span class="math notranslate nohighlight">\(\beta^t\)</span> 看做是参数的初始值(或者说上一轮迭代后的值)，其值是已知的。
把 <span class="math notranslate nohighlight">\(\beta^{(t+1)}\)</span> 看做是对数似然函数的极值点(或者说当前轮迭代后的值)，其值是未知的，是函数的自变量。</p>
<p>最大化对数似然函数是希望对数似然函数取得极大值点，那么就要求每一次迭代 <span class="math notranslate nohighlight">\(\ell(\beta)\)</span> 的值要变得更大，直到达到极大值点。
那么就要满足：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-26">
<span class="eqno">(19.2.29)<a class="headerlink" href="#equation-probability-model-25-34-26" title="此公式的永久链接"></a></span>\[\ell(\beta^{(t+1)}) -  \ell(\beta^t) =\ell'(\beta^t)(\beta^{(t+1)} - \beta^t) +constant \ge 0\]</div>
<p>当等号成立的时候 <span class="math notranslate nohighlight">\(\beta^{(t+1)}\)</span> 就是我们要找的极大值点，对上述公式进行移项处理，可得：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-31">
<span class="eqno">(19.2.30)<a class="headerlink" href="#equation-eq-34-31" title="此公式的永久链接"></a></span>\[\beta^{(t+1)} = \beta^t - \frac{constant}{\ell'(\beta^t)}\]</div>
<p>通常我们把一阶导 <span class="math notranslate nohighlight">\(\ell'(\beta^t)\)</span> 称为梯度(gradient)，
<a class="reference internal" href="#equation-eq-34-31">公式(19.2.30)</a> 说明只要 <span class="math notranslate nohighlight">\(\beta^{(t+1)}\)</span> 沿着 <span class="math notranslate nohighlight">\(\beta^t\)</span> 的负梯度方向进行移动，我们终将能达到极值点。
注意 <span class="math notranslate nohighlight">\(\frac{constant}{\ell'(\beta^t)}\)</span> <strong>的绝对值的大小影响着前进的速度，</strong>
<strong>其方向(正负号)决定目标函数是否向着极大值点移动。</strong>
所以和下面的公式是等价的，<span class="math notranslate nohighlight">\(\alpha\)</span> 称为学习率(learning rate)，是一个人工设置参数，控制的迭代的速度。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-32">
<span class="eqno">(19.2.31)<a class="headerlink" href="#equation-eq-34-32" title="此公式的永久链接"></a></span>\[\beta^{(t+1)} = \beta^t - \alpha \ell'(\beta^t)\]</div>
<p>利用 <a class="reference internal" href="#equation-eq-34-32">公式(19.2.31)</a> 进行参数迭代求解的方法就称为梯度下降法，梯度下降法的核心就是让参数(自变量)沿着负梯度的方向前进。
虽然理论上最终一定能到达极值点，但是实际上会受到学习率参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的影响，
学习率可以理解成每次迭代前进的步长(step size)，步长越大前进的越快，收敛性速度就越快；反之，步长越小，收敛越慢。
但是步长如果大了，就会造成震荡现象，即一步迭代就越过了终点(极值点)，并且在极值点附近往返震荡，永远无法收敛。
为了保证算法能一定收敛，通常会为 <span class="math notranslate nohighlight">\(\alpha\)</span> 设定一个较小的值。
关于 <span class="math notranslate nohighlight">\(\alpha\)</span> 的更多讨论请参考其它资料。</p>
<figure class="align-center" id="id24">
<span id="fg-34-3"></span><a class="reference internal image-reference" href="../_images/34_31.png"><img alt="../_images/34_31.png" src="../_images/34_31.png" style="width: 864.5px; height: 335.29999999999995px;" /></a>
<figcaption>
<p><span class="caption-number">图 19.2.1 </span><span class="caption-text">梯度下降法中学习率的影响(图片来自网络)</span><a class="headerlink" href="#id24" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
</section>
<section id="id8">
<h3><span class="section-number">19.2.2. </span>牛顿法<a class="headerlink" href="#id8" title="此标题的永久链接"></a></h3>
<p>梯度下降法虽然也能收敛到最优解，但是如果学习率设置(通常人工设置)不合理，可能会造成收敛速度太慢或者无法收敛的问题。
现在我们讨论另一中迭代算法，牛顿–拉夫森方法(Newton–Raphson)，一般简称牛顿法。
还是从泰勒展开公式开始，让我们考虑二阶泰勒展开：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-33">
<span class="eqno">(19.2.32)<a class="headerlink" href="#equation-eq-34-33" title="此公式的永久链接"></a></span>\[\ell(\beta^{(t+1)}) = \ell(\beta^t) + \ell'(\beta^t)(\beta^{(t+1)} - \beta^t) +
\frac{1}{2}\ell''(\beta^t)(\beta^{(t+1)} - \beta^t)^2 + constant\]</div>
<p>我们知道目标函数在极值点处的导数应该为0，
所以如果 <span class="math notranslate nohighlight">\(\beta^{(t+1)}\)</span> 是极值点，那么有 <span class="math notranslate nohighlight">\(\ell'(\beta^{(t+1)})=0\)</span>
。我们对 <a class="reference internal" href="#equation-eq-34-33">公式(19.2.32)</a> 进行求导，注意 <span class="math notranslate nohighlight">\(theta_t\)</span> 和 <span class="math notranslate nohighlight">\(\ell(\beta^t)\)</span> 都是已知常数量。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-27">
<span class="eqno">(19.2.33)<a class="headerlink" href="#equation-probability-model-25-34-27" title="此公式的永久链接"></a></span>\[\ell'(\beta^{(t+1)})= \ell'(\beta^t) + \ell''(\beta^t)(\beta^{(t+1)}-\beta^t)=0\]</div>
<p>通过移项可得：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-34">
<span class="eqno">(19.2.34)<a class="headerlink" href="#equation-eq-34-34" title="此公式的永久链接"></a></span>\[\beta^{(t+1)} = \beta^t - \frac{\ell'(\beta^t)}{\ell''(\beta^t)}\]</div>
<p><span class="math notranslate nohighlight">\(\ell''(\beta^t)\)</span> 是目标函数的二阶偏导数，由于参数 <span class="math notranslate nohighlight">\(\beta\)</span> 是一个向量，所以二阶导是一个矩阵，
通常被称为海森矩阵(Hessian matrix)，常用符号 <span class="math notranslate nohighlight">\(H\)</span> 表示。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-35">
<span class="eqno">(19.2.35)<a class="headerlink" href="#equation-eq-34-35" title="此公式的永久链接"></a></span>\[ \beta^{(t+1)} = \beta^t - H(\beta^{(t)})^{-1} \ell'(\beta^t)\]</div>
<p>和梯度下降法的 <a class="reference internal" href="#equation-eq-34-32">公式(19.2.31)</a> 对比下发现，两者非常相似，不同的是牛顿法用Hessian矩阵的逆矩阵 <span class="math notranslate nohighlight">\(H(\beta^{(t)})^{-1}\)</span>
替代了学习率参数，避免了需要人工设置学习率的问题。相比梯度下降法，牛顿法收敛速度更快，并且也没有震荡无法收敛的问题。
<span class="math notranslate nohighlight">\(H(\beta^{(t)})\)</span> 是对数似然函数的二阶导数，可以在一阶导 <a class="reference internal" href="#equation-eq-34-jac">公式(19.2.25)</a> 的基础上再进行一次偏导得到。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-36">
<span class="eqno">(19.2.36)<a class="headerlink" href="#equation-eq-34-36" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\left (\frac{\partial^2 \ell }{\partial \beta_j \partial \beta_k} \right ) &amp;=
\sum_{n=1}^N \frac{1}{a(\phi)} \left (  \frac{\partial}{\partial \beta_k}   \right )
\left \{ \frac{y_n-\mu_n}{\nu(\mu_n)} \left ( \frac{\partial \mu}{\partial \eta} \right)_n x_{jn} \right \}\\
&amp;= \sum_{n=1}^N \frac{1}{a(\phi)} \left [
    \left ( \frac{\partial \mu }{\partial \eta} \right )_n
    \left \{
        \left (  \frac{\partial  }{\partial \mu} \right )_n
        \left ( \frac{\partial \mu }{\partial \eta} \right )_n
        \left (  \frac{\partial \eta }{\partial \beta_k} \right )_n
    \right \} \frac{y_n-\mu_n}{\nu(\mu_n)}
    + \frac{y_n-\mu_n}{\nu(\mu_n)}
        \left \{
                \left ( \frac{\partial  }{\partial \eta} \right )_n
                \left ( \frac{\partial \eta }{\partial \beta_k} \right )_n
        \right \}
    \left ( \frac{\partial \mu }{\partial \eta} \right )_n
\right ] x_{jn}\\
&amp;= -\sum_{n=1}^N \frac{1}{a(\phi)}
\left [
    \frac{1}{\nu(\mu_n)}  \left ( \frac{\partial \mu}{\partial \eta} \right )_n^2
    -(\mu_n-y_n)
        \left \{
            \frac{1}{\nu(\mu_n)^2}  \left ( \frac{\partial \mu }{\partial \eta} \right )_n^2 \frac{\partial \nu(\mu_n)}{\partial \mu}
            - \frac{1}{\nu(\mu_n)}  \left ( \frac{\partial^2 \mu}{\partial \eta^2} \right )_n
        \right \}
\right ] x_{jn}x_{kn}\end{aligned}\end{align} \]</div>
<p>有了一阶导数和二阶导数(Hessian matrix)，就可以按照 <a class="reference internal" href="#equation-eq-34-35">公式(19.2.35)</a>
迭代的更新参数值，直到算法收敛，最终得到参数的最大似然估计值。
完成优化后，必须为参数 <span class="math notranslate nohighlight">\(\beta\)</span> 估计值估计一个合适的方差矩阵。
一个明显而合适的选择是基于估计的观测到的海森矩阵。
这是软件实现中最常见的默认选择，因为观察到的Hessian矩阵是估计算法的组成部分。
这样，它已经被计算并可用。
我们所介绍的 Newton–Raphson 算法并没有包括对 scale parameter，
<span class="math notranslate nohighlight">\(\phi\)</span> ，的估计功能，有兴趣的读者可以参考其它资料。</p>
<p>最终，Newton–Raphson 提供了如下功能：</p>
<ol class="arabic simple">
<li><p>为所有单参数指数族的GLM成员模型提供一个参数估计算法。</p></li>
<li><p>参数估计值的标准误(standard errors)的计算：负的 Hessian matrix 逆矩阵的对角线元素的平方根。</p></li>
</ol>
<p>我们这里描述的 Newton–Raphson 算法不支持分散参数(dispersion parameter)， <span class="math notranslate nohighlight">\(\phi\)</span>
，的估计，一些 Newton–Raphson 的扩展算法可以提供分散参数的ML估计。</p>
<p>然而，Newton–Raphson 方法存在两大缺点：</p>
<ol class="arabic simple">
<li><p>Hessian matrix 的计算复杂度非常高，<a class="reference internal" href="#equation-eq-34-36">公式(19.2.36)</a> 。</p></li>
<li><p>Hessian matrix 并不是一定存在逆矩阵的，当不存在逆矩阵时，算法无法执行。</p></li>
</ol>
<p>由于这两个缺陷的存在，在实际应用中是很少直接使用 Newton–Raphson 法的，
而是使用牛顿法的改进算法。有很多牛顿法的改进算法被研究者提了出来，
多数改进算法的思路是找到一个海森矩阵的近似矩阵去替换原有的海森矩阵，进而避免上述两个缺点。</p>
<p><strong>迭代初始值的设定</strong></p>
<p>要实现 Newton–Raphson 迭代法，
我们必须对参数初始值有一个猜测。
但目前没有用于获得良好参数初值的全局机制，
但是有一个合理的解决方案可以在模型中存在”常数项系数”时获得起始值。</p>
<p>这里的”常数项”指的是线性预测器中截距部分</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-28">
<span class="eqno">(19.2.37)<a class="headerlink" href="#equation-probability-model-25-34-28" title="此公式的永久链接"></a></span>\[\eta = \beta_0 \times 1 + \beta_1 x_1 +\dots + \beta_px_p\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\beta_0\)</span> 就是常数项系数。
如果模型包含常数项，则通常的做法是找到仅包含常数项系数的模型的估计值。
我们令：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-29">
<span class="eqno">(19.2.38)<a class="headerlink" href="#equation-probability-model-25-34-29" title="此公式的永久链接"></a></span>\[\eta = \beta_0\]</div>
<p>然后令对数似然函数的一阶导数 <a class="reference internal" href="#equation-eq-34-jac">公式(19.2.25)</a> 为0，找到 <span class="math notranslate nohighlight">\(\beta_0\)</span>
的解析解。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-37">
<span class="eqno">(19.2.39)<a class="headerlink" href="#equation-eq-34-37" title="此公式的永久链接"></a></span>\[\sum_{n=1}^N \frac{y_n-\mu_n}{a(\phi) \nu(\mu_n) } \left ( \frac{\partial \mu}{\partial \eta} \right )_n
=0\]</div>
<p>通过上式是可以得到 <span class="math notranslate nohighlight">\(\beta_0\)</span> 的一个估计值的。
比如如果是逻辑回归模型，则有：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-30">
<span class="eqno">(19.2.40)<a class="headerlink" href="#equation-probability-model-25-34-30" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}a(\phi) &amp;= 1\\\nu(\mu) &amp;= \mu(1-\mu)\\\mu &amp;= \text{sigmoid}(\eta_n) = \text{sigmoid}(\beta_0)\\\frac{\partial \mu}{\partial \eta} &amp;= \frac{\partial }{\partial \eta} \text{sigmoid} (\eta) = \mu(1-\mu)\end{aligned}\end{align} \]</div>
<p>代入到 <a class="reference internal" href="#equation-eq-34-37">公式(19.2.39)</a> 可得：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-31">
<span class="eqno">(19.2.41)<a class="headerlink" href="#equation-probability-model-25-34-31" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\sum_{n=1}^N \frac{(y_n- \mu_n ) }{\mu_n(1-\mu_n)} \mu_n(1-\mu_n) &amp;= 0\\ &amp;\Downarrow\\\sum_{n=1}^N (y_n- \mu_n) &amp;=0\\ &amp;\Downarrow\\\sum_{n=1}^N (y_n- \frac{1}{1+e^{-\beta_0}}) &amp;=0\\ &amp;\Downarrow\\ \underbrace{\frac{1}{N}\sum_{n=1}^N y_n}_{\text{均值}\bar{y}} &amp;=  \frac{1}{1+e^{-\beta_0}}\\
&amp;\Downarrow{\text{sigmoid反函数求解}}\\\hat{\beta}_0 &amp;= \ln \left (  \frac{\bar{y}}{1-\bar{y}}   \right )\end{aligned}\end{align} \]</div>
<p>然后我们就用 <span class="math notranslate nohighlight">\(\beta=(\hat{\beta}_0,0,0,\dots,0)^T\)</span> 作为 Newton–Raphson 算法首次迭代时参数向量的初始值。
使用这种方法为我们提供了两个优点。
首先，我们从参数空间的合理子集开始迭代。
其次，对于ML，因为我们知道仅常数项系数模型的解决方案，
所以可以将训练模型的对数似然与算法初始步骤中获得的仅常数项系数模型的对数似然进行比较。
此比较是每个协变量（常数项系数除外）均为零的似然比检验，在下一节会介绍什么是似然比检验。
如果模型中没有常量项系数，或者如果我们无法解析法求解纯常数项系数模型，则必须使用更复杂的方法，
比如使用搜索方法寻找合理的初始点来开始我们的 Newton-Raphson 算法。</p>
</section>
<section id="irls">
<h3><span class="section-number">19.2.3. </span>迭代重加权最小二乘(IRLS)<a class="headerlink" href="#irls" title="此标题的永久链接"></a></h3>
<p>我们已经知道在牛顿法中，参数是按照如下公式进行迭代更新的：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-32">
<span class="eqno">(19.2.42)<a class="headerlink" href="#equation-probability-model-25-34-32" title="此公式的永久链接"></a></span>\[\beta^{(t+1)} = \beta^t - H(\beta^{(t)})^{-1} \ell'(\beta^t)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(H(\beta^{(t)})\)</span> 是对数似然函数的二阶偏导数，
GLM的对数似然函数的二阶偏导数如 <a class="reference internal" href="#equation-eq-34-36">公式(19.2.36)</a> 所示，
显然其计算成本是高昂的，即使计算出来还面临着求逆矩阵的难题。
那么是不是可以找到 <span class="math notranslate nohighlight">\(H(\beta^{(t)})^{-1}\)</span> 的一个近似替代值呢。
我们在 <a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-fisher-information"><span class="std std-numref">节 2.6</span></a> 已经讲过 Fisher information
的定义，我们知道信息矩阵(information matrix)就等于海森矩阵(Hessian matrix)的期望的负数。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-33">
<span class="eqno">(19.2.43)<a class="headerlink" href="#equation-probability-model-25-34-33" title="此公式的永久链接"></a></span>\[I(\beta) = - \mathop{\mathbb{E}}_{p(\mathcal{D} ; \beta)}[H(\beta^{(t)})]\]</div>
<p>我们可以用 <span class="math notranslate nohighlight">\(I(\beta)\)</span> 代替 <span class="math notranslate nohighlight">\(-H(\beta^{(t)})\)</span>
，回顾一下 <a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-fisher-information"><span class="std std-numref">节 2.6</span></a> 的内容，
信息矩阵 <span class="math notranslate nohighlight">\(I(\beta)\)</span> 就是 Score function (对数似然函数的一阶导数) 的方差，
当然我们也可以直接对 <a class="reference internal" href="#equation-eq-34-36">公式(19.2.36)</a> 的求期望得到，结果是一样的。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-44">
<span class="eqno">(19.2.44)<a class="headerlink" href="#equation-eq-34-44" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}I(\beta) = - \mathbb{E}[H(\beta^{(t)})] &amp;= \mathbb{E}[S(\beta)S(\beta)^T]\\&amp;= \mathbb{E}[ \frac{ \partial \ell}{\partial \beta_j} \cdot \frac{ \partial \ell}{\partial \beta_k}   ]\\&amp;= \mathbb{E} \left [
\sum_{n=1}^N \frac{y_n-\mu_n}{a(\phi) \nu(\mu_n) } \left ( \frac{\partial \mu}{\partial \eta} \right )_n x_{jn}
\cdot
\sum_{n=1}^N \frac{y_n-\mu_n}{a(\phi) \nu(\mu_n) } \left ( \frac{\partial \mu}{\partial \eta} \right )_n x_{kn}
\right ]\\&amp;= \sum_{n=1}^N \left ( \frac{\partial \mu}{\partial \eta} \right )^2_n  \frac{ 1}{ a(\phi) \nu(\mu_n) }   x_{jn} x_{kn}\end{aligned}\end{align} \]</div>
<p>现在我们把 <a class="reference internal" href="#equation-eq-34-35">公式(19.2.35)</a> 稍微变换一下，并且我们用 <span class="math notranslate nohighlight">\(I(\beta^{(t)})\)</span> 替换
<span class="math notranslate nohighlight">\(- H(\beta^{(t)})\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-45">
<span class="eqno">(19.2.45)<a class="headerlink" href="#equation-eq-34-45" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}   \Delta \beta^{(t)} = \beta^{(t+1)} - \beta^{(t)} &amp;=  - H(\beta^{(t)})^{-1} \ell'(\beta^{(t)})\\ &amp; \Downarrow{\text{移项}}\\  - H(\beta^{(t)})  \Delta \beta^{(t)} &amp;= \ell'(\beta^{(t)})\\ &amp; \Downarrow{\text{Hessian的期望}}\\- \mathbb{E}[H(\beta^{(t)})] \Delta \beta^{(t)} &amp;= \ell'(\beta^{(t)})\\&amp; \Downarrow{\text{信息矩阵}}\\I(\beta^{(t)})\Delta \beta^{(t)} &amp;= \ell'(\beta^{(t)})\end{aligned}\end{align} \]</div>
<p>现在我们把 <a class="reference internal" href="#equation-eq-34-jac">公式(19.2.25)</a> 和 <a class="reference internal" href="#equation-eq-34-44">公式(19.2.44)</a> 代入上式，得到一个等式：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-46">
<span class="eqno">(19.2.46)<a class="headerlink" href="#equation-eq-34-46" title="此公式的永久链接"></a></span>\[\left \{
\sum_{n=1}^N \left ( \frac{\partial \mu}{\partial \eta} \right )^2_n  \frac{ 1}{ a(\phi) \nu(\mu_n) }   x_{jn} x_{kn}
\right \}
\Delta \beta^{(t)} =
 \sum_{n=1}^N \frac{y_n-\mu_n}{a(\phi) \nu(\mu_n) } \left ( \frac{\partial \mu}{\partial \eta} \right )_n x_{n}^T
\ \ \ \ \text{(等式A)}\]</div>
<p><a class="reference internal" href="#equation-eq-34-46">公式(19.2.46)</a> 先保留，我们记为等式A，我还需要借助另外一个等式。
我们知道，对于每条样本线性预测器的方程为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-34">
<span class="eqno">(19.2.47)<a class="headerlink" href="#equation-probability-model-25-34-34" title="此公式的永久链接"></a></span>\[\eta_n^{(t)} =  x_n^T \beta^{(t)}\]</div>
<p>我们把 <a class="reference internal" href="#equation-eq-34-46">公式(19.2.46)</a> 中的 <span class="math notranslate nohighlight">\(\Delta \beta^{(t)}\)</span>
换成 <span class="math notranslate nohighlight">\(\beta^{(t)}\)</span>
，并结合线性预测器，可以得到如下等式。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-47">
<span class="eqno">(19.2.48)<a class="headerlink" href="#equation-eq-34-47" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}&amp; \left \{
\sum_{n=1}^N \left ( \frac{\partial \mu}{\partial \eta} \right )^2_n  \frac{ 1}{ a(\phi) \nu(\mu_n) }   x_{jn} x_{kn}
\right \} \beta^{(t)}\\&amp;= \left \{
\sum_{n=1}^N \left ( \frac{\partial \mu}{\partial \eta} \right )^2_n  \frac{ 1}{ a(\phi) \nu(\mu_n) }  x_n  x_n^T
\right \} \beta^{(t)}\\&amp;=
\sum_{n=1}^N \left ( \frac{\partial \mu}{\partial \eta} \right )^2_n  \frac{ 1}{ a(\phi) \nu(\mu_n) } \beta^{(t)T}  x_n  x_n^T\\
&amp;=
\sum_{n=1}^N \left ( \frac{\partial \mu}{\partial \eta} \right )^2_n  \frac{ 1}{ a(\phi) \nu(\mu_n) } \eta_n^{(t)}  x_n^T\end{aligned}\end{align} \]</div>
<p>去掉 <a class="reference internal" href="#equation-eq-34-47">公式(19.2.48)</a> 的中间推导过程，直接得到如下等式，我们记为等式B。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-48">
<span class="eqno">(19.2.49)<a class="headerlink" href="#equation-eq-34-48" title="此公式的永久链接"></a></span>\[\left \{
\sum_{n=1}^N \left ( \frac{\partial \mu}{\partial \eta} \right )^2_n  \frac{ 1}{ a(\phi) \nu(\mu_n) }   x_{jn} x_{kn}
\right \} \beta^{(t)}
=
\sum_{n=1}^N \left ( \frac{\partial \mu}{\partial \eta} \right )^2_n  \frac{ 1}{ a(\phi) \nu(\mu_n) } \eta_n^{(t)}  x_n^T
\ \ \ \ \text{(等式B)}\]</div>
<p>现在我们把等式A( <a class="reference internal" href="#equation-eq-34-46">公式(19.2.46)</a> )和等式B( <a class="reference internal" href="#equation-eq-34-48">公式(19.2.49)</a> )的等号两边相加：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-49">
<span class="eqno">(19.2.50)<a class="headerlink" href="#equation-eq-34-49" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\left \{
\sum_{n=1}^N \left ( \frac{\partial \mu}{\partial \eta} \right )^2_n  \frac{ 1}{ a(\phi) \nu(\mu_n) }   x_{jn} x_{kn}
\right \} (\beta^{(t)} + \Delta \beta^{(t)} ) &amp;=
\sum_{n=1}^N \left ( \frac{\partial \mu}{\partial \eta} \right )^2_n \frac{ 1}{ a(\phi) \nu(\mu_n) }
\left \{
(y_n-\mu_n)\left ( \frac{\partial \eta}{\partial \mu}_n \right) + \eta_n^{(t)}
\right \} x_n^T\\
 &amp; \Downarrow\\\underbrace{ \left \{
\sum_{n=1}^N \left ( \frac{\partial \mu}{\partial \eta} \right )^2_n  \frac{ 1}{ a(\phi) \nu(\mu_n) }   x_{jn} x_{kn}
\right \} }_{\text{矩阵}}
\beta^{(t+1)}
&amp;=
\sum_{n=1}^N \left ( \frac{\partial \mu}{\partial \eta} \right )^2_n \frac{ 1}{ a(\phi) \nu(\mu_n) }
\left \{
(y_n-\mu_n)\left ( \frac{\partial \eta}{\partial \mu} \right)_n+ \eta_n^{(t)}
\right \} x_n^T\end{aligned}\end{align} \]</div>
<p>上式看上去很复杂，但其实可以转化成矩阵的乘法，我们定义如下两个矩阵：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-35">
<span class="eqno">(19.2.51)<a class="headerlink" href="#equation-probability-model-25-34-35" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}W^{(t)} &amp;= \text{diag} \left \{ \frac{ 1}{ a(\phi) \nu(\mu) }
\left ( \frac{\partial \mu}{\partial \eta} \right )^2
\right \}_{(n\times n)}  \ \ \ \ \text{对角矩阵}\\
\mathcal{\eta}^{(t)} &amp;= \left \{ (y-\mu)  \left ( \frac{\partial \eta}{\partial \mu} \right) + \eta^{(t)}
\right \}_{(n\times 1 )}\end{aligned}\end{align} \]</div>
<p>其中 <span class="math notranslate nohighlight">\(\frac{\partial \mu}{\partial \eta}\)</span> 是激活函数的对 <span class="math notranslate nohighlight">\(\eta\)</span> 的导数，
<span class="math notranslate nohighlight">\(\frac{\partial \eta}{\partial \mu}\)</span> 是链接函数对 <span class="math notranslate nohighlight">\(\mu\)</span> 的导数。
<a class="reference internal" href="#equation-eq-34-49">公式(19.2.50)</a> 就可以改写成矩阵乘积的形式：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-49-1">
<span class="eqno">(19.2.52)<a class="headerlink" href="#equation-eq-34-49-1" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}(X^TW^{(t)} X) \beta^{(t+1)} &amp;= X^T W^{(t)} \eta^{(t)}\\ &amp; \Downarrow{\text{移项}}\\
\beta^{(t+1)} &amp;= (X^TW^{(t)} X)^{-1} X^T W^{(t)} \eta^{(t)}\end{aligned}\end{align} \]</div>
<p><a class="reference internal" href="#equation-eq-34-49-1">公式(19.2.52)</a> 就是最终参数向量更新的公式，它在形式上等价于加权的最小二乘法，
其中 <span class="math notranslate nohighlight">\(W\)</span> 是权重矩阵，并且每一次迭代都要重新计算 <span class="math notranslate nohighlight">\(W\)</span>
，所以我们把这个算法称为迭代重加权最小二乘法(Iteratively Reweighted Least Square,IRLS)，
“reweighted” 指的就是每次迭代重新计算权重矩阵。
IRLS和牛顿法的区别就是，使用的 Hessian 矩阵的期望矩阵(expected Hessian)代替了原来的 Hessian 矩阵。
原来的 Hessian 矩阵是在观测样的基础上计算的，所以称为 observed Hessian
，对其在观测样本变量上求期望就得到了 expected Hessian 。</p>
<p><strong>迭代初始值的设定</strong></p>
<p>对比下 Newton–Raphson 算法的参数迭代公式( <a class="reference internal" href="#equation-eq-34-35">公式(19.2.35)</a> )
和IRLS算法的参数迭代公式( <a class="reference internal" href="#equation-eq-34-49-1">公式(19.2.52)</a> )，
可以发现IRLS算法并不需要直接在 <span class="math notranslate nohighlight">\(\beta^{(t)}\)</span> 的基础上进行参数迭代，
IRLS算法的参数迭代仅仅依赖 <span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\eta\)</span>
，因此与 Newton–Raphson 算法不同的是，IRLS 不需要对参数向量 <span class="math notranslate nohighlight">\(\beta\)</span>
进行初始值的猜测，只需要给 <span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\eta\)</span> 赋予一个初始值即可。</p>
<ul class="simple">
<li><p>对于二项式分布，可以令 <span class="math notranslate nohighlight">\(\mu_n^{(0)}=k_n(y_n+0.5)/(k_n+1)\)</span>
，<span class="math notranslate nohighlight">\(\eta_n^{(0)}=g(\mu_n^{(0)})\)</span> 。</p></li>
<li><p>对于非二项式分布，可以令 <span class="math notranslate nohighlight">\(\mu_n^{(0)}=y_n\)</span>
， <span class="math notranslate nohighlight">\(\eta_n^{(0)}=g(\mu_n^{(0)})\)</span> 。</p></li>
</ul>
<p><strong>收敛性判断</strong></p>
<p>在迭代的过程中，我们可以检查参数 <span class="math notranslate nohighlight">\(\beta\)</span> 的相对变化来决定是否结束算法。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-36">
<span class="eqno">(19.2.53)<a class="headerlink" href="#equation-probability-model-25-34-36" title="此公式的永久链接"></a></span>\[\sqrt{\frac{ (\beta^{new}-\beta^{old})^T (\beta^{new}-\beta^{old})  }{ \beta^{old^T} \beta^{new} } } &lt; \epsilon\]</div>
<p>也可以通过相对偏差来判断。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-37">
<span class="eqno">(19.2.54)<a class="headerlink" href="#equation-probability-model-25-34-37" title="此公式的永久链接"></a></span>\[\left|\frac{D(y-\mu^{new})-D(y,\mu^{old})   }{D(y,\mu^{old})} \right| &lt;\epsilon\]</div>
<p><strong>估计量的标准误差</strong></p>
<p>回顾一下在 <a class="reference internal" href="2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-mle-estimator"><span class="std std-numref">节 2.7.3</span></a> 我们讲的最大似然估计量的评价，
我们知道最大似然估计量的协方差矩阵就是 <span class="math notranslate nohighlight">\(I(\beta)^{-1}\)</span>
，显然在IRLS算法过程中已经计算出了 <span class="math notranslate nohighlight">\(I(\beta)^{-1}=- \mathbb{E}[H(\beta)]=(X^TW^{(t)} X)^{-1}\)</span>
，所以使用IRLS我们可以很方便的得到估计量的标准误差。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-38">
<span class="eqno">(19.2.55)<a class="headerlink" href="#equation-probability-model-25-34-38" title="此公式的永久链接"></a></span>\[SSE = \sqrt{ \text{diag} [{(X^TW^{(t)} X)}^{-1} ]}\]</div>
<p><strong>分散参数的估计</strong></p>
<p>虽然IRLS算法本身并不包含对分散参数的估计，但我们可以通过  Pearson <span class="math notranslate nohighlight">\(\mathcal{X}^2\)</span> 统计来得到 <span class="math notranslate nohighlight">\(a(\phi)\)</span>
。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-39">
<span class="eqno">(19.2.56)<a class="headerlink" href="#equation-probability-model-25-34-39" title="此公式的永久链接"></a></span>\[\hat{a}(\phi) = \frac{1}{N-p} \sum_{n=1}^N \frac{ (y_n - \hat{\mu}_n)^2}{\nu(\hat{\mu}_n)}\]</div>
<p>或者使用偏差：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-40">
<span class="eqno">(19.2.57)<a class="headerlink" href="#equation-probability-model-25-34-40" title="此公式的永久链接"></a></span>\[\hat{a}(\phi) = \frac{D(y,\hat{\mu})}{N-p}\]</div>
<p><span class="math notranslate nohighlight">\(N\)</span> 是观测样本的数量，<span class="math notranslate nohighlight">\(p\)</span> 是参数向量 <span class="math notranslate nohighlight">\(\beta\)</span> 的长度，
<span class="math notranslate nohighlight">\(\hat{\mu}_n\)</span> 是第n条样本的模型预测值，
<span class="math notranslate nohighlight">\(\hat{a}(\phi)\)</span> 服从自由度为 <span class="math notranslate nohighlight">\(n-p\)</span> 的 <span class="math notranslate nohighlight">\(\mathcal{X}^2_{n-p}\)</span> 分布。</p>
</section>
</section>
<section id="goodness-of-fit">
<h2><span class="section-number">19.3. </span>goodness of fit<a class="headerlink" href="#goodness-of-fit" title="此标题的永久链接"></a></h2>
<p>我们知道，模型的参数越多对数据的拟合程度就越好，极端情况下，模型参数的数量和样本的数量相同，
这时就相当于对每条样本都有一个独立的参数(模型)去拟合它，理论上可以完美拟合所有的样本。
我们把这样的模型成为之饱和模型(saturated model)，也可以称为
完整模型(full model)或者最大模型(maximal model)。
饱和模型虽然能完美拟合数据集，但它并没有从数据集中学习出任何的统计信息(统计规律)，所不具备泛化能力，
俗称过拟合(over-fitted)。
通过为饱和模型中的参数添加约束，比如令一些参数值为0，相当于去掉了一个参数，这样就得到了简化的模型。
简化模型对数据集拟合度下降了，但是其泛化能力会得到提升，
更少的参数数量可以得到更大的泛化能力。
但是参数数量变少，会降低拟合程度，参数数量越少拟合度就越差，所以也不是参数越少越好。</p>
<p>在开发一个模型时，我们希望模型的预测值 <span class="math notranslate nohighlight">\(\hat{y}\)</span> 尽可能的接近数据的真实值 <span class="math notranslate nohighlight">\(y\)</span>
，对于一个规模为N的观测值样本，我们可以考虑参数数量在 <span class="math notranslate nohighlight">\([1,N]\)</span> 之间的候选模型，
最简单的模型是只有一个参数的模型，但它对所有的样本的预测值都是一样的，缺乏拟合能力。
最复杂的模型是含有N个参数的模型，它可以完美拟合所有样本，但是它缺乏泛化能力。
理论上，我们期望得到一个参数数量尽可能少，又能保持拟合能力的模型。</p>
<p>在统计学中，似然比检验(likelihood-ratio test,LRT)用来对比两个模型对于当前数据集的拟合程度，
其是利用似然比(likelihood-ratio LR)的值来比较两个模型的优劣。
LRT的计算公式如下：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-41">
<span class="eqno">(19.3.1)<a class="headerlink" href="#equation-probability-model-25-34-41" title="此公式的永久链接"></a></span>\[LR = 2 \ln \frac{L1}{L2} = 2 (\ln L1-\ln L2)\]</div>
<p>其中L1为复杂模型最大似然值，L2为简单模型最大似然值。
从公式可以看出，似然比就是两个模型的似然值之比的对数，也可以看成是两个模型对数似然值的差值。
似然(likelihood)，实际上也可以翻译为可能性，表示的是样本发生的概率，显然似然值越大的模型对数据的拟合也就越好。
似然比就是直接比较两个模型的似然值大小。
但是并不是任意两个模型都可以应用似然比去比较，只有在特定条件下似然比才有意义。</p>
<ol class="arabic simple">
<li><p>两个模型采用同一份数据集，样本的数量和特征都是相同的。这很好理解，不同数据集似然值自然是不同的，没有比较的意义。</p></li>
<li><p>两个模型是嵌套关系(nested)。所谓嵌套关系就是，其中一个模型是通过把另一个模型中的部分参数设置为0而得到的。</p></li>
</ol>
<p>当样本足够大时，似然比是渐进服从卡方分布的，其自由度等于两个模型的参数数量之差(参数值为0的参数的数量)。
这样根据卡方分布临界值表，我们就可以判断模型差异是否显著。</p>
<p>在GLM中，我们定义一个评估模型拟合优度(Goodness of fit)的指标，称之为偏差(deviance)。
<strong>偏差的计算方法就是饱和模型和拟合模型之间的似然比。</strong>
我们用符号 <span class="math notranslate nohighlight">\(L_m\)</span> 表示我们拟合出的目标模型的似然值，
用符号 <span class="math notranslate nohighlight">\(L_f\)</span> 表示饱和模型的似然值。
用符号 <span class="math notranslate nohighlight">\(D\)</span> 表示偏差，其通过下式给出：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-50">
<span class="eqno">(19.3.2)<a class="headerlink" href="#equation-eq-34-50" title="此公式的永久链接"></a></span>\[D = 2 (\ln L_f -\ln L_m)\]</div>
<p>我们知道在GLM中，模型的预测值 <span class="math notranslate nohighlight">\(\hat{y}\)</span> 就是分布 <span class="math notranslate nohighlight">\(p(y|x)\)</span> 的期望值
<span class="math notranslate nohighlight">\(\mathbb{E}[p(y|x)]=\hat{\mu}\)</span> ，即 <span class="math notranslate nohighlight">\(\hat{y}=\hat{\mu}\)</span> 。
所以这里我们用 <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 表示模型的预测值。
并且我们知道，指数族形式的规范参数 <span class="math notranslate nohighlight">\(\theta\)</span> 可以看做是一个关于均值参数 <span class="math notranslate nohighlight">\(\mu\)</span> 的函数，
因此，在GLM框架下，目标模型的似然值为：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-51">
<span class="eqno">(19.3.3)<a class="headerlink" href="#equation-eq-34-51" title="此公式的永久链接"></a></span>\[L_m = \exp \left \{  \sum_{n=1}^N \frac{y_n \theta(\hat{\mu}_n) -b(\theta(\hat{\mu}_n))}{a(\phi)}
+ \sum_{n=1}^N  c(y_n;\phi) \right \}\]</div>
<p>对于饱和模型，每条样本的预测值就是样本的真实值，即 <span class="math notranslate nohighlight">\(\hat{y_n}=y_n\)</span> ，换句话说，
对于饱和模型，满足 <span class="math notranslate nohighlight">\(\hat{y_n}=\hat{\mu}_n=y_n\)</span>
。因此，饱和模型的似然值为：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-52">
<span class="eqno">(19.3.4)<a class="headerlink" href="#equation-eq-34-52" title="此公式的永久链接"></a></span>\[L_f = \exp \left \{  \sum_{n=1}^N \frac{y_n \theta(y_n) -b(\theta(y_n))}{a(\phi)}
+ \sum_{n=1}^N  c(y_n;\phi) \right \}\]</div>
<p>把 <a class="reference internal" href="#equation-eq-34-51">公式(19.3.3)</a> 和 <a class="reference internal" href="#equation-eq-34-52">公式(19.3.4)</a> 代入到 <a class="reference internal" href="#equation-eq-34-50">公式(19.3.2)</a>
可得到GLM的偏差：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-42">
<span class="eqno">(19.3.5)<a class="headerlink" href="#equation-probability-model-25-34-42" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}D &amp;= 2 (\ln L_f -\ln L_m)\\&amp;= \frac{2}{a(\phi)} \sum_{n=1}^N  [ y_n \{ \theta(y_n) - \theta(\hat{\mu}_n) \} - b\{\theta(y_n)\} + b\{\theta(\hat{\mu}_n)\} ]\\&amp;\triangleq 2 \sum_{n=1}^N  [ y_n \{ \theta(y_n) - \theta(\hat{\mu}_n) \} - b\{\theta(y_n)\} + b\{\theta(\hat{\mu}_n)\} ]\end{aligned}\end{align} \]</div>
<p>由于 <span class="math notranslate nohighlight">\(a(\phi)\)</span> 与样本和参数都无关，是一个常量，所以通常会被省略掉。
对于一个特定的样本集和模型，饱和模型的似然值是一个常数值，
因此
<strong>最大化似然函数和最小化偏差是等价的</strong>
<strong>，在进行参数学习时可以用最小化偏差替代最大化对数似然函数。</strong></p>
<p>在统计中，偏差(deviance)是统计模型的拟合优度统计；
它通常用于统计假设检验。
它是将普通最小二乘中的残差平方和用于通过最大似然实现模型拟合的情况的概括。
它在指数弥散模型和广义线性模型中起着重要作用。</p>
<p><a class="reference external" href="https://newonlinecourses.science.psu.edu/stat504/node/220/">https://newonlinecourses.science.psu.edu/stat504/node/220/</a></p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Deviance_(statistics">https://en.wikipedia.org/wiki/Deviance_(statistics</a>)</p>
<p><a class="reference external" href="https://www.xiaofandajie.top/2018/03/06/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E6%B3%95%E4%B8%8Elogistic%E5%9B%9E%E5%BD%92/">https://www.xiaofandajie.top/2018/03/06/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E6%B3%95%E4%B8%8Elogistic%E5%9B%9E%E5%BD%92/</a></p>
<p><a class="reference external" href="https://bookdown.org/hezhijian/book/est.html#section-30">https://bookdown.org/hezhijian/book/est.html#section-30</a></p>
</section>
<section id="id9">
<h2><span class="section-number">19.4. </span>连续值响应模型<a class="headerlink" href="#id9" title="此标题的永久链接"></a></h2>
<p>我们知道在大部分机器学习问题中，都是在寻找因变量 <span class="math notranslate nohighlight">\(Y\)</span>
和自变量 <span class="math notranslate nohighlight">\(X\)</span> 之间的关系，
在概率的语义下，用条件概率 <span class="math notranslate nohighlight">\(P(Y|X)\)</span> 定义这种关系。
GLM框架给了定义条件概率 <span class="math notranslate nohighlight">\(P(Y|X)\)</span> 的一种通用性方法，
GLM包含了一类统计模型，可以在不同的场景下选择其中合适的模型去应用。
在GLM框架下要确定一个具体的模型，理论上需要确定两个信息：</p>
<ul class="simple">
<li><p>根据标签 <span class="math notranslate nohighlight">\(Y\)</span> 的数据分布选取一个合适的指数族分布作为变量 <span class="math notranslate nohighlight">\(Y\)</span> 的概率分布假设。</p></li>
<li><p>确定一个连接函数 <span class="math notranslate nohighlight">\(g(\cdot)\)</span>，把特征数据 <span class="math notranslate nohighlight">\(X\)</span> 的线性预测器 <span class="math notranslate nohighlight">\(\beta^T x\)</span> 与 <span class="math notranslate nohighlight">\(Y\)</span> 的概率分布的均值参数 <span class="math notranslate nohighlight">\(\mu\)</span>
连接在一起 <span class="math notranslate nohighlight">\(\beta^T x=g(\mu)\)</span> 。</p></li>
</ul>
<p>不同的场景标签 <span class="math notranslate nohighlight">\(Y\)</span> 拥有不同的数据范围和分布，就需要选取特定的指数分布。
本节开始，我们介绍指数族中常见分布的GLM，帮助大家在遇到具体的场景时，
能用GLM解决问题。根据数据的不同，我们分为如下几类：</p>
<ul class="simple">
<li><p>连续值变量，对应着实数值回归问题场景。</p></li>
<li><p>二值离散变量，对应着二分类问题场景。</p></li>
<li><p>多值离散变量，对应着多分类问题场景。</p></li>
<li><p>计数离散变量。</p></li>
</ul>
<section id="id10">
<h3><span class="section-number">19.4.1. </span>高斯族<a class="headerlink" href="#id10" title="此标题的永久链接"></a></h3>
<p>1800年代，
约翰·卡尔·弗里德里希·高斯（Johann Carl Friedrich Gauss）
描述了最小二乘拟合方法和以他的名字命名的分布。
高斯密度函数具有对称的钟形形状，通常称为正态分布。
正态累积分布函数是指数分布族的成员，因此可以用于GLM框架。
此外，由于GLM的理论最初被认为是对普通最小二乘（OLS）模型的扩展，
因此我们将首先说明OLS如何适合GLM框架。
这样一来，了解其他GLM模型如何概括此基本形式就变得容易了。</p>
<p>高斯分布是实数域的连续分布，其输入域是整个实数域 <span class="math notranslate nohighlight">\(R=(-\infty,+\infty)\)</span>
。用均值 <span class="math notranslate nohighlight">\(\mu\)</span> 进行参数化的高斯概率密度函数可以表示为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-43">
<span class="eqno">(19.4.1)<a class="headerlink" href="#equation-probability-model-25-34-43" title="此公式的永久链接"></a></span>\[f(y;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi \sigma^2}} \exp \left \{ - \frac{(y-\mu)^2}{2\sigma^2} \right \}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(f(\cdot)\)</span> 表示在给定参数 <span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 时，变量 <span class="math notranslate nohighlight">\(y\)</span> 的概率密度函数的一般形式。
<span class="math notranslate nohighlight">\(y\)</span> 表示输出变量， <span class="math notranslate nohighlight">\(\mu\)</span> 表示均值参数， <span class="math notranslate nohighlight">\(\sigma^2\)</span> 表示 scale parameter 。</p>
<p>基于高斯或正态分布的回归模型通常称为OLS模型，该标准回归模型通常是统计学模型的入门模型。
在高斯回归模型中，我们定义响应变量和特征变量的之间的关系是：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-44">
<span class="eqno">(19.4.2)<a class="headerlink" href="#equation-probability-model-25-34-44" title="此公式的永久链接"></a></span>\[y = \beta^T x + \epsilon\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\beta^T x\)</span> 是输入变量的线性预测器，<span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0,\sigma^2)\)</span> 是一个高斯误差项，
输出变量 <span class="math notranslate nohighlight">\(y\)</span> 是一个服从高斯分布的变量 <span class="math notranslate nohighlight">\(y \sim \mathcal{N}(\beta^Tx,\sigma^2)\)</span>
。现在我们用GLM框架来解释OLS。</p>
<section id="glm">
<h4><span class="section-number">19.4.1.1. </span>GLM中高斯族的定义<a class="headerlink" href="#glm" title="此标题的永久链接"></a></h4>
<p>当响应变量 <span class="math notranslate nohighlight">\(Y\)</span> 数值范围是实数值时，并且其数据分布是近似高斯分布时，
我们就可以为响应变量 <span class="math notranslate nohighlight">\(Y\)</span> 建立高斯分布假设。
在GLM框架下，我们需要用指数分散族(EDF)的形式表示指数族的分布，高斯分布的EDF表示为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-45">
<span class="eqno">(19.4.3)<a class="headerlink" href="#equation-probability-model-25-34-45" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}f(y;\mu,\sigma^2) &amp;=\frac{1}{\sqrt{2\pi \sigma^2}} \exp \left \{ - \frac{(y-\mu)^2}{2\sigma^2} \right \}\\&amp;= \exp \left \{ -\frac{(y-\mu)^2}{2\sigma^2} - \frac{1}{2} \ln (2\pi\sigma^2) \right \}\\&amp;= \exp \left \{ -\frac{y\mu - \mu^2/2}{\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{1}{2} \ln (2\pi\sigma^2) \right \}\end{aligned}\end{align} \]</div>
<p>和EDF的标准形式 <a class="reference internal" href="#equation-eq-34-edf">公式(19.1.12)</a> 对比下，有：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-46">
<span class="eqno">(19.4.4)<a class="headerlink" href="#equation-probability-model-25-34-46" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\theta &amp;= \mu\\
b(\theta) &amp;= \mu^2/2\\a(\phi) &amp;= \sigma^2\end{aligned}\end{align} \]</div>
<p>对于高斯分布的GLM来说，链接函数 <span class="math notranslate nohighlight">\(g\)</span> 是恒等函数，即 <span class="math notranslate nohighlight">\(\eta=\mu\)</span> ，我们称之为规范链接函数(canonical link)。
此外，对于高斯分布，规范参数 <span class="math notranslate nohighlight">\(\theta\)</span> 和均值参数 <span class="math notranslate nohighlight">\(\mu\)</span> 之间也是恒等函数关系，
所以，对于高斯分布有：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-47">
<span class="eqno">(19.4.5)<a class="headerlink" href="#equation-probability-model-25-34-47" title="此公式的永久链接"></a></span>\[\beta^Tx = \eta=\mu=\theta\]</div>
<p>均值参数可以通过累积函数 <span class="math notranslate nohighlight">\(b(\theta)\)</span> 的一阶导数求得：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-48">
<span class="eqno">(19.4.6)<a class="headerlink" href="#equation-probability-model-25-34-48" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}b'(\theta) &amp;= \frac{\partial b}{\partial \theta}\\&amp;= \frac{\partial b}{\partial \mu} \frac{\partial \mu}{\partial \theta}\\&amp;= (\mu)(1) = \mu\end{aligned}\end{align} \]</div>
<p>方差可以通过累积函数 <span class="math notranslate nohighlight">\(b(\theta)\)</span> 的二阶导数和 <span class="math notranslate nohighlight">\(a(\phi)\)</span> 的乘积求得：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-49">
<span class="eqno">(19.4.7)<a class="headerlink" href="#equation-probability-model-25-34-49" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}b''(\theta) &amp;= \frac{\partial^2 b}{\partial \theta^2}\\&amp;= \frac{\partial }{\partial \theta} \left ( \frac{\partial b}{\partial \mu} \frac{\partial \mu}{\partial \theta} \right )\\&amp;= \frac{\partial }{\partial \theta}(\mu)\\&amp;= \frac{\partial }{\partial \mu} \mu \frac{\partial \mu}{\partial \theta}\\&amp;= (1)(1)=1\end{aligned}\end{align} \]</div>
<p>高斯分布的模型预测值为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-50">
<span class="eqno">(19.4.8)<a class="headerlink" href="#equation-probability-model-25-34-50" title="此公式的永久链接"></a></span>\[\hat{y} = \mathbb{E}[p(y|x)] = \mu =\beta^Tx\]</div>
<p>这和OLS是等价的。
现在我们来看下高斯分布的GLM模型的偏差的计算。
观测样本数据的对数似然函数可以通过下式计算：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-51">
<span class="eqno">(19.4.9)<a class="headerlink" href="#equation-probability-model-25-34-51" title="此公式的永久链接"></a></span>\[\ell(\hat{\mu},\sigma^2;y)=\sum_{n=1}^N \left \{ \frac{y_n \hat{\mu}_n - \hat{\mu}_n^2/2}{\sigma^2} -
\frac{y_n^2}{2\sigma^2} - \frac{1}{2} \ln (2\pi\sigma^2) \right \}\]</div>
<p>饱和模型的对数似然函数为；</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-52">
<span class="eqno">(19.4.10)<a class="headerlink" href="#equation-probability-model-25-34-52" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\ell(y,\sigma^2;y) &amp;=\sum_{n=1}^N \left \{ \frac{y_n^2 - y_n^2/2}{\sigma^2} -
\frac{y_n^2}{2\sigma^2} - \frac{1}{2} \ln (2\pi\sigma^2) \right \}\\&amp;= \sum_{n=1}^N \left \{- \frac{1}{2} \ln (2\pi\sigma^2) \right \}\end{aligned}\end{align} \]</div>
<p>模型的偏差为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-53">
<span class="eqno">(19.4.11)<a class="headerlink" href="#equation-probability-model-25-34-53" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}D &amp;= 2\{ \ell(y,\sigma^2;y) - \ell(\hat{\mu},\sigma^2;y) \}\\&amp;= 2 \sum_{n=1}^N \left \{
 - \frac{1}{2} \ln (2\pi\sigma^2)
 - \frac{y_n \hat{\mu}_n - \hat{\mu}_n^2/2}{\sigma^2}
+ \frac{y_n^2}{2\sigma^2}
+ \frac{1}{2} \ln (2\pi\sigma^2)   \right \}\\&amp;= 2 \sum_{n=1}^N \left \{\frac{ y_n^2  -  2y_n \hat{\mu}_n  + \hat{\mu}_n^2 }{2\sigma^2}  \right \}\\&amp;= \frac{2}{2\sigma^2} \sum_{n=1}^N \left \{ y_n^2  -  2y_n \hat{\mu}_n  + \hat{\mu}_n^2   \right \}\\&amp;\triangleq  \sum_{n=1}^N  (y_n-\hat{\mu}_n)^2\end{aligned}\end{align} \]</div>
<p>我们发现高斯偏差等价于平方损失的和，这是高斯族独有的属性。</p>
</section>
<section id="ml">
<h4><span class="section-number">19.4.1.2. </span>ML 估计<a class="headerlink" href="#ml" title="此标题的永久链接"></a></h4>
<p>现在我们来看下高斯分布的GLM的似然估计，首先我们需要把均值参数化的模型转化成使用 <span class="math notranslate nohighlight">\(\beta^Tx\)</span>
参数化的形式。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-54">
<span class="eqno">(19.4.12)<a class="headerlink" href="#equation-probability-model-25-34-54" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}f(y;\mu,\sigma^2) &amp;=\frac{1}{\sqrt{2\pi \sigma^2}} \exp \left \{ - \frac{(y-\mu)^2}{2\sigma^2} \right \}\\&amp;= \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left \{ - \frac{(y-\beta^Tx)^2}{2\sigma^2} \right \}\end{aligned}\end{align} \]</div>
<p>对数似然函数为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-55">
<span class="eqno">(19.4.13)<a class="headerlink" href="#equation-probability-model-25-34-55" title="此公式的永久链接"></a></span>\[\ell(\beta,\sigma^2;y) = \sum_{n=1}^N \left \{
\frac{y_n \beta^Tx_n -(\beta^Tx_n)^2/2}{\sigma^2} -\frac{y_n^2}{2\sigma^2} - \frac{1}{2} \ln(2\pi\sigma^2)
\right \}\]</div>
<p>对于采用规范链接函数的高斯模型的对数似然函数的一阶偏导数为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-56">
<span class="eqno">(19.4.14)<a class="headerlink" href="#equation-probability-model-25-34-56" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\frac{\partial \ell}{\partial \beta_j} &amp;= \sum_{n=1}^N \frac{1}{\sigma^2}(y_n-\beta^T x_n)x_{jn}\\\frac{\partial \ell}{\partial \sigma} &amp;= \sum_{n=1}^N \frac{1}{\sigma}
 \left \{ \left ( \frac{y_n-\beta^T x_n}{\sigma} \right )^2 -1 \right \}\end{aligned}\end{align} \]</div>
<p>二阶偏导数为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-57">
<span class="eqno">(19.4.15)<a class="headerlink" href="#equation-probability-model-25-34-57" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\frac{\partial^2 \ell}{\partial \beta_j\beta_k} &amp;= - \sum_{n=1}^N \frac{1}{\sigma^2}x_{jn}x_{kn}\\\frac{\partial^2 \ell}{\partial \beta_j \partial \sigma} &amp;= -\sum_{n=1}^N \frac{2}{\sigma^3} (y_n-\beta^Tx_n) x_{jn}\\\frac{\partial^2 \ell}{\partial \sigma \partial \sigma} &amp;= -\sum_{n=1}^N
\frac{1}{\sigma^2} \left \{ 3\left ( \frac{y_n-\beta^T x_n}{\sigma} \right )^2 - 1\right \}\end{aligned}\end{align} \]</div>
</section>
<section id="log-gaussian">
<h4><span class="section-number">19.4.1.3. </span>对数高斯(log-Gaussian)模型<a class="headerlink" href="#log-gaussian" title="此标题的永久链接"></a></h4>
<p>使用GLM作为模型构建框架的重要原因是能够轻松调整模型以适合特定的标签数据情况。
规范链接(canonical-link)高斯模型假定标签为正态分布。
尽管正态分布模型对于克服此假设具有一定的鲁棒性，但仍然有许多数据情况不适合正态分布模型的情况。
不幸的是，许多研究人员已将规范链接高斯模型用于不符合高斯模型所基于假设的数据情况。
当然，许多研究人员很少接受非正态分布模型建模方面的培训。
现在，大多数流行的软件包都具有GLM功能，或者至少实现了许多最广泛使用的GLM程序，
包括 logistic, probit, and Poisson regression。</p>
<p>线性预测器 <span class="math notranslate nohighlight">\(\eta=\beta^Tx\)</span> 的取值范围是整个实数域 <span class="math notranslate nohighlight">\(R=(-\infty,+\infty)\)</span>
，链接函数的主要作用就是将实数域的 <span class="math notranslate nohighlight">\(\eta\)</span> 映射到标签数据的值域。
当标签数据仍然服从高斯分布，但是其范围不再是整个实数域，而是大于0的实数域时，恒等函数的规范链接(canonical-link)函数将不再适用，
这时就需要选取一个合适的链接函数，将 <span class="math notranslate nohighlight">\(\eta\)</span> 从 <span class="math notranslate nohighlight">\(R=(-\infty,+\infty)\)</span> 映射到
<span class="math notranslate nohighlight">\(R=(0,+\infty)\)</span> 。
log-Gaussian模型仍然是基于高斯分布，但它的链接函数不再是规范链接(canonical-link)，
而是对数链接函数，对数链接通常用于标签数据是非负值的情况。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-58">
<span class="eqno">(19.4.16)<a class="headerlink" href="#equation-probability-model-25-34-58" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\eta &amp;= \ln \mu\\\mu &amp;= \exp \{ \eta\}\end{aligned}\end{align} \]</div>
<p>在提出GLM框架之前，研究人员在遇到标签数据都是正数的情形时，采用的方法是把标签数据进行对数转化，
<span class="math notranslate nohighlight">\(y=\ln(y)\)</span> ，通过这种转换令数据符合整个实数域上的高斯模型假设，然后再利用标准高斯模型进行建模。
然而事实证明，这种方法在易用性和效果上都不如采用对数链接函数的对数高斯模型。
对数高斯模型的实现非常简单，只需要把标准高斯模型（采用恒等函数，规范链接函数）的链接函数替换成对数链接函数即可。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-59">
<span class="eqno">(19.4.17)<a class="headerlink" href="#equation-probability-model-25-34-59" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}f(y;\beta,\sigma^2) &amp;=\frac{1}{\sqrt{2\pi \sigma^2}} \exp \left \{ - \frac{(y-\exp(\beta^Tx))^2}{2\sigma^2} \right \}\\&amp;= \exp \left \{ -\frac{(y-\exp(\beta^Tx))^2}{2\sigma^2} - \frac{1}{2} \ln (2\pi\sigma^2) \right \}\\&amp;= \exp \left \{ -\frac{y\exp(\beta^Tx) - \exp(\beta^Tx)^2/2}{\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{1}{2} \ln (2\pi\sigma^2) \right \}\end{aligned}\end{align} \]</div>
<p>对数高斯模型的对数似然函数(log-likelihood)为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-60">
<span class="eqno">(19.4.18)<a class="headerlink" href="#equation-probability-model-25-34-60" title="此公式的永久链接"></a></span>\[\ell(\mu;y,\sigma^2) = \sum_{n=1}^N \left [
\frac{y_n \exp(\beta^Tx_n) - \{\exp(\beta^Tx_n)\}^2/2  }{\sigma^2}
-\frac{y_n^2}{2\sigma^2} - \frac{1}{2}\ln(2\pi\sigma^2)
\right ]\]</div>
<p>对数高斯模型就是在标准高斯模型的基础上，把链接函数由 <span class="math notranslate nohighlight">\(\eta=\mu\)</span> 改成了
<span class="math notranslate nohighlight">\(\eta=\ln(\mu)\)</span>
，相应的反链接函数由 <span class="math notranslate nohighlight">\(\mu=\eta\)</span> 变成 <span class="math notranslate nohighlight">\(\mu=\exp(\eta)\)</span>
。
规范链接函数的导数为1，然而对数链接函数的导数为 <span class="math notranslate nohighlight">\(1/\mu\)</span> 。
注意对 <a class="reference internal" href="#equation-eq-34-jac">公式(19.2.25)</a> 相应位置替换即可得到似然函数的梯度。</p>
</section>
</section>
<section id="gamma">
<h3><span class="section-number">19.4.2. </span>Gamma族<a class="headerlink" href="#gamma" title="此标题的永久链接"></a></h3>
<section id="id11">
<h4><span class="section-number">19.4.2.1. </span>Gamma函数<a class="headerlink" href="#id11" title="此标题的永久链接"></a></h4>
<p>Gamma模型用于对输出数据只能是大于等于0的情况，</p>
</section>
</section>
</section>
<section id="id12">
<h2><span class="section-number">19.5. </span>二项响应模型<a class="headerlink" href="#id12" title="此标题的永久链接"></a></h2>
</section>
<section id="id13">
<h2><span class="section-number">19.6. </span>多项响应模型<a class="headerlink" href="#id13" title="此标题的永久链接"></a></h2>
</section>
<section id="id14">
<h2><span class="section-number">19.7. </span>计数响应模型<a class="headerlink" href="#id14" title="此标题的永久链接"></a></h2>
<p>泊松分布就是描述某段时间内，事件具体的发生概率。</p>
<section id="id15">
<h3><span class="section-number">19.7.1. </span>泊松分布<a class="headerlink" href="#id15" title="此标题的永久链接"></a></h3>
<p>泊松分布实际上是二项分布的试验(trials)次数n趋近于无穷时的场景，我们用一个例子说明。
假设一个交通观察员需要对某个路口的车流量进行建模，然后用模型预测未来一个小时内从这个路口通过的车次。
为了简化问题，我们假设路口的交通量不存在高峰期低峰期，即交通量不会随着时间的变化而变化，
并且每个时间片段内通过的车辆是互不影响的，当前一小时内车辆通过与否不影响下一个小时车辆。
观察员首先根据这个路口历史上车辆通过情况，计算出平均每小时通过车辆的数量为
<span class="math notranslate nohighlight">\(\lambda\)</span>
。我们把一个小时内从路口通过的车次数看做一个随机变量，用符号 <span class="math notranslate nohighlight">\(X\)</span> 表示，
那么 <span class="math notranslate nohighlight">\(\lambda\)</span> 就是变量 <span class="math notranslate nohighlight">\(X\)</span> 的数学期望。</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-61">
<span class="eqno">(19.7.1)<a class="headerlink" href="#equation-probability-model-25-34-61" title="此公式的永久链接"></a></span>\[\mathbb{E}[X] = \lambda\]</div>
<p>我们把一辆车通过与否看做是一个伯努利变量，类似投硬币实验，1表示车辆通过，0表示车辆不通过。
把一个小时的时间区间均分成n个时间片段，比如每分钟作为一个片段，这时 <span class="math notranslate nohighlight">\(n=60\)</span> 。
每个时间片段有车辆通过就是一次成功的实验(类似于投硬币正面向上)，
没有车辆通过就是一次失败的实验(类似于投硬币反面向上)，
这样就把一小时内车辆通过问题转化成一个二项分布问题，
在n次实验中有k次成功(车辆通过)概率分布函数可以写成：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-201">
<span class="eqno">(19.7.2)<a class="headerlink" href="#equation-eq-34-201" title="此公式的永久链接"></a></span>\[p(X=k) = \binom{n}{k} p^k \left ( 1-p  \right )^{n-k}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(p\)</span> 是一次实验的成功概率，我们已经通过历史数据知道平均每小时(n次实验)中通过的车次数为
<span class="math notranslate nohighlight">\(\lambda\)</span>
，意味着n次实验中有 <span class="math notranslate nohighlight">\(\lambda\)</span> 次成功，
单次实验成功的概率(平均一分钟内通过车辆数)为：</p>
<div class="math notranslate nohighlight" id="equation-probability-model-25-34-62">
<span class="eqno">(19.7.3)<a class="headerlink" href="#equation-probability-model-25-34-62" title="此公式的永久链接"></a></span>\[p = \frac{\lambda}{n}\]</div>
<p>但是我们并不能保证每分钟只有一辆车通过，我们需要保证一个时间片段内只有一辆车通过(一次实验)
以上的二项分布的假设才有意义。
理论上，我们只要把一小时的时间区间拆的足够小，比如拆成每秒，甚至是每毫秒为一个时间片段，
这样就能尽量保证每个时间片段内只会有一辆车通过。
<span class="math notranslate nohighlight">\(n\)</span> 越大时间片段就越小，极限情况，我们可以把一小时分割成每个车辆通过的”瞬间”。
换句话说，只要 <span class="math notranslate nohighlight">\(n \to \infty\)</span>
上述假设就是成立的，因此我们为 <a class="reference internal" href="#equation-eq-34-201">公式(19.7.2)</a>
加上极限操作。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-202">
<span class="eqno">(19.7.4)<a class="headerlink" href="#equation-eq-34-202" title="此公式的永久链接"></a></span>\[p(X=k) = \lim_{n \to \infty} \binom{n}{k} p^k \left ( 1-p  \right )^{n-k}\]</div>
<p>我们发现 <a class="reference internal" href="#equation-eq-34-202">公式(19.7.4)</a> 就是二项分布的极限情况，表示的是路口未来一小时内通过的车辆数的概率分布，
<span class="math notranslate nohighlight">\(p(X=k)\)</span> 表示在一小时内通过车辆数为 <span class="math notranslate nohighlight">\(k\)</span>
的概率。
<span class="math notranslate nohighlight">\(\lambda\)</span> 表示这个时间区间内通过车辆数的期望值，
至于这个时间区间是一小时还是两小时并不重要，
只要是一个固定的时间区间就行，
所以可以看成是单位时间区间内，或者 <span class="math notranslate nohighlight">\(t\)</span> 时间区间内。</p>
<p><a class="reference internal" href="#equation-eq-34-202">公式(19.7.4)</a> 带有极限操作，事实上可以通过一些变换去掉极限符号，
现在我们尝试对其进行一些变换。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-210">
<span class="eqno">(19.7.5)<a class="headerlink" href="#equation-eq-34-210" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}p(X=k) &amp;=
\lim_{n \to \infty} \binom{n}{k}
p^k
\left ( 1-p  \right )^{n-k}\\&amp;=
\lim_{n \to \infty} \frac{n!}{(n-k)!k!}
\left (\frac{\lambda}{n} \right )^k
\left ( 1-\frac{\lambda}{n} \right )^{n-k}\\&amp;=
\lim_{n \to \infty} \frac{n!}{(n-k)!k!} \frac{\lambda^k}{n^k}
\left ( 1-\frac{\lambda}{n} \right )^n
\left ( 1-\frac{\lambda}{n} \right )^{-k}\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight" id="equation-eq-34-211">
<span class="eqno">(19.7.6)<a class="headerlink" href="#equation-eq-34-211" title="此公式的永久链接"></a></span>\[\frac{n!}{(n-k)!}
= \frac{n (n-1) \cdots 2\times  1}{(n-k)(n-k-1) \cdots 2\times 1}
= \underbrace{n (n-1) \cdots  (n-k+1)}_{\text{k个}}\]</div>
<div class="math notranslate nohighlight" id="equation-eq-34-212">
<span class="eqno">(19.7.7)<a class="headerlink" href="#equation-eq-34-212" title="此公式的永久链接"></a></span>\[\lim_{x \to a} f(x)g(x) = \lim_{x \to a} f(x)  \lim_{x \to a}g(x)\]</div>
<p><a class="reference internal" href="#equation-eq-34-210">公式(19.7.5)</a> 变成：</p>
<div class="math notranslate nohighlight" id="equation-eq-34-213">
<span class="eqno">(19.7.8)<a class="headerlink" href="#equation-eq-34-213" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}p(X=k) &amp;= \lim_{n \to \infty} \frac{n (n-1) \cdots  (n-k+1)}{n^k}
\frac{\lambda^k}{k!}
    \left ( 1-\frac{\lambda}{n} \right )^n
\left ( 1-\frac{\lambda}{n} \right )^{-k}\\&amp;=\frac{\lambda^k}{k!} \lim_{n \to \infty} \left [ \frac{n (n-1) \cdots  (n-k+1)}{n^k} \right ]
  \lim_{n \to \infty} \left [ \left ( 1-\frac{\lambda}{n} \right )^n \right ]
\lim_{n \to \infty}\left [  \left ( 1-\frac{\lambda}{n} \right )^{-k}\right ]\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight" id="equation-eq-34-214">
<span class="eqno">(19.7.9)<a class="headerlink" href="#equation-eq-34-214" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\lim_{n \to \infty}  \frac{n (n-1) \cdots  (n-k+1)}{n^k} = 1\\\lim_{n \to \infty}  \left ( 1-\frac{\lambda}{n} \right )^n = e^{\lambda}\\\lim_{n \to \infty}\left [  \left ( 1-\frac{\lambda}{n} \right )^{-k}\right ] = 1\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight" id="equation-eq-34-215">
<span class="eqno">(19.7.10)<a class="headerlink" href="#equation-eq-34-215" title="此公式的永久链接"></a></span>\[p(X=k|n) = \frac{\lambda^k}{k!} \times 1 \times  e^{\lambda} \times 1
    =\frac{\lambda^k}{k!}  e^{\lambda}\]</div>
<p>各个时间区间之间是相互独立的，互不影响，也就是不会因为当前时间区间内有车辆通过，而导致下一个时间区间内通过的车辆受到影响。</p>
<p>泊松分布的应用并不是仅限于固定的时间区间，理论上只要是固定的区间(fixed interval)即可，
比如固定大小的时间、长度、空间、面积、体积等等。</p>
</section>
</section>
<section id="id16">
<h2><span class="section-number">19.8. </span>GLM扩展<a class="headerlink" href="#id16" title="此标题的永久链接"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="页脚">
        <a href="22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html" class="btn btn-neutral float-left" title="18. 分类模型" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html" class="btn btn-neutral float-right" title="20. 混合模型" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2018, zhangzhenhu(acmtiger@outlook.com) 禁止一切形式的转载！.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用的 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a> 开发.
  


    <script src="https://utteranc.es/client.js"
            repo="zhangzhenhu/blog"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>



</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>