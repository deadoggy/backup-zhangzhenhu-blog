
我们知道在大部分机器学习问题中，都是在寻找因变量 :math:`Y`
和自变量 :math:`X` 之间的关系，
在概率的语义下，用条件概率 :math:`P(Y|X)` 定义这种关系。
GLM框架给定义条件概率 :math:`P(Y|X)` 提供了一种通用性方法，
GLM包含了一类统计模型，可以在不同的场景下选择其中合适的模型去应用。
在GLM框架下要确定一个具体的模型，理论上需要确定两个信息：

- 根据标签 :math:`Y` 的数据分布选取一个合适的指数族分布作为变量 :math:`Y` 的概率分布假设。
- 确定一个连接函数 :math:`g(\cdot)`，把特征数据 :math:`X` 的线性预测器 :math:`\beta^T x` 与 :math:`Y` 的概率分布的均值参数 :math:`\mu`
  连接在一起 :math:`\beta^T x=g(\mu)` 。




不同的场景标签 :math:`Y` 拥有不同的数据范围和分布，就需要选取特定的指数分布。
本节开始，我们介绍指数族中常见分布的GLM，帮助大家在遇到具体的场景时，
能用GLM解决问题。根据数据的不同，我们分为如下几类：

- 连续值变量，对应着实数值回归问题场景。
- 二值离散变量，对应着二分类问题场景。
- 多值离散变量，对应着多分类问题场景。
- 计数离散变量。



OLS模型通过最小化残差平方和(Residual Sum of Squares,RSS)来估计模型的参数，
有时也叫平方损失(Square loss)。

.. math::

    RSS = \sum_{n=1}^{N} (y_n - \hat{y}_n)^2


其中，:math:`y_n` 是样本的真实值，:math:`\hat{y}_n` 是此样本的模型预测值。
在OLS中，通过极小化上述损失函数求得参数的解。利用直接解析法或者梯度下降法

.. math::

    \hat{\beta} = \mathop{\arg \max}_{\beta} \sum_{n=1}^{N} (y_n - \hat{y}_n)^2


