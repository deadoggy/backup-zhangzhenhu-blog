


现在假设随机变量 :math:`X` 是类别变量，其值域空间是 :math:`\mathcal{X}=\{x_1,\dots,x_M\}` ，
类别分布的概率分布函数可以写成如下形式：

.. math::
    :label: eq_cat_distribution

    p(X|\theta) = \prod_{m=1}^{M} \theta_m^{\delta (x,x_m) }

其中 :math:`\delta (x,x_m)` 是一个指示函数，当 :math:`x=x_m` 时，
:math:`\delta (x,x_m)=1` ；反之， :math:`\delta (x,x_m)=0`
。 :eq:`eq_cat_distribution` 的含义就是，随机变量 :math:`X` 取值为 :math:`x_m`
的概率是 :math:`p(x_m|\theta)=\theta_m`
，可以看出 :eq:`eq_cat_distribution` 是伯努利分布函数的扩展。

假设进行N次类别分布的实验，其中 :math:`x_m` 出现的次数记为 :math:`n_m` ，
则有 :math:`\sum_{m=1}^M n_m=N` 。
把N次实验中每个值出现的次数 :math:`n_m` 看做一个随机变量，
则变量 :math:`n` 是服从多项式分布的。
注意，**多项式分布不是观测变量** :math:`X` **的概率分布，而是次数变量** :math:`n` **的概率分布**。
多项式分布的概率分布函数为：

.. math::
    p(n_1,n_2,\ldots,n_M|\theta_1,\theta_2,\ldots,\theta_M) = \underbrace{\frac{\Gamma(N+1)}{\prod_{m=1}^{M} \Gamma (n_m +1 )}}_{\text{组合数}} \prod_{m=1}^M \theta_m^{n_m}


多项式分布表示，在N次 **独立** 实验中，观测变量 :math:`X` 各个值出现次数为 :math:`(n_1,n_2,\ldots,n_M)` 的概率分布，
其中包含Gamma函数的子项是实验顺序相关的组合数，如果只有单次实验就不需要这部分。
显然，多项式分布是二项分布的扩展。
本小节的内容暂时和多项式分布无关，本节讨论类别分布 :math:`p(X;\theta)` 的参数估计。
假设我们已经有随机变量 :math:`X` 的一些观测样本，
我们用费我们的观测样本独立的从类别变量 :math:`X` 采样得到的，每一个样本都看成是独立的单次实验。
单次实验的多项式分布的概率质量函数可以表示为：



最大似然的估计值就等于经验估计值(在样本中出现的相对频次) :math:`\hat{p}_{\mathcal{D}}(1)` 。
这个值称为伯努利分布的充分统计量(sufficient statistics)。
通过强大的大数定律，经验分布最终会收敛到正确的概率，因此我们看到在这种情况下最大似然与之是一致的。

.. hint:: 充分统计量(sufficient statistic)

    对于一个概率分布(或者一个分布族)都有一个固定的概率密度(质量)函数，而这个函数中又存在着一些可变参数，
    只有当这些参数的值确定时才能确定一个具体的分布。对于一个参数未知的分布，
    很多时候我们可以通过这个分布的一些IID样本去估计（比如最大似然估计）出参数的值。
    当我们通过分布的样本去估计分布的参数时，自然是需要样本中包含的一些"信息(information)"，利用这些信息去估计参数值。

    统计量(statistic)是 **样本的一个函数**，其代表着从样本中提取的一些"信息"，比如样本的均值(mean)，样本的总和(sum)等等。
    很多时候这些信息可以用于确定这个分布的未知参数，
    如果仅需要一个统计量就能确定这个分布的未知参数，而不再需要其它的额外"信息"，那么这个统计量就称为这个分布(或者分布族)
    的 **充分统计量(sufficient statistic)** 。
    在估计分布参数时，只需要保留这个充分统计量，而不再需要样本本身。





在极大似然估计中，我们尝试寻找一个最优的 :math:`\theta` 值，
而在贝叶斯估计中，我们并不是求解出参数变量 :math:`\theta` 具体的取值。
而是利用训练数据计算出参数变量 :math:`\theta` 的后验概率分布(posterior distribution)。
我们继续用符号 :math:`\mathcal{D}=\{x^{(1)},x^{(2)},\dots,x^{(N)}\}` 表示观测数据集，
符号 :math:`x^{(1)}` 的上标数字代表观测数据集(训练数据集)的样本编号，
数据集的大小是 :math:`|\mathcal{D}|=N` ，
参数变量 :math:`\theta` 的后验概率分布可以表示为：

.. math::

    p(\theta|\mathcal{D})=p(\theta|x^{(1)},x^{(2)},\dots,x^{(N)})


后验概率分布可以根据贝叶斯定理求得：



.. math::
    :label: 20_bayes

    p(\theta|\mathcal{D})
    =\frac{p'(\theta) p(\mathcal{D}|\theta)}{p(\mathcal{D})}


我们先看分子部分，其中 :math:`p(\mathcal{D}|\theta)`
就是观测数据集的似然函数， :math:`p'(\theta)` 是参数变量 :math:`\theta` 的先验概率分布。
先验分布的选择，会影响着计算后验分布的复杂程度，一个不合适的先验分布，甚至导致后验分布无法进行积分计算。
选取共轭先验可以令后验分布拥有与先验相同的形式，这使得计算得以简化。
上文我们知道，类别分布的似然函数具有多项式分布的形式，而多项分布的共轭先验是狄利克雷(Dirichlet)分布。
分母部分其实就是对分子的归一化，在 :math:`\mathcal{D}` 已经确定的情况下是个常数。


.. hint::
    后验概率分布正比于似然函数和先验的乘积，如果我们选择一个拥有合适函数形式的先验分布，
    使得两者乘积的结果(后验概率分布)拥有和先验分布相同的函数形式，这样的先验分布就叫做共轭先验，这个性质称为共轭性(conjugacy)。
    比如对于具有多项式分布形式的似然函数是 :math:`\theta_m` 的幂指函数 :eq:`20_mutil_likelihood` ，
    如果选择一个同样是关于  :math:`\theta_m` 的幂指函数的先验分布，则二者乘积后的后验概率分布也就具有同样函数形式。



我们令 :math:`n_m` 表示 :math:`x_m` 的样本在全部数据集中出现的次数，
则似然函数(未加对数log)为：


.. math::
    :label: 20_liklihood

    L(\mathcal{D};\theta) =p(\mathcal{D}|\theta)
    &= \prod_{i=1}^N \prod_{m=1}^M \theta_m^{\delta (x,x_m) }

    &=\prod_{m=1}^M \theta_m^{n_m}



- :math:`1= \sum_m \mathbb I \{x_m=1\}` ，因为在类别变量 :math:`X` 的一个采样中 :math:`[x_1,x_2,\ldots,x_M]` 只有一个元素为1，其它都是0 。






.. math::
    p(x_{new} = x_m )
    &=  \int \underbrace{p(\theta|\mathcal{D})}_{\text{后验概率分布}}
    \underbrace{p(x^{'} | \pmb{\mathrm{\theta}})}_{\text{类别分布}}   d \theta

    & =  \frac{\Gamma(\sum_m n_m + \alpha_m)}{\prod_m \Gamma(n_m+ \alpha_m)}
    \int \prod_{m=1}^M \theta_m^{n_m+\alpha_m -1 } \prod_{m=1}^{M} \theta_m^{\mathbb I \{x_m=1\} }   d \theta


    &=\frac{\Gamma(\sum_m n_m + \alpha_m)}{\prod_m \Gamma(n_m+ \alpha_m)}
    \int  \prod_{m=1}^M \theta_m^{n_m + \alpha_m - 1 }  \prod_{m=1}^{M} \theta_m^{\mathbb I \{x_m=1\} }  d \theta

    &=\frac{\Gamma(\sum_m n_m + \alpha_m)}{\prod_m \Gamma(n_m+ \alpha_m)}
    \int  \prod_{m=1}^M \theta_m^{n_m + \alpha_m + \mathbb I \{x_m=1\}  - 1 }  d \theta


    &= \frac{\Gamma(\sum_m n_m + \alpha_m)}{\prod_m \Gamma(n_m+ \alpha_m)}
    \frac{\prod_m \Gamma(n_m+ \alpha_m+\mathbb I \{x_m=1\} )}{\Gamma(\sum_m (n_m + \alpha_m+\mathbb I \{x_m=1\}))}



    &= \frac{\Gamma(\sum_m n_m + \alpha_m)}{\prod_m \Gamma(n_m+ \alpha_m)}
    \frac{(n_m+ \alpha_m) \prod_m \Gamma(n_m+ \alpha_m) }{\Gamma(  \sum_m (n_m + \alpha_m) + \sum_m \mathbb I \{x_m=1\} ))}


    &= \frac{(n_m+\alpha_m) \Gamma(\sum_m n_m + \alpha_m)}{\Gamma(\sum_m (n_m + \alpha_m)+ 1 ))}

    &= \frac{(n_m+\alpha_m) \Gamma(\sum_m n_m + \alpha_m)  }{\sum_m (n_m + \alpha_m) \Gamma(\sum_m n_m + \alpha_m)}

    &= \frac{(n_m+\alpha_m)   }{\sum_m (n_m + \alpha_m) }


    &= \frac{\alpha_m +n_m } {N + \sum_{m=1}^M \alpha_m  }














然而在GLM中，通常并不使用上述形式指数族，而是指数族的一个特殊子集，叫做自然指数族(Natural Exponential Family,NEF)，
满足条件 :math:`T(y)=y` 的指数族被称为自然指数族。


.. math::
    :label: eq_34_09

    p(y|\theta) = \exp \{\theta^T y - A(\theta) + S(y)\}



指数族的这个形式被称为自然形式(natural form)或者规范形式(canonical form)，其中参数 :math:`\theta`
称为自然参数(natural parameter)或者规范参数(canonical parameter)。
自然指数族相对于一般指数族的关键变化就是要求 :math:`T(y)=y` ，因为只有满足这个条件时
:math:`A(\theta)` 的一阶导数才等于 :math:`Y` 的期望 :math:`\mathbb{E}[Y]` 。
在GLM的定义中，我们并不直接使用 :eq:`eq_34_09` 形式，而是在这基础上引入一个额外的参数 :math:`\phi` 。


.. math::
    :label: eq_34_EDF

    p(y|\theta) = \exp \{\frac{\theta y - b(\theta)}{a(\phi)} + c(y,\phi)\}




