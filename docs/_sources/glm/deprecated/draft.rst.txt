
**线性回归**


在线性回归模型中，输出变量 :math:`Y` 的值域是实数值，并且假设其服从高斯分布。
此时输出变量 :math:`Y` 的边缘概率分布可以写成：

.. math::
    p(Y) \sim \mathcal{N}(\mu,\sigma^2)

其中 :math:`\mu` 是均值(期望)参数，表示变量 :math:`Y` 的期望 :math:`\mu=\mathbb{E}[Y]` ，
:math:`\sigma^2` 是方差参数，反映变量的波动性。

在回归问题中我们需要对条件概率建模 :math:`p(Y|X)` 进行建模，所谓的条件概率，
就是在变量 :math:`X` 取某个值时(条件下)变量 :math:`Y` 服从什么样的概率分布。
也就是变量 :math:`X` 取不同值时，变量 :math:`Y` 服从"不同的分布"，
这里的"不同分布"通常是不同参数的同种分布。

.. note::

    如果在 :math:`X` 取不同值时，变量 :math:`Y` 服从同一个分布(参数也相同)，
    那么说明变量 :math:`X` 和变量 :math:`Y` 是相互独立的，此时有 :math:`p(Y)=P(Y|X)` 。
    如果某个输入变量 :math:`X` 与输出变量 :math:`Y` 是相互独立的，
    那么这个输入变量就不能作为我们的特征变量，因为它对确定 :math:`Y` 没有任何贡献(影响)。
    所以无论是在分类问题还是回归问题中，作为特征的输入变量 :math:`X` 都不能独立于输出变量 :math:`Y` 。



线性回顾模型是通过在边缘概率分布 :math:`p(Y)` 的基础上"引入"变量 :math:`X` 进而得到条件概率分布 :math:`p(Y|X)` 。
我们令变量 :math:`Y` 的期望(均值)值 :math:`\mu` 等于变量 :math:`X` 的线性组合，即：

.. math::

    \mu = \beta^T x

也就是说在不同的 :math:`x` 值下，变量 :math:`Y` 的均值不同，进而我们就得到了条件概率分布 p(Y|X)：

.. math::

    p(Y|X)  \sim \mathcal{N}(\beta^T x,\sigma^2)


通过参数化 :math:`\mathbb{E}[Y]` 将变量 :math:`X` 引入到边缘概率分布 :math:`p(Y)` 进而得到条件概率分布 :math:`p(Y|X)`
。注意，在经典线性回归模型中，方差 :math:`\sigma^2` 被认为是已知的，并且是和均值独立的，
只有 :math:`\beta` 是需要学习的未知参数。


**逻辑回归**

现在我们回顾一下逻辑回归模型，在逻辑回归模型中，输出变量 :math:`Y` 是伯努利变量，服从伯努利分布：

.. math::

    p(y) = \mu^y(1-\mu)^{1-y}



在逻辑回归模型中，我们假设均值参数 :math:`\mu` 是一个关于线性组合的 :math:`\beta^Tx` 的sigmoid函数：

.. math::

    \mu=\mu(x)=\frac{1}{1+e^{-\beta^T x}}


条件概率分布 :math:`p(Y|X)` 就是：

.. math::

    p(y|x) =\mu(x)^y(1-\mu(x))^{1-y}



我们发现线性回归模型和逻辑回归模型都是通过参数化 :math:`Y` 的期望 :math:`\mu=\mathbb{E}[Y]` ，
进而得到条件概率分布 :math:`p(Y|X)` ，事实上，我们将这种方式进行扩展，
就得到了广义线性模型(generalized linear model,GLM)。















要想定义条件概率分布 :math:`P(Y|X)` 的概率分布函数，
就需要把输入变量 :math:`X` 和响应变量 :math:`Y`
"连接"在一起。





显然我们可以令 :math:`Y` 的概率分布函数和 :math:`\eta` 相关。

我们知道 :math:`Y` 是服从指数族分布的，指数族分布的概率分布函数都可以写成如下形式：

.. math::

        p(y|\theta) = \exp \{\frac{\theta y - b(\theta)}{a(\phi)} + c(y,\phi)\}





假设 :math:`Y` 的概率分布函数是 :math:`p(Y)=f(y,\theta)`
，
.. math::





