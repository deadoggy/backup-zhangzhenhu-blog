###################################################################
DreamBooth论文翻译
###################################################################

大型文本-图像模型在人工智能的发展中实现了一个显著的飞跃，能够从给定的文本提示中合成高质量和多样化的图像。然而，这些模型缺乏模仿给定参考集中的主体外观的能力，也缺乏在不同背景下对其进行合成的新的渲染。在这项工作中，我们提出了一种 "个性化 "的文本-图像融合模型的新方法。考虑到只有几个子对象的图像作为输入，我们对预训练的文本-图像模型进行了微调，使其学会将一个独特的标识符与该特定的子对象结合起来。一旦主体被嵌入到模型的输出域中，唯一的标识符就可以被用来合成不同场景中的主体的新的逼真图像。通过利用嵌入在模型中的语义先验和新的自体类先验的保存损失，我们的技术可以在不同的场景、姿势、视角和照明条件下合成参考图像中没有出现的主体。我们将我们的技术应用于几个以前无法完成的任务，包括主体重构、文本引导的视图合成和艺术渲染，同时保留了子对象的关键特征。
我们还为这一主题驱动生成的新任务提供了新的数据集和评估协议。项目页面：https://dreambooth.github.io/

为了评估这项新任务，我们还构建了一个新的数据集，其中包含在不同背景下拍摄的各种主体，
并提出了一个新的评估协议，以衡量主体的保真度和生成结果的提示保真度。
我们在项目网页上公开提供了我们的数据集和评估协议。

2. Related work
###################################################################


**图像合成** 图像合成技术[13, 38, 70]旨在将一个给定的主体克隆到一个新的背景中，
使主体与场景融为一体。为了考虑新姿势的合成，
人们可以应用三维重建技术[6, 8, 41, 49, 68]，该技术通常适用于刚性物体，需要更多的视图。
一些缺点包括场景整合（照明、阴影、接触）和无法生成新的场景。相比之下，我们的方法能够生成新姿势和新背景下的主体。


**文本到图像的编辑和合成**。
文本驱动的图像操作最近取得了重大进展，使用GANs[9,22,28-30]结合图像-文本还原，如CLIP[52]，
产生了使用文本的现实操作[2, 7, 21, 43, 48, 71]。
这些方法在结构化的场景中工作得很好（例如人脸编辑），但在不同的数据集上可能会有困难，
因为其中的子对象是不同的。Crowson等人[14]使用VQ-GAN[18]并在更多不同的数据上进行训练以缓解这一问题。
其他作品[4, 31]利用最近的扩散模型[25, 25, 45, 58, 60, 62-66]，
在高度多样化的数据集上实现了最先进的生成质量，经常超过GANs[15]。
虽然大多数只需要文本的工作仅限于全局编辑[14, 33]，
但Bar-Tal等人[5]提出了一种不使用掩码的基于文本的本地化编辑技术，
显示了令人印象深刻的结果。虽然这些编辑方法大多允许修改全局属性或对给定的图像进行局部编辑，
但没有一个能在新的背景下对给定的主题产生新的演绎效果。

也有关于文本到图像合成的工作[14, 16, 19, 24, 27, 35, 36, 50, 51, 55, 58, 67, 74] 。最近的大型文本-图像模型，如Imagen[61]、DALL-E2[54]、Parti[72]、CogView2[17]和Stable Diffusion[58]展示了前所未有的语义生成能力。这些模型没有对生成的图像进行精细的控制，只使用文本指导。
具体来说，在不同的合成图像中一致地保留主体的身份是具有挑战性的，甚至是不可能的。


**可控生成模型**。有各种控制生成模型的方法，其中一些可能被证明是主题驱动的提示引导的图像合成的可行方向。
Liu等人[39]提出了一种基于扩散的技术，允许在参考图像或文本指导下进行图像变化。
为了克服主体修改，一些作品[3, 44]假设用户提供的掩码来限制修改区域。反转[12, 15, 54]可以用来保留一个主题，同时修改背景。
Prompt-to-prompt[23]允许在没有输入掩码的情况下进行局部和整体编辑。这些方法都没有达到保留身份的新颖样本生成主体的目的。


在GAN的背景下，Pivotal Tuning[57]通过用倒置的潜伏代码锚对模型进行微调，可以进行真实的图像编辑，Nitzan等人[46]将这项工作扩展到对人脸进行GAN微调，以训练个性化的先验，这需要大约100张图像，并且仅限于人脸领域。
Casanova等人[11]提出了一个可以生成实例变化的实例CON-ditioned GAN，尽管它在处理独特的主题时可能会遇到困难，并且不能保留所有的主题细节。

最后，Gal等人[20]的同期工作提出了一种方法，通过冻结的文本到图像模型的嵌入空间中的新标记来表示视觉概念，如一个物体或一种风格，从而产生小的个性化标记嵌入。
虽然这种方法受到冻结的扩散模型的表达能力的限制，但我们的微调方法使我们能够在模型的输出域中嵌入主题，从而产生保留其关键视觉特征的主题的新图像。


3. Method
####################################

鉴于只有几张（通常是3-5张）随手拍摄的特定主题的图像，没有任何文字描述，
我们的目标是生成具有高细节保真度的主题的新图像，
并在文字提示的指导下进行变化。变化的例子包括改变主体的位置，
改变主体的属性，如颜色或形状，修改主体的姿势、视角，以及其他语义上的修改。
我们对输入的图像采集设置没有任何限制，而且主体图像可以有不同的背景。
接下来我们提供一些关于文本到图像扩散模型的背景（第3.1节），
然后介绍我们的微调技术，将一个独特的标识符与几幅图像中描述的主体结合起来（第3.2节），
最后提出一个特定类别的先验保留损失，使我们能够在微调模型中克服语言漂移（第3.3节）。


3.2. Personalization of Text-to-Image Models
=====================================================================

我们的首要任务是将主体实例植入模型的输出域中，这样我们就可以查询模型，以获得该主体的各种新图像。
一个自然的想法是利用被摄对象的少数照片数据集对模型进行微调。
在几张照片的情况下微调生成模型（如GANs）时，必须小心谨慎，
因为它可能导致过度拟合和模式崩溃，以及不能充分捕捉目标分布。
已经有关于避免这些缺陷的技术研究[37, 42, 47, 56, 69]，尽管与我们的工作相反，
这类工作主要是为了生成与目标分布相似的图像，但没有对主体保留的要求。
关于这些缺陷，我们观察到一个奇特的发现，即鉴于使用公式1中的扩散损失进行了仔细的微调设置，
大型文本到图像的扩散模型似乎擅长将新的信息整合到它们的领域中，而不会忘记先验或对一小部分训练图像过度拟合。


**Designing Prompts for Few-Shot Personalization**

为少量的个性化设计提示 我们的目标是将一个新的（唯一标识符，主题）对 "植入 "扩散模型的 "字典 "中。
为了绕过为给定图像集编写详细图像描述的开销，我们选择了一种更简单的方法，
将所有输入的主题图像标记为 "一个[标识符][类名词]"，其中[标识符]是与主题相关的唯一标识符，
[类名词]是主题的粗略类描述符（例如，猫、狗、手表等）。类描述符可以由用户提供或通过分类器获得。
我们在句子中使用类描述符，以便将类的先验与我们独特的主题联系起来，并发现使用错误的类描述符或没有类描述符会增加训练时间和语言漂移，同时降低性能。
从本质上讲，
我们试图利用模型对特定类别的先验，并将其与我们主体的独特标识符的嵌入联系起来，这样我们就可以利用视觉先验来生成主体在不同语境中的新姿势和表述。


**Rare-token Identifiers**

稀有标识符 我们通常发现现有的英语单词（如 "unique"、"special"）是次优的，
因为模型必须学习将它们与原来的含义分开，并重新将它们与我们的主题相联系。
这就促使我们需要一个在语言模型和扩散模型中都具有弱先验的标识符。
一个危险的方法是选择英语中的随机字符，并将它们连接起来，生成一个罕见的标识符（例如 "xy5syt00"）。
在现实中，标记器可能会对每个字母分别进行标记，而扩散模型的先验对这些字母是很强的。
我们经常发现，这些标记会招致与使用普通英语单词类似的弱点。
我们的方法是在词汇中寻找罕见的令牌，然后将这些令牌反转到文本空间，以使标识符具有强先验的概率最小。
我们在词汇中进行稀有标记的查找，并获得稀有标记标识符f（Vˆ）的序列，其中f是一个标记器；
一个将字符序列映射到标记的函数，Vˆ是源自标记f（Vˆ）的解码文本。
序列的长度可以是可变的k，并发现相对较短的k={1，...，3}的序列效果很好。
然后，通过使用f (Vˆ)上的去标记器反转词汇，我们得到一个字符序列，定义我们的唯一标识符Vˆ。
对于Imagen，
我们发现使用对应于3个或更少的Unicode字符（不含空格）的统一随机抽样，并使用T5-XXL标记器范围内的标记{5000, ..., 10000}效果很好。

3.3. Class-specific Prior Preservation Loss
==========================================================

根据我们的经验，通过对模型的所有层进行微调，可以获得最大的主体保真度的最佳结果。这包括对以文本嵌入为条件的层进行微调，这就产生了语言漂移的问题。语言漂移是语言模型中观察到的一个问题[34, 40]，在这个问题上，一个在大型文本语料库上预训练的模型，后来为一个特定的任务进行了微调，逐渐失去了语言的句法和语义知识。
**据我们所知，我们是第一个发现影响扩散模型的类似现象，即模型慢慢忘记了如何生成与目标主体相同类别的主体。**

**另一个问题是输出多样性减少的可能性**。文本到图像的扩散模型自然拥有大量的输出多样性。
在对一小部分图像进行微调时，我们希望能够以新的视角、姿势和衔接方式生成主体。然而，有一种风险是减少输出姿势和主体视图的变化量（例如，抢拍到少数的视图）。
我们观察到，这种情况经常发生，特别是当模型训练时间过长时。


为了缓解上述两个问题，我们提出了一个自生类的先验保留损失，以鼓励多样性并对抗语言漂移。
从本质上讲，我们的方法是用它自己产生的样本来监督模型，以使它在少数次微调开始时保留先验。
这使得它能够生成不同的类先验图像，并保留关于类先验的知识，它可以结合关于主题实例的知识来使用。
具体来说，我们使用祖先采样器对冻结的预训练扩散模型生成数据xpr = xˆ(zt1 , cpr)，
随机初始噪声zt1∼N(0,I)，条件向量cpr := Γ(f ("a [class noun]")。损失变为：



4. Experiments
##########################################

在本节中，我们展示了实验和应用。我们的方法能够对我们的主题实例进行广泛的文本指导的语义修改，包括重新定义，修改主题属性，如材料和物种，艺术演绎和观点修改。
**重要的是，在所有这些修改中，我们都能够保留赋予主体身份和本质的独特视觉特征。**
**如果任务是重新语境化，那么主体特征就不会被修改，但外观（如姿势）可能会改变。**
如果任务是更强的语义修改，例如在我们的主体和另一个物种/物体之间的交叉，那么主体的关键特征在修改后会被保留下来。
在本节中，我们使用[V]来引用主体的唯一标识符。我们在补充材料中包括具体的Imagen和稳定扩散的实施细节。