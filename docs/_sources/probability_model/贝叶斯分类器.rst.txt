########################################################
贝叶斯分类器
########################################################

在之前的章节中，主要讨论概率图的基础知识，包括概率图的表示、推断和学习三个基本问题。
本章开始，我们讨论基于概率图的应用模型，虽然这些应用模型未必都是基于概率图理论提出的，但是都是可以用概率图去解释的。
通过概率图理论，可以为这些模型构建一套完整的理论体系，更有利于这些模型的学习和研究。
我们首先从最简单的模型开始，相信每一个机器学习算法工程师在入门时都会学习到逻辑回归、线性回归等入门算法。
实际上，这些算法都可以纳入到概率图模型的框架下，借助概率图工具可以更好的理解算法。


在很多机器学习算法中，比如分类、回归、聚类等相关算法，会把变量分成两类：

- 输入变量，也叫特征变量、自变量，一般用符号 :math:`X` 表示。
- 输出变量，也叫标签变量、隐变量，一般用符号 :math:`Y` 表示。

多数情况下，输入变量 :math:`X` 有多个，输出变量 :math:`Y` 只有一个，
然而在贝叶斯网中允许 :math:`Y` 存在多个的情景，在本章最后一节再讨论这种情况，
这里先默认 :math:`Y` 是单变量。
逻辑回归、线性回归、K-means等常见的分类、回归、聚类算法，
都可以看做是对条件概率分布 :math:`P(Y|X)` 进行建模，
模型输入 :math:`X` ，输出 :math:`Y` 的期望值或者最大概率值。
不同的算法模型就是对条件概率分布 :math:`P(Y|X)` 不同的建模方法，
这里最容易想到的方法就是利用贝叶斯定理，


.. math::
    :label: eq_bc_002

    P(Y|X) = \frac{P(X,Y)}{P(X)} = \frac{P(Y)P(X|Y)}{P(X)}


条件概率 :math:`P(Y|X)` 可以通过联合概率 :math:`P(X,Y)` 推断得到，
我们用贝叶斯网络（概率有向图）来表示联合概率 :math:`P(X,Y)` ，其图形结构如
:numref:`fig_bc_002` 所示。
注意其中边的方向是从 :math:`Y` 指向 :math:`X` 的，
这是因为对 :math:`P(X,Y)` 的分解是 :math:`P(Y)P(X|Y)`
。

.. _fig_bc_002:

.. digraph:: 一个树形概率图模型
    :align: center
    :caption: 贝叶斯分类

    node[shape=circle]

    graph[labelloc="b", color=none];

    x1[label=<X<SUB>1</SUB>>]
    x2[label=<X<SUB>2</SUB>>]
    x3[label=<X<SUB>3</SUB>>]
    xh[label="..."  color=none ]
    xm[label=<X<SUB>m</SUB>>]
    y[label=Y]

    y -> xh[ style=invisible dir=none ]
    y -> {x1 x2 x3 xm}



:numref:`fig_bc_002` 所表示的结构忽略了特征变量 :math:`X` 之间的关系，
实际上特征变量 :math:`X` 之间很可能会存在某些关系，
可以根据特征变量 :math:`X` 之间是否满足条件独立分成三种情况：

- 特征变量之间 *完全* 条件独立，如 :numref:`fig_bc_003` (a) 所示。
- 特征变量之间 *部分* 条件独立，如 :numref:`fig_bc_003` (b) 所示。
- 特征变量之间 *完全不* 独立，如 :numref:`fig_bc_003` (c) 所示。


.. _fig_bc_003:

.. digraph:: 一个树形概率图模型
    :align: center
    :caption: （a）特征变量完全条件独立；（b）特征变量部分条件独立；（c）特征变量完全不独立。

    node[shape=circle]

    subgraph cluster_1 {
        graph[labelloc="b", color=none ,label="(a)"];

        a_x1[label=<X<SUB>1</SUB>>]
        a_x2[label=<X<SUB>2</SUB>>]
        a_x3[label=<X<SUB>3</SUB>>]
        a_xh[label="..."  color=none ]

        a_xn[label=<X<SUB>m</SUB>>]
	    a_y[label=Y]

	    a_y -> a_xh[ style=invisible dir=none]
	    a_y -> {a_x1 a_x2 a_x3 a_xn}

    }

    subgraph cluster_2 {
        graph[labelloc="b", color=none ,label="(b)"];

        b_x1[label=<X<SUB>1</SUB>>]
        b_x2[label=<X<SUB>2</SUB>>]
        b_x3[label=<X<SUB>3</SUB>>]
        b_xh[label="..."  color=none ]

        b_xn[label=<X<SUB>m</SUB>>]
	    b_y[label=Y]

	    b_y -> b_xh[ style=invisible dir=none]
	    b_y -> {b_x1 b_x2 b_x3 b_xn}

	    b_x1-> b_x2

	    {rank="same"; b_x1 b_x2 b_x3 b_xh b_xn}
    }

    subgraph cluster_3 {
        graph[labelloc="b", color=none ,label="(c)"];

        c_x[label=X]
	    c_y[label=Y]

	    c_y -> c_x
    }



特征变量 :math:`X` 的条件独立性影响着类别条件概率 :math:`P(X|Y)` 的因子分解方式，
如果特征变量 :math:`X` 之间是完全条件独立的，则类别条件概率 :math:`P(X|Y)`
可以分解成一个简单模式，

.. math::

    P(X|Y) = \prod_{i=1}^m P(X_i|Y)


相反，如果特征变量 :math:`X` 之间不是完全独立的，则类别条件概率 :math:`P(X|Y)`
的因子分解式是一个相对比较复杂的形式，

.. math::

    P(X|Y) = \prod_{i=1}^m P(X_i|Y,Pa(X_i))


其中 :math:`Pa(X_i)` 表示 :math:`X_i` 的父结点集合（不包括 :math:`Y`）。
这个形式无论是空间复杂度还是时间复杂度都将是指数级的。





朴素贝叶斯模型
########################################################

朴素贝叶斯模型(naive Bayes classifier)是贝叶斯分类器的一个特例，是众多贝叶斯分类器中最简单的一个。
**"朴素"两个字特指特征变量** :math:`X` **之间满足完全条件独立性**，
这个假设极大的简化了模型的复杂度，因此称为朴素（naive）。
除此之外，朴素贝叶斯模型还假设所有特征变量都是离散变量，
如果实际应用场景中存在连续值特征，可以先对其进行分段离散化，再应用朴素贝叶斯模型。

模型表示
=================================================


.. _fig_bc_012:

.. digraph:: fig_bc_012
    :align: center
    :caption: 朴素贝叶斯模型的有向图结构

    node[shape=circle]

    graph[labelloc="b", color=none];

    x1[label=<X<SUB>1</SUB>>]
    x2[label=<X<SUB>2</SUB>>]
    x3[label=<X<SUB>3</SUB>>]
    xh[label="..."  color=none ]
    xn[label=<X<SUB>m</SUB>>]
    y[label=Y]

    y -> xh[ style=invisible dir=none ]
    y -> {x1 x2 x3 xn}


朴素贝叶斯模型的有向图形式如 :numref:`fig_bc_012` 所示，
根据条件独立性假设，条件概率 :math:`P(X|Y)`
可以分解成

.. math::

    P(X|Y) = P(X_1,X_2,\cdots,X_m|Y)
    = \prod_{j=1}^m P(X_j|Y)



根据贝叶斯定理，后验概率 :math:`P(Y|X)`
可以分解成

.. math::
    :label: eq_bc_013

    P(Y|X) &= \frac{P(X,Y)}{P(X)}

    &= \frac{P(Y)P(X|Y)}{P(X)}

    &= \frac{P(Y) \prod_{j=1}^m P(X_j|Y)}{ P(X) }



我们用符号 :math:`\theta_y` 表示 :math:`P(Y)` 的参数，
:math:`\theta_{xj}` 表示 :math:`P(X_j|Y)` 的参数，
并且令 :math:`\theta=\{\theta_y,\theta_{x1},\theta_{x2},\cdots,,\theta_{xm} \}`
，:eq:`eq_bc_013` 的参数化表示为

.. math::
    :label: eq_bc_014

    P(Y|X;\theta) = \frac{P(Y;\theta_y) \prod_{j=1}^m P(X_j|Y;\theta_{xj})}{ P(X) }


其中分母部分 :math:`P(X)` 是分子的归一化系数，可以通过对分子进行边际化得到，
它是一个常量值。
:eq:`eq_bc_014` 就是朴素贝叶斯模型的参数化表示，
它表示在已知 :math:`X` 的条件下，:math:`Y` 的条件概率分布，
核心理论就是贝叶斯定理。




参数估计
=================================================

现在看下如何利用最大似然估计，估计出朴素贝叶斯模型的参数 :math:`\theta`。
假设观测样本集为 :math:`\mathcal{D}=\{(x_1,y_1),\cdots,(x_N,y_N)\}`
，共有 :math:`N` 条观测样，
朴素贝叶斯模型的对数似然函数为


.. math::
    :label: eq_bc_015

    \ell (\theta;\mathcal{D}) &= \ln \prod_{i=1}^N  p(y_i|x_i;\theta)


    &= \ln \prod_{i=1}^N \left [ \frac{p(y_i;\theta_y)p(x_i|y_i;\theta_x)}{p(x_i)}  \right ]

    &= \sum_{i=1}^N \ln \left [ \frac{p(y_i;\theta_y)p(x_i|y_i;\theta_x)}{p(x_i)}  \right ]

    &= \sum_{i=1}^N  \left [ \ln p(y_i;\theta_y) + \ln p(x_i|y_i;\theta_x) - \ln p(x_i)  \right ]

    &= \sum_{i=1}^N  \left [ \ln p(y_i;\theta_y) + \ln \prod_{j=1}^m p(x_{ij}|y_i;\theta_{xj}) - \ln p(x_i)  \right ]

    &= \sum_{i=1}^N  \left [ \ln p(y_i;\theta_y) + \sum_{j=1}^m \ln  p(x_{ij}|y_i;\theta_{xj}) - \ln p(x_i)  \right ]

    &= \sum_{i=1}^N  \ln p(y_i;\theta_y) +  \sum_{i=1}^N \sum_{j=1}^m \ln p(x_{ij}|y_i;\theta_{xj}) - \sum_{i=1}^N \ln p(x_i)


可以看到对数似然函数分成三部分，最后一部分 :math:`-\sum_{i=1}^N \ln p(x_i)`
是一个常量，对于极大化对数似然函数没有影响，可以忽略。
第一项和第二项分别对应着先验概率 :math:`P(Y;\theta_y)`
和似然部分 :math:`P(X|Y;\theta_x)`，
两部分的参数是互不相关的，是可以分别极大化求解的。


极大化第一部分得到 :math:`\theta_y` 的似然估计值，

.. math::
    :label: eq_bc_016

    \hat{\theta}_y = \mathop{\arg \max}_{\theta_y} \sum_{i=1}^N  \ln p(y_i;\theta_y)

无论标签变量 :math:`Y` 是伯努利变量（二分类）还是类别变量（多分类），
都可以直接使用观测样本集中 :math:`Y` 的观测数据进行参数估计。
伯努利变量和类别变量最大似然估计的过程在前面的章节中已经讨论过，
这里不再赘述。


极大化第二部分得到 :math:`\hat{\theta}_x=\{\theta_{x1},\theta_{x2},\cdots,\theta_{xm}\}` 的似然估计值，

.. math::
    :label: eq_bc_017

    \hat{\theta}_x = \mathop{\arg \max}_{\theta_x}  \sum_{i=1}^N \sum_{j=1}^m \ln p(x_{ij}|y_i;\theta_{xj})

观察下 :eq:`eq_bc_017`，
由于特征变量 :math:`X` 之间的条件独立性，每一项局部因子 :math:`p(x_{ij}|y_i;\theta_{xj})`
都是独立的，每一项都是可以独立进行极大化求解的。

.. math::
    :label: eq_bc_018

    \hat{\theta}_{xj} = \mathop{\arg \max}_{\theta_{xj}}  \sum_{i=1}^N  \ln p(x_{ij}|y_i;\theta_{xj})


:eq:`eq_bc_018` 和 :eq:`eq_bc_016` 的一个区别就是，:math:`p(x_{ij}|y_i;\theta_{xj})`
是一个条件概率，这里需要理解"条件"的含义，
直观的理解就是在 :math:`Y` 取某个值的条件下 :math:`X` 的概率分布。
好在 :math:`Y` 是离散变量，比较容易处理。
可以把样本集 :math:`\mathcal{D}` 按照 :math:`Y` 的值进行划分，
:math:`\mathcal{D}=\{\mathcal{D}(y=0),\mathcal{D}(y=1),\cdots,\mathcal{D}(y=k)\}`
，在每一份样本子集下分别估计 :math:`p(x_{j}|\mathcal{D}(y=k);\theta_{xj})`。
特征变量 :math:`X_j` 可以是伯努利变量也可以是类别变量，同样它的参数估计过程不需要再赘述，




到这里应该已经明白，朴素贝叶斯模型中的条件独立性假设，使得每一项 :math:`p(x_{j}|y;\theta_{xj})`
都是独立的，其参数可以分别独立的进行估计，这极大的简化了参数估计的复杂度。





高斯判别模型
########################################################

上一节讨论的朴素贝叶斯模型中，假设所有所有特征变量都是离散变量，
然而实际应用场景是复杂多变的，很多时候是无法满足这样的强假设的。
如果特征变量 :math:`X_j` 是连续值怎么办？当然，
我们可以通过离散化的手段把连续值转换成离散变量后，
再应用朴素贝叶斯模型进行处理。
但这样做显然不够优雅，事实上，
贝叶斯分类器并没有约束变量服从何种概率分布，理论上任何概率分布都是支持的，
无论是标签变量 :math:`Y` ，还是特征变量 :math:`X` 都可以是任意类型的分布。
本节我们讨论，特征变量 :math:`X` 是高斯分布，标签变量 :math:`Y` 仍然是离散（类别）变量的分类模型，
这类模型有个名字，称之为高斯判别分析（Gaussian Discriminant Analysis,GDA)。

高斯判别模型和朴素贝叶斯模型的图结构是一样的，并没有区别，仅仅是对 :math:`P(X|Y)` 假设不一样。
原始的高斯判别模型假设 :math:`P(X|Y)=P(X_1,X_2,\cdots,X_m|Y)` 是一个多元高斯分布，
这里我们先讨论独立的一元高斯分布的情况，然后再讨论如何用多元高斯分布对 :math:`P(X|Y)` 建模。



一元高斯模型
======================================================

当我们把 :math:`P(X|Y)=P(X_1,X_2,\cdots,X_m|Y)` 看做完全条件独立的高斯模型时，高斯判别模型的对数似然函数和朴素贝叶斯模型没有任何差别，
其形式也是 :eq:`eq_bc_015`，不同的地方于 :math:`P(X_j|Y)` 是一个高斯分布。

假设 :math:`Y` 是伯努利变量，
当 :math:`Y=0` 是，服从高斯分布的特征变量 :math:`X_j` 的概率密度函数为

.. math::
    :label: eq_bc_020

    P(X_j|Y=0;\theta_j) = \frac{1}{ (2\pi\sigma_{0j}^2)^{1/2} } \exp \left \{
    - \frac{1}{2\sigma_{0j}^2}(x_j-\mu_{0j})^2 \right \}

其中 :math:`\mu_{0j},\sigma_{0j}^2` 分别表示当类别为 :math:`Y=0` 时，
第 :math:`j` 个特征变量 :math:`X_j` 的高斯分布的均值参数和方差参数。
同理，当类别 :math:`Y=1` 时，有

.. math::
    :label: eq_bc_021

    P(X_j|Y=1;\theta_j) = \frac{1}{ (2\pi\sigma_{1j}^2)^{1/2} } \exp \left \{
    - \frac{1}{2\sigma_{1j}^2}(x_j-\mu_{1j})^2 \right \}

类似于朴素贝叶斯模型的参数估计过程，可以把观测样本集按照 :math:`Y` 的值划分成两份，
用每一份数据分别去估计上述两个高斯分布的参数即可，高斯分布的参数估计过程可以回顾 :numref:`ch_estimate`
，这里不再赘述。

这需要注意的是，在高斯判别模型的理论中，
通常假设同一个特征变量 :math:`X_j` ，:math:`Y=0` 与 :math:`Y=1` 条件下的高斯分布拥有相同的方差参数
，即假设 :math:`\sigma_{0j}^2=\sigma_{1j}^2`，
此时可以用全部样本集估计出一个方差参数即可 :math:`\sigma_j^2`，
这个假设被称为同方差假设，同方差假设使得模型参数变少，模型变得简单，但会限制模型的表达能力，
如果你的应用场景下数据并不是同方差的，可以不使用这个假设，按照异方差建模。





多元高斯模型
======================================================


多元高斯分布是对多个高斯变量组成的联合概率分布进行建模，假设有 :math:`m` 个高斯变量
:math:`X=\{X_1,X_2,\cdots,X_m\}`
，他们的联合概率分布可以写为

.. math::
    :label: eq_bc_022

    P(X_1,X_2,\cdots,X_m) = \frac{1}{(2\pi)^{1/2} {|\Sigma|}^{1/2}  }
    \exp \left \{ -\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu) \right \}

其中 :math:`x` 是一个长度为 :math:`m` 的向量，代表 :math:`m` 个高斯随机变量，
:math:`\mu` 是均值向量参数，:math:`\Sigma` 是协方差矩阵。
协方差矩阵是一个对称矩阵，
对角线元素 :math:`\Sigma_{ii}` 表示变量 :math:`X_i` 的方差，
非对角线元素 :math:`\Sigma_{ij}=\Sigma_{ji}` 表示变量 :math:`X_i`
和变量 :math:`X_j` 的相关性，有关协方差更详细的解释请参考其它专业资料。


.. note::
    在统计学与概率论中，协方差矩阵（也称离差矩阵、方差-协方差矩阵）是一个矩阵，
    其 :math:`i,j` 位置的元素是第 :math:`i` 个与第 :math:`j` 个随机变量之间的协方差。
    协方差描述的是两个变量之间的相关性，如果两个变量是正相关，协方差大于0；如果两个变量是负相关，协方差小于0；
    如果两个变量不相关，协方差为0。协方差矩阵对角线的元素 :math:`\Sigma[ii]` 表示的是变量 :math:`X_i` 的方差，即
    :math:`\Sigma[ii]=\sigma_i^2` 。


在贝叶斯分类器中，如果特征变量中全部是高斯变量，
可以使用多元高斯对类别条件概率 :math:`P(X|Y)`
进行建模，对于标签变量 :math:`Y`
的每一个取值 :math:`k` 有


.. math::
    :label: eq_bc_023

    p(x|y=k;\theta)=\frac{1}{(2\pi)^{1/2} {|\Sigma|}^{1/2}  }
    \exp \left \{ -\frac{1}{2}(x-\mu_k)^T \Sigma^{-1} (x-\mu_k) \right \}
    ,\quad k=0,1

在高斯判别模型中，通常假设每个类别下的多元高斯分布有不同的均值向量参数，
**而协方差参数是相同的**，
并且假设特征变量之间关于变量 :math:`Y` 条件独立的，所以协方差矩阵 :math:`\Sigma`
应该是一个对角矩阵 :math:`\Sigma = diag(\sigma^2_1,\sigma^2_2,\dots,\sigma^2_m)`
，对角线的元素是每个变量的方差，其它元素都是0。



现在讨论下特征变量间方差的关系，如 :numref:`fg_bc_210` 所示，
假设只有两个特征 :math:`X_1` 和 :math:`X_2` ，
每幅图中右上角的同心圆表示类别 :math:`Y=1` 的条件下的高斯分布的轮廓图，
左下角的同心圆表示类别 :math:`Y=0` 的条件下的高斯分布的轮廓图，
注意两个类别条件高斯分布有相同的方差参数，所以同一幅图中两组同心圆形状是相同的。
我们用 :math:`\sigma_1` 表示高斯分布 :math:`p(x_1|y;\sigma_1)` 的方差，
:math:`\sigma_2` 表示高斯分布 :math:`p(x_2|y;\sigma_2)` 的方差。
当 :math:`\sigma_1=\sigma_2` 时，我们看到是(a)图，同心圆是正圆，
当 :math:`\sigma_1 \ne \sigma_2` 时，我们看到是(b)图，同心圆是椭圆。


.. _fg_bc_210:

.. figure:: pictures/32_8.jpg
    :scale: 40 %
    :align: center

    (a)方差为 :math:`\sigma_1=1,\sigma_2=1` 的高斯类别条件密度的轮廓图；
    (b)方差为 :math:`\sigma_1=0.5,\sigma_2=2.0` 的高斯类别条件密度的轮廓图；





逻辑回归
########################################################

相信大部分读者在入门时都有学过逻辑回归模型（logistic model），
本节我们探讨下朴素贝叶斯模型、高斯判别模型和逻辑回归模型的关系，
最后会发现一些很有意思的结论。


假设类别变量 :math:`Y` 是伯努利变量（二分类问题），
其概率质量函数定义为

.. math::
    :label: eq_bc_141

    P(Y;\theta_y) = \lambda^y (1- \lambda)^{1-y}



根据上一节的讨论，在 :math:`X=x` 的条件下 :math:`Y=1` 的概率为

.. math::
    :label: eq_bc_142

    &p(Y=1|X=x;\theta)

    &= \frac{p(Y=1;\theta_y) p(x|Y=1;\theta_x)}
    {p(Y=1;\theta_y) p(x|Y=1;\theta_y) +p(Y=0;\theta_y)p(x|Y=0;\theta_x)  }

    &= \frac{1}{1 + \frac{p(Y=0;\theta_y) p(x|Y=0;\theta_x)  }{ p(Y=1;\theta_y)p(x|Y=1;\theta_x)}  }

    &= \frac{1}{1 + \exp \left \{ \ln \left [
    \frac{p(Y=0;\theta_y) p(x|Y=0;\theta_x)  }{ p(Y=1;\theta_y)p(x|Y=1;\theta_x)}
    \right ] \right \}
     }

    &= \frac{1}{1 + \exp \left [
    \ln p(Y=0;\theta_y)
    - \ln p(Y=1;\theta_y)
    + \ln   p(x|Y=0;\theta_x)
    - \ln  p(x|Y=1;\theta_x)
    \right ]
     }

    &= \frac{1}{1 + \exp \left [
    \ln p(Y=0;\theta_y)
    - \ln p(Y=1;\theta_y)
    + \ln   p(x|Y=0;\theta_x)
    - \ln  p(x|Y=1;\theta_x)
    \right ]
    }


朴素贝叶斯模型和高斯判别模型都可以纳入到贝叶斯分类器这个框架下，
在二分类的场景下，都可以抽象成 :eq:`eq_bc_142` 的形式，
差别只在于类别条件概率分布 :math:`P(X|Y;\theta_x)`
的形式不同，朴素贝叶斯模型假设特征变量 :math:`X` 是离散类别分布，
高斯判别模型假设特征变量 :math:`X` 高斯分布。
下一步，我们分别看下朴素贝叶斯模型、高斯判别模型和逻辑回归模型的关系是怎么样的。


**朴素贝叶斯模型**



假设每一个特征变量 :math:`X_j` 都是伯努利变量，其概率质量函数定义为

.. math::
    :label: eq_bc_143

    P(X_j|Y=k) = \pi_{kj}^{x_j} (1- \pi_{kj})^{1-x_j}


.. hint::

    如果实际应用场景中，遇到特征变量 :math:`X_j` 是类别分布（多值离散变量），假设其有 :math:`K` 个可能值，
    可以把它转换成 :math:`K` 个伯努利特征变量，也就是常说的独热编码（One-hot encoding），
    不影响这里的结论。



把 :eq:`eq_bc_143` 代入到 :eq:`eq_bc_142` ，可得


.. math::
    :label: eq_bc_144


    &p(Y=1|X=x;\theta)

    &=  \frac{1}{1 + \exp \left [
    \ln (1-\lambda)
    - \ln \lambda
    + \ln  \prod_{j=1}^m \pi_{0j}^{x_j} (1- \pi_{0j})^{1-x_j}
    - \ln  \prod_{j=1}^m \pi_{1j}^{x_j} (1- \pi_{1j})^{1-x_j}
    \right ]
    }

    &=  \frac{1}{1 + \exp \left [
    \ln (1-\lambda)
    - \ln \lambda
    +  \sum_{j=1}^m [ {x_j} \ln \pi_{0j} +(1-x_j) ln (1- \pi_{0j})]
    -  \sum_{j=1}^m [ {x_j} \ln \pi_{1j} +(1-x_j) ln (1- \pi_{1j})]
    \right ]
    }

    &=  \frac{1}{1 + \exp \left [
    \ln (1-\lambda)
    - \ln \lambda
    +  \sum_{j=1}^m \left [ {x_j} (\ln \pi_{0j} - \ln \pi_{1j})
    +(1-x_j) (ln (1- \pi_{0j}) - ln (1- \pi_{1j})) \right ]
    \right ]
    }

    &=  \frac{1}{1 + \exp \left [
    \ln (1-\lambda)
    - \ln \lambda
    +  \sum_{j=1}^m \left [ {x_j} \left [ \ln \pi_{0j} + ln (1- \pi_{1j})-ln (1- \pi_{0j}) - \ln \pi_{1j} \right ] \right ]
    + \sum_{j=1}^m \left [( ln (1- \pi_{0j}) - ln (1- \pi_{1j})) \right ]
    \right ]
    }


:eq:`eq_bc_144` 看上去非常复杂，没关系，我们重新参数化一下，
重新定义两类新的未知参数


.. math::


    \beta_j &=  - \left [ \ln \pi_{0j} + ln (1- \pi_{1j})-ln (1- \pi_{0j}) - \ln \pi_{1j} \right ]

    \beta_0 &= - \left \{ \ln (1-\lambda)   - \ln \lambda
    + \sum_{j=1}^m \left [( ln (1- \pi_{0j}) - ln (1- \pi_{1j})) \right ] \right \}


用 :math:`\beta_0,\beta_j` 重新参数化 :eq:`eq_bc_144`

.. math::
    :label: eq_bc_145


    p(Y=1|X=x;\theta)
    =  \frac{1}{1 + \exp \left [ -\beta_0 - \sum_{j=1}^m x_j \beta_j  \right ] }


定义两个向量

.. math::
    :label: eq_bc_146

    \beta &= [\beta_0,\beta_1,\beta_2,\cdots,\beta_j,\cdots,\beta_m]^T

    x &= [1,x_1,x_2,\cdots,x_j,\cdots,x_m]^T


:eq:`eq_bc_145` 的向量化表示为


.. math::
    :label: eq_bc_147


    p(Y=1|X=x;\theta)
    =  \frac{1}{1 + \exp  - x^T \beta }

相信很多读者已经看出来了，:eq:`eq_bc_147` 就是逻辑回归模型的表达式，它表示样本为正类的概率。



**高斯判别模型**

当特征变量 :math:`X` 是高斯变量时，这里直接使用多元高斯，类似朴素贝叶斯模型的推导过程，
把 :eq:`eq_bc_023` 代入 :eq:`eq_bc_142`




.. math::
    :label: eq_bc_150

    &p(Y=1|X=x;\theta)

    &= \frac{1}{1+
    \exp \{ -ln \frac{\lambda}{1-\lambda} + \frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)
     - \frac{1}{2} (x-\mu_0)^T\Sigma^{-1}(x-\mu_0)   \}
    }

    &=\frac{1}{1+\exp \{ -(\mu_1-\mu_0)^T\Sigma^{-1}x  +\frac{1}{2}(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1+\mu_0)
    - \ln \frac{\lambda}{1-\lambda}\}  }



同样地，重新定义两个新的参数
:math:`\beta` 和 :math:`\beta_0` 来简化公式。

.. math::
    :label: eq_bc_152


    \beta_x &\triangleq \Sigma^{-1}(\mu_1-\mu_0)

    \beta_0 &\triangleq -\frac{1}{2}(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1+\mu_0)
    + \ln \frac{\lambda}{1-\lambda}

:eq:`eq_bc_150` 重新参数化为


.. math::
    :label: eq_bc_153


    p(Y=1|X=x;\theta) &= \frac{1}{1+\exp\{ -x^T \beta_x- \beta_0 \}}

    &= \frac{1}{1+\exp\{ -x^T \beta \}}


到这里可以看到，无论是朴素贝叶斯模型还是高斯判别模型，最后都可以重新参数化逻辑回归的形式，
显然这意味着，贝叶斯分类器（朴素贝叶斯、高斯判别）和逻辑回归模型在某个层面上是等价的。
事实上，贝叶斯分类器属于生成模型，而逻辑回归模型是对应的判别模型，
二者在使用上和效果上还是存在一定的差异的，下一节再详细介绍生成模型和判别模型的区别与联系，
这里继续探讨逻辑函数的一些特色。



:eq:`eq_bc_153` 是一个很特别的函数，记作

.. math::

    \phi(z) = \frac{1}{1+e^{-z}}

通常会把 :math:`z=x^T \beta` 称为线性预测器，它是 :math:`x` 的仿射函数。

.. note::

    仿射变换，又称仿射映射，是指在几何中，一个向量空间进行一次线性变换并接上一个平移，变换为另一个向量空间。



函数 :math:`\phi(z)` 是一个平滑的S型曲线，
通常称之为 *逻辑函数(logistic function)* ，
函数的形状如 :numref:`fg_bc_019` 所示，
这种形状是一个类似 "S" 型曲线的函数，
也被称为 sigmod 函数。


.. _fg_bc_019:

.. figure:: pictures/32_9.jpg
    :scale: 60 %
    :align: center

    逻辑函数的曲线


.. _fg_32_10:

.. figure:: pictures/32_10.jpg
    :scale: 40 %
    :align: center

    图中虚线和实线都是等后验概率的轮廓线，
    (a)当 :math:`\sigma_1 = \sigma_2` ，这些等线正交于两个类别的均值向量间的连线。
    (b)当 :math:`\sigma_1 \ne \sigma_2` ，后验概率等轮廓线仍然是直线，但是他们不再正交于均值向量的连线。



现在我们探讨一下高斯判别模型后验概率的几何解释，如 :numref:`fg_32_10` 所示。
特征向量 :math:`x` 通过一个仿射函数线性变换成 :math:`z` ，
然后再经过逻辑函数 :math:`\phi(z)` 得到后验概率值。
仿射函数 :math:`z=x^T \beta`
在几何上表示特征向量 :math:`x` 到 :math:`\beta` 向量上的投影，
而向量 :math:`\beta` 和两个类别高斯的均值相关。

.. math::

    \beta \triangleq \Sigma^{-1} (\mu_1-\mu_0)


其中 :math:`\mu_1-\mu_0` 两个均值向量的差值，表示为 :numref:`fg_32_10` 中两组同心圆的连线。
协方差矩阵 :math:`\Sigma` 是一个对角矩阵，我们假设所有特征的方差相同，即协方差矩阵 :math:`\Sigma`
对角元素都相等，我们假设方差都为1，即协方差矩阵为单位矩阵 :math:`\Sigma=I` ，
这时 :math:`\beta=(\mu_1-\mu_0)` 。
此时在特征空间中，特征向量 :math:`x` 到 :math:`\beta` 的投影就是：

.. math::


    \beta^T x = (\mu_1-\mu_0)^T x


此时，所有落在正交于 :math:`\beta` 的直线上的特征向量 :math:`x` 的投影都是相等的，
:numref:`fg_32_10` (a)中斜直线。
:math:`z` 的 :math:`\gamma` 部分与 :math:`x` 无关，对于特征空间中的所有 :math:`x` ，
:math:`\beta_0` 部分都是一样的值，
所以投影相同意味着 :math:`z` 相同，:math:`z` 相同意味着后验概率 :math:`\phi(z)` 相同。
**特征空间中，同一条斜直线上的特征点，其后验概率相同。**
换句话说，这些斜直线是等后验概率轮廓线。


再来看一个特殊的情况，当两个类别的先验概率相同 :math:`\lambda=1-\lambda` 时，
:math:`z=0` 中的子项 :math:`log(\lambda/(1-\lambda))` 就会被消除掉。
这时 :math:`z` 可以改写成：

.. math::

    z&= (\mu_1-\mu_0)^T\Sigma^{-1}x  - \frac{1}{2}(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1+\mu_0)

    &=(\mu_1-\mu_0)^T\left ( x-\frac{(\mu_1+\mu_0)}{2} \right )

此时，当 :math:`x=\frac{(\mu_1+\mu_0)}{2}` 时，有 :math:`z=0` 。
进而有 :math:`p(Y=1|X=x)=p(Y=0|X=x)=0.5` ，这是因为当 :math:`z=0` 时，
逻辑函数的值为0.5，两个类别的后验概率相等，:numref:`fg_32_10` 中的实斜线，
实斜线代表的是后验概率等于0.5的位置。



类别变量的先验参数 :math:`\lambda` 通过 *对数几率比(log odds ratio)* :math:`log(\lambda/(1-\lambda))`
影响着后验概率，这一项可以看成 :numref:`fg_bc_019` 横轴上的平移。
当 :math:`\lambda` 大于0.5时，会使得图像向左平移，后验概率 :math:`p(Y=1|x)` 得到一个较大的的值，
参考 :numref:`fg_32_11` (a)。
反之，当 :math:`\lambda` 小于0.5时，相当于图像右移，参考 :numref:`fg_32_11` (b)。


.. _fg_32_11:

.. figure:: pictures/32_11.jpg
    :scale: 40 %
    :align: center

    每个图中右上角的图形是类别 :math:`Y=1` 条件下的高斯，左下角是类别 :math:`Y=0` 的高斯。
    (a)当类别先验 :math:`\lambda` 大于0.5时，等后验概率轮廓线向左移动，得到的后验概率更倾向于类别 :math:`Y=1` 。
    (b)当类别先验 :math:`\lambda` 小于0.5时，等后验概率轮廓线向右移动，得到的后验概率更倾向于类别 :math:`Y=0` 。



我们额外增加一个新的特征，其值为固定值1，:math:`x=[x^{old},1]` ，
并且定义 :math:`\beta \triangleq (\beta_0-log(\lambda/(1-\lambda)),\beta)^T`
， :math:`\beta` 是一个关于 :math:`\mu_k,\Sigma,\beta_0` 的函数。
用这个新的符号改写一下后验概率可得：

.. math::

    p(Y=1|X=x;\beta) = \frac{1}{1+e^{-x^T \beta}}


经过简化后我们发现，类别条件高斯密度的后验概率是特征向量 :math:`x` 的线性函数外套上一个的逻辑函数。
我们得到了一个 *线性分类(linear classifier)器* ，"线性"的含义是：特征空间的等后验概率轮廓线是直线。
事实上，如果直接用这个函数进行分类建模，就是我们常说的逻辑回归(logistics regression)模型。

.. important::

    原始高斯判别模型是建立在很多假设的基础上的，(1)特征变量关于类别变量完全条件独立
    :math:`X_i \perp \!\!\! \perp X_j | Y,(i,j)\in [1,m]` 。
    (2) 特征变量 :math:`X_j` 服从高斯分布。
    (3 )两个类别的条件高斯分布具有相同的协方差矩阵，类别1的条件高斯分布 :math:`p(X|Y=1;\mu_0,\Sigma)`
    和类别0的条件高斯分布 :math:`p(X|Y=0;\mu_1,\Sigma)` 就有不同的均值参数，相同的协方差参数。
    这三个假设任意一个不满足时，原始高斯判别模型就不适用。

    其中一个假设是两个类别的条件高斯分布的协方差参数是相同的，这个假设使得后验概率的分子分母中的
    二次项 :math:`x^T\Sigma^{-1}x` 被消除掉。
    如果我们取消了这个假设，后验概率仍然是逻辑函数的形式，但是逻辑函数内会包含特征 :math:`x` 的二次项，
    这时，等后验概率的轮廓线将不再是直线，而是二次曲线，得到的分类器将是二次分类器(quadratic classifier)。


生成模型和判别模型
########################################################

无论是贝叶斯分类器还是逻辑回归模型，都是在求解后验条件概率分布 :math:`P(Y|X)`
，两者在细节上有些差异。

贝叶斯分类器先对 :math:`P(Y)`
和 :math:`P(X|Y)` 分别建模，然后得到联合概率分布 :math:`P(X,Y)`
，再利用模型推断（贝叶斯定理）得到后验条件概率分布 :math:`P(Y|X)`
，参考公式 :eq:`eq_bc_210`。

.. math::
    :label: eq_bc_210

    P(Y|X) = \frac{P(X,Y)}{P(X)} = \frac{P(Y)P(X|Y)}{P(X)}

像贝叶斯分类器这类对联合概率分布进行建模的模型称之为 **生成模型（generative model）**，
生成模型对联合概率分布进行了完整的建模和参数化，由于有了完整的联合概率分布，
后期可以从中进行随机采样。

逻辑回归模型是对贝叶斯模型的重新参数化，减少了参数数量的同时，
也弱化了模型的表达能力，重新参数化后，
相当于直接对后验概率分布 :math:`P(Y|X)` 进行参数化表示，
已无法得到 :math:`P(Y)` 和 :math:`P(X|Y)` ，
此类模型称之为 **判别模型（discrimination model）**。




.. _fig_bc_102:

.. digraph:: 生成模型与判别模型
    :align: center
    :caption: （a）生成模型，对联合概率建模，然后推断出后验条件概率分布；（b）判别模型，直接对因子（ :math:`P(Y|X)` ）建模。

    node[shape=circle]

    subgraph cluster_1 {
        graph[labelloc="b", color=none ,label="(a)"];

        a_x1[label=<X<SUB>1</SUB>>]
        a_x2[label=<X<SUB>2</SUB>>]
        a_x3[label=<X<SUB>3</SUB>>]
        a_xh[label="..."  color=none ]

        a_xn[label=<X<SUB>n</SUB>>]
	    a_y[label=Y]

	    a_y -> a_xh[ style=invisible dir=none]
	    a_y -> {a_x1 a_x2 a_x3 a_xn}

    }

    subgraph cluster_2 {
        graph[labelloc="b", color=none ,label="(b)"];

        b_x1[label=<X<SUB>1</SUB>>]
        b_x2[label=<X<SUB>2</SUB>>]
        b_x3[label=<X<SUB>3</SUB>>]
        b_xh[label="..."  color=none ]

        b_xn[label=<X<SUB>n</SUB>>]
	    b_y[label=Y]

	    b_s1[shape=box label="" width=0.1 height=0.1 ]
	    b_s2[shape=box label="" width=0.1 height=0.1 ]
	    b_s3[shape=box label="" width=0.1 height=0.1 ]
	    b_sh[shape=box label="" width=0.1 height=0.1,color=none ]
	    b_sn[shape=box label="" width=0.1 height=0.1 ]

	    b_y -> b_sh[ style=invisible dir=none]
	    b_y -> {b_s1 b_s2 b_s3 b_sn}[dir=none shape=obox ]
	    b_s1 -> b_x1[dir=none]
	    b_s2 -> b_x2[dir=none shape=box ]
	    b_s3 -> b_x3[dir=none]
	    b_sn -> b_xn[dir=none]
	    b_sh -> b_xh[dir=none style=invisible]

	    {rank="same"; b_x1 b_x2 b_x3 b_xh b_xn}
    }







多分类
#############################################

我们已经讨论完二分类的高斯判别模型，现在我们讨论下在多分类场景下的高斯判别模型。
在二分类问题中，我们假设类别变量为伯努利变量 :math:`Y \in {0,1}` 。
显然在多分类的场景中，类别变量 :math:`Y` 将不再是伯努利变量，而是一个多项式变量。
我们假设一共有K个类别，即类别变量 :math:`Y` 是一个K值多项式变量，
我们用one-hot方法表示变量 :math:`Y` ，用一个K维的向量表示变量 :math:`Y` 的取值，
:math:`y=[\lambda_1,\lambda_2,\dots,\lambda_K], \sum_{k=1}^K \lambda_k = 1` ，
其中 :math:`\lambda_k` 表示向量的第k个元素为1的概率，也就是多项式变量 :math:`Y` 是第k个类别的概率。



.. math::

    \lambda_k=p(Y^k=1;\lambda)


其中 :math:`Y^k` 表示类别向量的第k个元素。多项式变量 :math:`Y` 的概率质量函数可以写成：

.. math::

    p(y;\lambda) = \prod_{k=1}^K \lambda_k^{\delta(y,y_k)}

其中 :math:`\delta(y,y_k)` 是指示函数，当 :math:`y=y_k` 成立的时候值为1，否则值为0。
同样，对于每种类别k，定义一个条件高斯密度函数：

.. math::

    p(x|Y^k=1;\theta)=\frac{1}{(2\pi)^{1/2}|\Sigma|^{1/2}}
    \exp \left \{  -\frac{1}{2}(x-\mu_k)^T \Sigma^{-1} (x-\mu_k)  \right \}

其中 :math:`\mu_k` 是第k个类别下特征变量高斯分布的均值参数，:math:`\Sigma` 是方差参数。
和之前二分类高斯判别模型一样，仍然假设所有类别下特征变量的高斯分布拥有同一个方差参数 :math:`\Sigma` 。
又因为在给定 :math:`Y` 时，所有特征变量是相互条件独立的( :numref:`fg_32_7` (a))，所以协方差矩阵 :math:`\Sigma` 是一个对角矩阵。
对于更一般的协方差矩阵  :math:`\Sigma` (不是对角矩阵)，就意味着是 :numref:`fg_32_7` (c) 所示的模型，特征变量间不满足条件独立性。

根据贝叶斯定义可以得到类别k的后验概率：

.. math::
    :label: eq_32_18

    p(Y^k=1|x;\theta) &= \frac{p(Y^k=1;\lambda) p(x|Y^k=1;\theta)  }
    { \sum_{l=1}^{K} p(Y^l=1;\lambda) p(x|Y^l=1;\theta) }

    &= \frac{ \lambda_k \exp \{ -\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)  \}  }
    {\sum_{l=1}^{K}  \lambda_l \exp \{ -\frac{1}{2}(x-\mu_l)^T\Sigma^{-1}(x-\mu_l)  \}  }

    &= \frac{ \exp \{ \mu_k^T\Sigma^{-1}x -\frac{1}{2} \mu_k^T \Sigma^{-1}\mu_k + \log \lambda_k \}}
    { \sum_{l=1}^{K} \exp \{ \mu_l^T\Sigma^{-1}x -\frac{1}{2} \mu_l^T \Sigma^{-1}\mu_l + \log \lambda_l \}}


计算过程中，x的二次项 :math:`x^T\Sigma^{-1}X` 可以被消除掉，使得最后得到的是x的线性函数的指数。
我们定义一个新的参数 :math:`\beta_k` 简化上述公式。

.. math::
    \beta_k \triangleq \left [
    \begin{matrix}
    -\mu_k^T \Sigma^{-1} \mu_k + \log \lambda_k \\
    \Sigma^{-1}\mu_k
    \end{matrix}
    \right ]


人为的给特定向量x在第一个位置增加一个常数元素1，然后用新的参数简化后验概率公式 :eq:`eq_32_18` 后为：

.. math::

    p(Y^k=1|x;\theta) = \frac{e^{\beta_k^T x}}{ \sum_{l=1}^{K} e^{\beta_l^T x}}

这个函数通常也被称为softmax函数，softmax函数是逻辑函数的扩展，逻辑函数是2分类函数，softmax扩展成多分类，
softmax拥有和逻辑函数类似的几何解释。







.. _fg_32_12:

.. figure:: pictures/32_12.jpg
    :scale: 40 %
    :align: center

    softmax函数在特征空间上的轮廓线。图中的实直线表示两个类别概率相同 :math:`\phi_k(z)=\phi_l(z)` 的位置，
    类别 :math:`k` 和类别 :math:`l` 后验概率相同的轮廓线。


多分类高斯分类模型的最大似然估计和二分类模型没有本质区别，参数的最大似然估计等于经验分布，区别就是原来样本被分成两类，
现在样本被分成多类。


.. math::

    \hat{\lambda}_{k,ML} &= \frac{\sum_{n=1}^N \delta(y_n=y_k) }{N}

    \hat{\mu}_{jk,ML} &= \frac{\sum_{n=1}^N \delta(y_n=y_k)  x_{j,n}}{\sum_{n=1}^N \delta(y_n=y_k)}


    \hat{\sigma}^2_{j,ML} &= \sum_{k=1}^K
    \frac{\sum_{n=1}^N \delta(y_n=y_k) (x_{j,n}-\hat{\mu}_{jk,ML})^2}{\sum_{n=1}^N \delta(y_n=y_k) }



本节我们讨论的高斯判别模型是一个线性分类(linear classifier)器，
当然如果把不同类别的高斯分布的协方差矩阵设置成不同的参数，就可以得到非线性分类器，
有兴趣的读者可以自己推导一下。











其它扩展
########################################################

