.. _ch_mixture_model:

########################################################
混合模型
########################################################

我们在 :numref:`第%s章<ch_EM>`
讨论了不完整观测数据的模型学习问题，并介绍了解决不完整观测数据的参数估计问题的EM算法。
本章开始讨论不完整观测数据的模型案例，当然我们按照从简单到复杂的顺序来介绍。
从模型中包含的变量数量上来讲，最简单的是双变量模型；
从不可观测变量的分布上来讲，离散分布要比连续分布简单一些。

- 双变量模型，不可观测变量是离散分布，比如高斯混合模型（Gaussian mixture model,GMM）。
- 双变量模型，不可观测变量是连续分布，比如主成分分析（Principal component analysis,PCA）模型。
- 时间序列模型，不可观测变量是离散分布，比如隐马尔科夫模型（Hidden Markov Model，HMM）。


本章我们先讨论混合模型（mixture model），它是一个典型的应用EM解决不完整观测数据的模型，
混合模型是一类模型，其中最常用的是高斯混合模型（Gaussian mixture model,GMM）。
我们先介绍一般形式的混合模型，再讨论高斯混合模型，
并且在本章最后介绍一下聚类场景中最基本的模型K-means与EM算法的关系。








一般混合模型
########################################################

在实际应用中，经常会遇到观测数据并不是从某个单一的总体分布中采样得到，
而是来源于多个拥有不同参数的相同分布。
这里首先给出一个概念：参数分布族(parametric family of distributions)。

.. glossary::

    参数分布族(parametric family of distributions)
        参数分布族是指，属于同一种概率分布仅仅参数值不同的一类分布的集合，
        比如拥有不同均值和方差的高斯分布组成的分布集合。


假设有一份某地小学生的身高数据，已知身高数据可以看做近似服从高斯分布，
这份数据中含有全部六个年级的学生，并且男生女生都有。
按照过去一般的处理逻辑，可以用一个高斯分布去拟合这份数据，
然而由于不同年龄段的孩子身高差异较大，用一个单一的高斯模型并不能很好的拟合数据，
此时可以用多个高斯分布去分别拟合不同年龄段不同性别的身高数据，
这些高斯分布拥有不同的均值和方差参数，但他们都是高斯分布，而不是什么其它的分布。
这就是一个典型的混合分布的例子，数据来源于一个参数分布族，
即不同参数的同一种分布。

模型的有向图表示
===========================================


现在我们用概率图的语言来描述混合模型，
首先定义一个离散类别变量，记作 :math:`Z`
，随机变量 :math:`Z` 用于表示当前数据来源于哪一个具体的分布。
假设数据来源于 :math:`K` 个不同参数的分布，
则类别变量 :math:`Z` 有 :math:`K` 个取值，
:math:`Z_i=z_k` 表示第 :math:`i`
条观测样本来源于第 :math:`k` 个分布。
用 :math:`X` 表示观测变量，
观测变量 :math:`X` 一共有 :math:`K` 个分量，
每个分量 :math:`X_k` 有不同的参数。
混合模型的有向图可以用
:numref:`fg_mixture_1.00`
所示，其中 :math:`\lambda`
表示类别变量 :math:`Z` 的参数，
:math:`\theta_k`
表示观测变量 :math:`X_k` 的参数。




.. _fg_mixture_1.00:

.. digraph:: 混合模型的有向图表示
    :align: center
    :caption: 混合模型的有向图表示

    node[shape=circle,fixedsize=true,width=0.5];
    rankdir = LR

    lambda[label="𝜆" shape="plaintext"]



    subgraph cluster_a {

        Z[label=<Z<SUB>i</SUB>>];
        X[label=<X<SUB>i</SUB>>, style=filled];
        Z -> {X};

        label="N";


    }

    subgraph cluster_b {
        theta[label=<𝜃<SUB>k</SUB>> shape="plaintext"]
        label="K";
    }

    lambda ->Z
    theta->X





在混合模型中，一条观测样本的生成（采样）由两个变量控制，样本的生成过程为：

1. 首先，从变量 :math:`Z` 进行采样，假设采样值为 :math:`z=k` 。
2. 然后，从变量 :math:`X` 的第 :math:`k` 个分量（记作 :math:`X_k`）中采样得到 :math:`x` 。

模型的联合概率分布可以写为

.. math::
    :label: eq_mixture_01.04

    P(Z,X;\lambda,\theta) = P(Z;\lambda)P(X|Z;\theta)

其中变量 :math:`Z` 是一个类别变量，它的边缘概率分布可以写为

.. math::
    :label: eq_mixture_01.05

    P(Z;\lambda) = \prod_{k=1}^K \lambda_k^{z_k} ,\quad \sum_{k=1}^K \lambda_k = 1

响应的变量 :math:`X` 有 :math:`K` 个分量，所有分量都属于同一种概率分布，只是参数不同。
条件概率分布 :math:`P(X|Z;\theta)` 就表示每个分量的概率分布，
可以写为

.. math::
    :label: eq_mixture_01.06

    P(X|Z;\theta) = \prod_{k=1}^K P(X;\theta_k)^{z_k}

分量 :math:`X_k` 的具体分布取决于你的实际场景（数据），
当 :math:`X_k` 是高斯分布时就被称为高斯混合模型，
这里我们暂时不关心它的具体形式，下一节再讨论高斯混合模型。

参数估计
===========================================

完整观测数据
------------------------------------------
首先假设 :math:`Z` 和 :math:`X` 都能观测到，
即观测数据中同时包含了 :math:`Z` 和 :math:`X` 的值，
此时称为完整观测数据，
数据的对数似然函数为


.. math::
    :label: eq_mixture_02.02

    \ell &= \sum_i^N \ln P(Z_i,X_i;\lambda,\theta)\\
    &= \sum_i^N \ln  P(Z_i;\lambda)P(X_i|Z_i;\theta) \\
    &=  \sum_i^N \ln  P(Z_i;\lambda) +  \sum_i^N \ln P(X_i|Z_i;\theta)\\
    &=  \sum_i^N \ln  P(Z_i;\lambda) +  \sum_i^N \ln \prod_{k=1}^K P(X_i;\theta_k)^{z_k} \\
    &=  \sum_i^N \ln  P(Z_i;\lambda) +  \sum_i^N \sum_{k=1}^K  {z_k} \ln  P(X_i;\theta_k)


可以看到，在完整观测数据的情况下，
:math:`P(Z_i;\lambda)` 和 :math:`P(X_k;\theta_k)`
可以分别独立的使用观测数据进行参数估计，估计过程比较简单。
这里就不赘述了，如果读者不理解这步之后的估计过程，可以重新学习本书之前的章节。


不完整观测数据
------------------------------------------

然而在实际应用场景中，类别变量 :math:`Z` 通常是不可观测的，
即我们并不知道样本是从 :math:`X` 的哪个分量取得的，
通常把模型中不可观测的变量称为隐变量或者潜在变量。
此时观测样本只有 :math:`X` 的值，
似然函数（所有样本的联合概率）为


.. math::
    :label: eq_mixture_02.03

    P(X_1,X_2,\cdots,X_N;\theta) = \prod_{i=1}^N P(X_i;\theta)


根据图模型的推断理论，需要在联合概率的基础上通过边际化（消元）的方法得到边缘概率
:math:`P(X_i;\theta)` ，

.. math::
    :label: eq_mixture_02.04

    P(X_i;\theta) = \sum_z P(X_i,Z_i;\lambda,\theta)


此时对数似然函数为

.. math::
    :label: eq_mixture_02.05

    \ell &= \sum_{i=1}^N \ln P(X_i;\theta) \\
    &= \sum_{i=1}^N \ln  \sum_z P(X_i,Z_i;\lambda,\theta)


可以看到上式中对数 :math:`\ln` 内部存在一个求和符号，
这使得无法对公式进一步的拆解，这种情况下很难对其求偏导，
给参数估计带来了极大的困难。
在含有隐变量的模型中，需要对隐变量进行消元，以便得到观测变量的边缘概率，
而这个消元操作（求和或者积分）会出现在对数似然函数的对数内部，
最终导致对数似然函数无法分解，使得参数估计无比困难。
EM算法就是对这一问题的有效解决，通过 Jensen 不等式的理论，
可以在等价的情况下，把对数内部的求和操作转换到对数外部。

EM算法
------------------------------------------

EM算法的具体推导过程这里不再赘述，读者可以回顾一下
:numref:`第%s章<ch_EM>` 的内容，
这里仅给出E步和M步的具体内容。


**E-步骤：** 得到隐变量的后验概率分布

用 :math:`\gamma(Z_i)` 表示 :math:`Z_i` 的后验概率分布的概率质量函数，
根据贝叶斯公式有

.. math::
    :label: eq_mixture_02.06

    \gamma(Z_i) &= P(Z_i|X_i;\lambda^{t-1},\theta^{t-1}) \\
    &= \frac{P(X_i,Z_i;\lambda^{t-1},\theta^{t-1})}{P(X_i)} \\
    &= \frac{P(Z_i;\lambda^{t-1})P(X_i|Z_i;\theta^{t-1})}{\sum_z  P(Z_i;\lambda^{t-1})P(X_i|Z_i;\theta^{t-1}) }

具体的 :math:`\gamma(Z=z_k)` 为

.. math::
    :label: eq_mixture_02.07

    \gamma(Z_i=z_k)
    &= \frac{P(Z_i=z_k;\lambda^{t-1})P(X_i|Z_i=z_k;\theta^{t-1})}{\sum_{k=1}^K P(Z_i=z_k;\lambda^{t-1})P(X_i|Z_i=z_k;\theta^{t-1})}\\
    &= \frac{P(Z_i=z_k;\lambda^{t-1})P(X_i;\theta_k^{t-1})}{\sum_{k=1}^K P(Z_i=z_k;\lambda^{t-1})P(X_i;\theta_k^{t-1})  }


其中 :math:`\lambda^{t-1},\theta^{t-1}` 都是已知量，其值使用上一轮迭代得到的估计值。
这一步的结果，
就是为每一条样本 :math:`X_i`
计算一个后验概率分布 :math:`\gamma_{ik}`，其表示样本 :math:`X_i`
来自于各个分量的概率，
也可以认为是各个分量对样本 :math:`X_i`
的"贡献"。


**M-步骤：** 极大化期望，得到参数的解。


极大化的目标函数记作 :math:`Q`，其形式为

.. math::
    :label: eq_mixture_02.08

    Q &= \sum_{i=1}^N \sum_{k=1}^K \gamma(Z_i=z_k;\lambda^{t-1},\theta^{t-1})
    \ln P(X_i,Z_i=z_k;\lambda^{t},\theta^{t}) \\
    &= \sum_{i=1}^N \sum_{k=1}^K \gamma(Z_i=z_k;\lambda^{t-1},\theta^{t-1})
    \ln \left [ P(Z_i;\lambda^{t})P(X_i|Z_i=z_k;\theta^{t})\right ] \\
    &= \sum_{i=1}^N \sum_{k=1}^K \gamma(Z_i=z_k;\lambda^{t-1},\theta^{t-1})
    \left [ \ln P(Z_i=z_k;\lambda^{t}) + \ln P(X_i|Z_i=z_k;\theta^{t}) \right ] \\
    &= \sum_{i=1}^N \sum_{k=1}^K \gamma(Z_i=z_k;\lambda^{t-1},\theta^{t-1})
    \left [ \ln P(Z_i=z_k;\lambda^{t}) + \ln P(X_{i};\theta^{t}) \right ] \\
    &= \left [ \sum_{i=1}^N \sum_{k=1}^K \gamma(Z_i=z_k;\lambda^{t-1},\theta^{t-1})\ln P(Z_i=z_k;\lambda^{t})
    \right ] \\
    &\quad + \left [ \sum_{i=1}^N \sum_{k=1}^K \gamma(Z_i=z_k;\lambda^{t-1},\theta^{t-1}) \ln P(X_{i};\theta^{t}) \right ]


令 :math:`\gamma_{ik}=\gamma(Z_i=z_k;\lambda^{t-1},\theta^{t-1})`
，上式的一个简洁表示为

.. math::
    :label: eq_mixture_02.09


    Q = \left [ \sum_{i=1}^N \sum_{k=1}^K  \gamma_{ik} \ln P(Z_i=z_k;\lambda^{t})
    \right ] +
    \left [ \sum_{i=1}^N \sum_{k=1}^K \gamma_{ik} \ln P(X_{i};\theta^{t}) \right ]



可以看到在 :eq:`eq_mixture_02.09` 中，对数内的联合概率项已经可以分解开，和完整观测的对数似然函数一样，
可以独立的估计每一项局部条件概率的参数。
稍微有点区别的是，每一项前面多了一个系数 :math:`\gamma_{ik}`
，并且需要对这个系数进行求和。
:math:`\gamma_{ik}` 可以看做是一个权重，可以理解成每一个分量对观测样本的"贡献"
，事实上，它含义就是在取得观测样本的条件下，观测样本 :math:`i`
来自于第 :math:`k` 个分量的概率。


高斯混合模型
########################################################


当观测变量 :math:`X` 是高斯分布时，就称为高斯混合模型（Gaussian mixture model,GMM）
，高斯混合模型是应用最广的混合模型。
此时构成 :math:`X` 的每个分量都是一个高斯变量，
通常在高斯混合模型中，:math:`X` 的每个分量是一个多元高斯分布，
为了以示区分，我们用粗体符号，观测变量整体记作 :math:`\pmb{X}`
，第 :math:`k` 个分量记作 :math:`\pmb{X}_k`
，粗体表示变量是一个多元高斯分布,
记作 :math:`\pmb{X}_k \sim \mathcal{N}(\pmb{\mu}_k,\Sigma_k)`
。隐变量 :math:`Z` 仍然是一个服从类别分布的单变量，
记作 :math:`Z \sim Cat(\lambda_k)`。

模型的表示
============================================================

高斯混合模型的有向图表示为



.. _fg_mixture_2.00:

.. digraph:: 高斯混合模型的有向图表示
    :align: center
    :caption: 高斯混合模型的有向图表示

    node[shape=circle,fixedsize=true,width=0.5];
    rankdir = LR

    lambda[label="𝜆" shape="plaintext"]



    subgraph cluster_a {

        Z[label=<Z<SUB>i</SUB>>];
        X[style=filled,label=<𝑋<SUB>i</SUB>>];
        Z -> {X};

        label="N";


    }

    subgraph cluster_b {
        mu[label=<𝜇<SUB>k</SUB>> shape="plaintext"]
        sigma[label=<Σ<SUB>k</SUB>> shape="plaintext"]
        label="K";
    }

    lambda ->Z
    mu,sigma->X




同理，模型的联合概率分布为

.. math::
    :label: eq_mixture_03.04

    P(Z,\pmb{X};\lambda,\pmb{\mu},\Sigma) = P(Z;\lambda)P(\pmb{X}|Z;\pmb{\mu},\Sigma)

类别变量 :math:`Z` 的边缘概率分布 :math:`P(Z;\lambda)` 为

.. math::
    :label: eq_mixture_03.05

    P(Z;\lambda) = \prod_{k=1}^K \lambda_k^{z_k} ,\quad \sum_{k=1}^K \lambda_k = 1

条件概率分布 :math:`P(\pmb{X}|Z)` 为

.. math::
    :label: eq_mixture_03.06

    P(\pmb{X}|Z) = \prod_{k=1}^K P(\pmb{X}_k;\theta_k)^{z_k}
    =  \prod_{k=1}^K \mathcal{N}(\pmb{X};\pmb{\mu}_k,\Sigma_k)^{z_k}

单一分量 :math:`\pmb{X}_k` 的概率密度函数为

.. math::

    P(\pmb{X}_k;\theta_k) = \mathcal{N}(\pmb{X};\pmb{\mu}_k,\Sigma_k)


参数估计
============================================================

按照上一节混合模型的 EM 算法的过程填充相应的部分即可。

**E-步骤：** 计算隐变量 :math:`Z` 的后验概率


.. math::
    :label: eq_mixture_03.07

    \gamma_{ik}
    &= \frac{P(Z_i=z_k;\lambda^{t-1})P(\pmb{X}_{i}=\pmb{x}_i;\theta_k^{t-1})}
    {\sum_{k=1}^K P(Z_i=z_k;\lambda^{t-1})P(\pmb{X}_{i}=\pmb{x}_i;\theta_k^{t-1})  }\\
    &= \frac{ \lambda_k   \mathcal{N}(\pmb{x}_i; \pmb{\mu}_k,\Sigma_k)   }
    {\sum_{k=1}^K \lambda_k   \mathcal{N}(\pmb{x}_i ; \pmb{\mu}_k,\Sigma_k) }


在这一步中，所有参数认为是已知的，使用上一轮迭代得到的估计值。

**M-步骤：** 极大化Q函数得到参数的解。

目标函数为

.. math::
    :label: eq_mixture_03.09


    Q &= \left [ \sum_{i=1}^N \sum_{k=1}^K  \gamma_{ik} \ln P(Z_i=z_k;\lambda^{t}) \right ]
     + \left [ \sum_{i=1}^N \sum_{k=1}^K \gamma_{ik} \ln P(\pmb{X}_{i}=\pmb{x}_i;\theta^{t}_k) \right ] \\
    &= \left [ \sum_{i=1}^N \sum_{k=1}^K  \gamma_{ik} \lambda_k \right ]
    + \left [ \sum_{i=1}^N \sum_{k=1}^K \gamma_{ik} \ln  \mathcal{N}(\pmb{x}_i ; \pmb{\mu}_k,\Sigma_k)  \right ]


在这一步骤中，:math:`\gamma_{ik}` 由上一步计算得到，其值是已知的；
所有模型参数是未知的，需要极大化求解。
对于高斯混合模型来讲，可以直接令目标函数参数的偏导数为零得到参数估计值。

.. math::

    \lambda_k &= \frac{N_k}{N}

    \pmb{\mu}_k &= \frac{\sum_{i=1}^N \gamma_{ik} \pmb{x}_i  }{N_k}


    \Sigma_k &= \frac{  \sum_{i=1}^N \gamma_{ik} (\pmb{x}_i- \pmb{\mu}_k)^2   }{N_k}


其中

.. math::

    N_k = \sum_{i=1}^N \gamma_{ik}


得到参数值后检查是否收敛，如果没有收敛则重复执行E步骤和M步骤，直到参数收敛为止。



K-means
########################################################




