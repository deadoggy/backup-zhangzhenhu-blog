高斯混合模型
########################################################


当观测变量 :math:`X` 是高斯分布时，就称为高斯混合模型（Gaussian mixture model,GMM）
，高斯混合模型是应用最广的混合模型。
此时构成 :math:`X` 的每个分量都是一个高斯变量，
通常在高斯混合模型中，:math:`X` 的每个分量是一个多元高斯分布，
为了以示区分，我们用粗体符号，观测变量整体记作 :math:`\pmb{X}`
，第 :math:`k` 个分量记作 :math:`\pmb{X}_k`
，粗体表示变量是一个多元高斯分布,
记作 :math:`\pmb{X}_k \sim \mathcal{N}(\pmb{\mu}_k,\Sigma_k)`
。隐变量 :math:`Z` 仍然是一个服从类别分布的单变量，
记作 :math:`Z \sim Cat(\lambda_k)`。

模型的表示
============================================================

高斯混合模型的有向图表示为



.. _fg_mixture_2.00:

.. digraph:: 高斯混合模型的有向图表示
    :align: center
    :caption: 高斯混合模型的有向图表示

    node[shape=circle,fixedsize=true,width=0.5];
    rankdir = LR

    lambda[label="𝜆" shape="plaintext"]



    subgraph cluster_a {

        Z[label=<Z<SUB>i</SUB>>];
        X[label=<X<SUB>i</SUB>>];
        Z -> {X};

        label="N";


    }

    subgraph cluster_b {
        mu[label=<𝜇<SUB>k</SUB>> shape="plaintext"]
        sigma[label=<𝜎<SUP>2</SUP><SUB>k</SUB>> shape="plaintext"]
        label="K";
    }

    lambda ->Z
    mu,sigma->X




同理，模型的联合概率分布为

.. math::
    :label: eq_mixture_03.04

    P(Z,\pmb{X};\lambda,\mu,\Sigma) = P(Z;\lambda)P(\pmb{X}|Z;\mu,\Sigma)

类别变量 :math:`Z` 的边缘概率分布 :math:`P(Z;\lambda)` 为

.. math::
    :label: eq_mixture_03.05

    P(Z;\lambda) = \prod_{k=1}^K \lambda_k^{z_k} ,\quad \sum_{k=1}^K \lambda_k = 1

条件概率分布 :math:`P(\pmb{X}|Z)` 为

.. math::
    :label: eq_mixture_03.06

    P(X|Z) = \prod_{k=1}^K P(\pmb{X}_k;\theta_k)^{z_k}
    =  \prod_{k=1}^K \mathcal{N}(\pmb{X};\pmb{\mu}_k,\Sigma_k)^{z_k}

单一分量 :math:`\pmb{X}_k` 的概率密度函数为

.. math::

    P(\pmb{X}_k;\theta_k) = \mathcal{N}(\pmb{X};\pmb{\mu}_k,\Sigma_k)


参数估计
============================================================

按照上一节混合模型的 EM 算法的过程填充相应的部分即可。

**E-步骤：** 计算隐变量 :math:`Z` 的后验概率


.. math::
    :label: eq_mixture_03.07

    \gamma_{ik}
    &= \frac{P(Z_i=z_k;\lambda^{t-1})P(\pmb{X}_{i}=\pmb{x}_i;\theta_k^{t-1})}
    {\sum_{k=1}^K P(Z_i=z_k;\lambda^{t-1})P(\pmb{X}_{i}=\pmb{x}_i;\theta_k^{t-1})  }\\
    &= \frac{ \lambda_k   \mathcal{N}(\pmb{x}_i; \pmb{\mu}_k,\Sigma_k)   }
    {\sum_{k=1}^K \lambda_k   \mathcal{N}(\pmb{x}_i ; \pmb{\mu}_k,\Sigma_k) }


在这一步中，所有参数认为是已知的，使用上一轮迭代得到的估计值。

**M-步骤：** 极大化Q函数得到参数的解。

目标函数为

.. math::
    :label: eq_mixture_03.09


    Q &= \left [ \sum_{i=1}^N \sum_{k=1}^K  \gamma_{ik} \ln P(Z_i=z_k;\lambda^{t}) \right ]
     + \left [ \sum_{i=1}^N \sum_{k=1}^K \gamma_{ik} \ln P(\pmb{X}_{i}=\pmb{x}_i;\theta^{t}_k) \right ] \\
    &= \left [ \sum_{i=1}^N \sum_{k=1}^K  \gamma_{ik} \lambda_k \right ]
    + \left [ \sum_{i=1}^N \sum_{k=1}^K \gamma_{ik} \ln  \mathcal{N}(\pmb{x}_i ; \pmb{\mu}_k,\Sigma_k)  \right ]


在这一步骤中，:math:`\gamma_{ik}` 由上一步计算得到，其值是已知的；
所有模型参数是未知的，需要极大化求解。
对于高斯混合模型来讲，可以直接令目标函数参数的偏导数为零得到参数估计值。

.. math::

    \lambda_k &= \frac{N_k}{N}

    \pmb{\mu}_k &= \frac{\sum_{i=1}^N \gamma_{ik} \pmb{x}_i  }{N_k}


    \Sigma_k &= \frac{  \sum_{i=1}^N \gamma_{ik} (\pmb{x}_i- \pmb{\mu}_k)^2   }{N_k}


其中

.. math::

    N_k = \sum_{i=1}^N \gamma_{ik}


得到参数值后检查是否收敛，如果没有收敛则重复执行E步骤和M步骤，直到参数收敛为止。



K-means
########################################################




