<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>3. 推断与检验 &mdash; 张振虎的博客 张振虎 文档</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
    <link rel="canonical" href="https://www.zhangzhenhu.com/glm/source/推断与检验/content.html" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
        <script src="../../../_static/translations.js"></script>
        <script>window.MathJax = {"tex": {"macros": {"RR": "{\\bf R}", "bold": ["{\\bf #1}", 1], "KL": ["{D_\\textrm{KL}\\left ( #1 \\| #2 \\right )}", 2], "EE": ["{\\mathbb{E}_{#1} \\left [ #2 \\right ]}", 2, ""], "scalemath": ["{\\scalebox{#1}{\\mbox{\\ensuremath{\\displaystyle #2}}}}", 2], "argmin": ["{\\operatorname*{\\arg\\min}}"], "ind": ["{\\perp\\!\\!\\!\\!\\perp}"]}}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" />
    <link rel="next" title="4. 贝叶斯估计" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html" />
    <link rel="prev" title="2. 最大似然估计" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            张振虎的博客
          </a>
              <div class="version">
                acmtiger@outlook.com
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="搜索文档" aria-label="搜索文档" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="导航菜单">
              <p class="caption" role="heading"><span class="caption-text">目录:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">广义线性模型</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id2">1.1. 概率模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id3">1.1.1. 概率律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id4">1.1.2. 离散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id5">1.1.3. 连续模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id6">1.2. 条件概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id7">1.3. 联合概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id8">1.4. 全概率与贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id9">1.5. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id10">1.6. 随机变量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id11">1.6.1. 离散随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id12">1.6.2. 连续随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id13">1.6.3. 累积分布函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id14">1.6.4. 随机变量的函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id15">1.6.5. 期望与方差</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id16">1.7. 边缘化</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id17">1.8. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#ch-basic-bernoulli">1.8.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id19">1.8.2. 二项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id20">1.8.3. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id21">1.8.4. 多项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id22">1.8.5. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id23">1.8.6. 卡方分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#t">1.8.7. t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#f">1.8.8. F分布</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html">2. 最大似然估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-liklihood">2.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id3">2.2. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id4">2.3. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-2-gaussian-ml">2.4. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id6">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">3. 推断与检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">3.1. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ch-sample-distribution">3.2. 抽样分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ch-sample-distribution-normal">3.2.1. 正态分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#t">3.2.2. 学生t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">3.2.3. 卡方分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id6">3.3. 极限理论</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id7">3.3.1. 马尔可夫和切比雪夫不等式</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">3.3.2. 弱大数定律</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id9">3.3.3. 依概率收敛</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ch-clt">3.3.4. 中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">3.3.5. 强大数定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id12">3.4. 似然估计量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id13">3.4.1. 估计量的偏差与方差</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ch-2-fisher-information">3.4.2. 信息量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ch-2-mle-estimator">3.4.3. 最大似然估计的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ch-influence-test-interval">3.5. 置信区间</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#z">3.5.1. 均值参数的 Z 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id18">3.5.2. 均值参数的 T 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id19">3.5.3. 方差参数的区间估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ch-influence-test-test">3.6. 简单假设检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id22">3.6.1. Z检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id23">3.6.2. T检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id24">3.6.3. 卡方检验</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html">4. 贝叶斯估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id2">4.1. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id3">4.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id4">4.1.2. 类别分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id5">4.2. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id6">4.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id7">4.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id8">4.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html">5. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-1">5.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id3">5.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id4">5.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id5">5.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id6">5.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id7">5.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-moments">5.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id9">5.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#kl">5.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html">6. 线性回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id2">6.1. 最小二乘</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id3">6.1.1. 最小误差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id4">6.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id5">6.2. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id6">6.2.1. 高斯假设</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id7">6.2.2. 参数估计</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html">7. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id2">7.1. 指数族分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id3">7.1.1. 自然指数族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id4">7.1.2. 示例：高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id5">7.1.3. 示例：伯努利分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id6">7.2. 广义线性模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id7">7.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html">8. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate">8.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id3">8.2. 泰勒级数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id4">8.3. 梯度下降法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id6">8.4. 牛顿法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id7">8.4.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id8">8.4.2. 标准连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id9">8.4.3. 迭代初始值的设定</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#irls">8.5. 迭代重加权最小二乘(IRLS)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id10">8.5.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id11">8.5.2. 算法过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id12">8.6. 估计量的标准误差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate-phi">8.7. 分散参数的估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html">9. 模型评估</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id2">9.1. 拟合优度</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id3">9.1.1. 嵌套模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#likelihood-ratio">9.1.2. 对数似然比(Likelihood ratio)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance">9.1.3. 偏差(deviance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#r-2">9.1.4. 决定系数 <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#ch-glm-gof-chi">9.1.5. 广义皮尔逊卡方统计量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#residual-analysis">9.2. 残差分析(Residual analysis)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#response-residuals">9.2.1. Response residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#working-residuals">9.2.2. Working residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#partial-residuals">9.2.3. Partial residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#pearson-residuals">9.2.4. Pearson residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance-residuals">9.2.5. Deviance residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#score-residuals">9.2.6. Score residuals</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#model-selection">9.3. 模型选择(model selection)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#aic">9.3.1. AIC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#bic">9.3.2. BIC</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html">10. 模型检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id2">10.1. 拉格朗日乘子检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id3">10.1.1. 得分统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id4">10.1.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#wald">10.2. wald 检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id5">10.2.1. 参数估计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id6">10.2.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id7">10.3. 似然比检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id8">10.3.1. 抽样分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id9">10.3.2. 模型比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id10">10.3.3. 偏差统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#f">10.3.4. F 检验</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id11">10.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">11. 高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">11.1. 传统线性回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">11.2. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">11.3. 高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">11.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">11.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">11.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">11.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id10">11.5. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">12. 逆高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">12.1. 逆高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">12.2. 逆高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">12.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">12.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">12.3.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">12.3.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">12.4. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">13. 二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">13.1. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">13.2. 逻辑回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">13.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">13.2.2. 参数估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#odds-logit">13.2.3. odds 与 logit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">13.3. 二项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">13.4. 二项式回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">13.4.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">13.4.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">13.5. 其它连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">13.5.1. 恒等连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#probit">13.5.2. probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#log-log-clog-log">13.5.3. log-log 和 clog-log</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">13.6. 分组数据与比例数据</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html">14. 泊松模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#poisson">14.1. 泊松(Poisson)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id2">14.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id3">14.1.2. 泊松分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id5">14.2. 泊松回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id6">14.3. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id7">14.4. 拟合统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id8">14.5. 频率模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id9">14.6. 泊松模型的局限性</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html">15. 指数模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#exponential">15.1. 指数(exponential)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id2">15.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id3">15.1.2. 分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id6">15.2. 指数回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id7">15.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id8">15.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id9">15.3.2. 拟合优度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#irls">15.3.3. IRLS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html">16. Gamma 模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id1">16.1. Gamma 函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id2">16.2. Gamma 分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id3">16.3. Gamma 回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id4">16.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id5">16.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#irls">16.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id6">16.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id7">16.5. 其他连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id8">16.5.1. 对数 Gamma 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#identity-gamma">16.5.2. 恒等(identity) Gamma 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html">17. 过度分散</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id2">17.1. 什么是过度分散</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id3">17.2. 过度分散的检测</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id4">17.3. 过度分散的影响</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id5">17.4. 标准误差的修正</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">18. 负二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">18.1. 负二项式分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">18.1.1. 从二项式分布推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">18.1.2. 泊松-伽马混合分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#alpha">18.1.3. 辅助参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的影响</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">18.2. 负二项回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">18.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#irls">18.3.1. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">18.3.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">18.4. 负二项式模型扩展</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">18.4.1. 对数连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">18.4.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">18.4.3. 几何模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">18.4.4. 广义负二项式模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html">19. 零计数问题</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id2">19.1. 零截断模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id3">19.1.1. 零截断泊松模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id4">19.1.2. 零截断负二项式模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id5">19.2. 零膨胀模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#hurdle">19.2.1. Hurdle 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#zero-inflate">19.2.2. Zero-inflate 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">20. 多项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">20.1. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#softmax">20.2. softmax 回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">20.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">20.3. 多项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id6">20.4. 多项式回归模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">21. 有序离散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">21.1. 有序逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">21.2. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">21.3. 连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#logit">21.3.1. logit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#probit">21.3.2. probit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#clog-log">21.3.3. clog-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#log-log">21.3.4. log-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#cauchit">21.3.5. cauchit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">21.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html">附录</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id2">标准正态累积分布表</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id3">卡方分布临界值表</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/content.html">参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../probability_model/index_html.html">概率图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id2">1.1. 概率分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id3">1.2. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#marginalization">1.3. 边缘化(marginalization)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id6">1.4. 贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id7">1.5. 期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id8">1.6. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id9">1.6.1. 离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id10">1.6.2. 连续变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id11">1.6.3. 计数变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id12">1.7. 大数定律</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id13">1.7.1. 独立同分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id14">1.7.2. 中心极限定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id15">1.8. 信息论基础</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id16">1.8.1. 信息熵</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#kl">1.8.2. KL散度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id18">1.8.3. 互信息</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html">2. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-liklihood">2.1. 极大似然估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id3">2.1.1. 二值离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id4">2.1.2. 一般离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-gaussian-ml">2.1.3. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id6">2.1.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-bayesian-estimation">2.2. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id8">2.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id9">2.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id10">2.3. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id11">2.3.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id12">2.3.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id13">2.4. 最大似然估计与贝叶斯估计的对比</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id14">2.5. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#fisher-information">2.6. Fisher Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id15">2.7. 估计量的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id16">2.7.1. 估计量的方差与偏差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id17">2.7.2. 大数定律和中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-mle-estimator">2.7.3. 最大似然估计的特性</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html">3. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-1">3.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id3">3.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id4">3.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id5">3.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id6">3.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id7">3.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-moments">3.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id9">3.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#kl">3.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/19.%E5%A4%9A%E7%BB%B4%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_26.html">4. 多维高斯分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html">5. 有向图(Directed Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id1">5.1. 有向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id2">5.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id3">5.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html">6. 无向图(Undirected Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id1">6.1. 无向图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id2">6.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id3">6.3. 图的分解</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#vs">6.4. 有向图 vs 无向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id4">6.5. 树</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id5">6.6. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html">7. 因子图</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id2">7.1. 因子图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id3">7.2. 图模型之间的转换</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id4">7.2.1. 转换为因子图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id5">7.2.2. 因子图转换为有向图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id6">7.3. 图模型的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#i-map">7.3.1. I-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#d-map">7.3.2. D-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#p-map">7.3.3. P-map</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html">8. 模型推断：消元法</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id2">8.1. 什么是模型的推断</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id3">8.2. 消元法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id4">8.2.1. 有向图消元算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#ch-condition-margin">8.2.2. 条件概率和边缘概率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id6">8.2.3. 无向图的消元法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id7">8.3. 图消除</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id9">8.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html">9. 加和乘积算法(sum-product algorithm)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id1">9.1. 树结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id2">9.2. 从消元法到信息传播</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id3">9.3. 树模型的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id4">9.4. 因子图的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id5">9.5. 类树结构图模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#polytrees">9.6. 多重树(polytrees)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id6">9.7. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html">10. 最大后验估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id2">10.1. 最大后验概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id3">10.2. 最大化后验的状态</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id4">10.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html">11. 完整观测的参数学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id2">11.1. 有向图的参数学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id3">11.2. 无向图的参数学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id4">11.2.1. 成对二值变量模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id5">11.2.2. 一般二值变量模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html">12. 不完整观测的学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#id2">12.1. 隐变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#em">12.2. 期望最大化算法(EM)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/14.%E5%9B%BE%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E5%AD%A6%E4%B9%A0_lecture_23.html">13. 有向图结构学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/16.%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD_lecture_17.html">14. 变分推断</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html">15. 马尔科夫蒙特卡洛</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#why-sampling">15.1. Why sampling？</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#monte-carlo">15.2. 蒙特卡罗(Monte Carlo)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain">15.3. 马尔科夫链(Markov Chain)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id2">15.3.1. 一个例子</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#time-reversibility">15.3.2. 时间可逆性(Time Reversibility)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id3">15.3.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain-monte-carlo">15.4. Markov Chain Monte Carlo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#metropolis-hastings">15.4.1. Metropolis-Hastings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id4">15.4.2. 例子：正态分布的采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id5">15.4.3. 多变量采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#gibbs">15.4.4. Gibbs 采样</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#mixing-time">15.5. Mixing Time</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#approximate-map-and-partitioning">15.6. Approximate MAP and Partitioning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html">16. 贝叶斯分类器</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id2">16.1. 朴素贝叶斯模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id3">16.1.1. 模型表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id4">16.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id5">16.2. 高斯判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id6">16.2.1. 一元高斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id7">16.2.2. 多元高斯模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id8">16.3. 逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id9">16.4. 生成模型和判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id10">16.5. 多分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id11">16.6. 其它扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html">17. 回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id2">17.1. 机器学习的概率解释</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id3">17.2. 经典线性回归</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id4">17.2.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id5">17.3. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id6">17.3.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id7">17.4. 凸函数最优化问题</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id8">17.5. 岭回归</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html">18. 分类模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id2">18.1. 生成模型与判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id3">18.2. 线性回归与线性分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id4">18.3. 生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id5">18.3.1. 高斯判别模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id6">18.3.2. 朴素贝叶斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id7">18.3.3. 指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id8">18.4. 判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id9">18.4.1. 逻辑回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id10">18.4.2. 多分类</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id11">18.4.3. 最大熵模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#probit">18.4.4. Probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#noisy-or">18.4.5. Noisy-OR 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id12">18.4.6. 其它指数模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html">19. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id2">19.1. 定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id3">19.1.1. 指数族分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id4">19.1.2. 链接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id5">19.1.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id6">19.2. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id7">19.2.1. 梯度下降法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id8">19.2.2. 牛顿法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#irls">19.2.3. 迭代重加权最小二乘(IRLS)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#goodness-of-fit">19.3. goodness of fit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id9">19.4. 连续值响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id10">19.4.1. 高斯族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#gamma">19.4.2. Gamma族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id12">19.5. 二项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id13">19.6. 多项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id14">19.7. 计数响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id15">19.7.1. 泊松分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id16">19.8. GLM扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html">20. 混合模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id2">20.1. 一般混合模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id3">20.1.1. 模型的有向图表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id5">20.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id8">20.2. 高斯混合模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id9">20.2.1. 模型的表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id10">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#k-means">20.3. K-means</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/32.%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90_42.html">21. 因子分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E4%BA%8C%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B.html">22. 二变量模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/33.LDA_43.html">23. 主题模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/33.LDA_43.html#plsa">23.1. PLSA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/33.LDA_43.html#lda">23.2. LDA</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html">24. 隐马尔可夫模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id2">24.1. 隐马尔可夫模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id3">24.1.1. 马尔可夫模型和朴素贝叶斯模型的关系</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id4">24.2. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/27.%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA_37.html">25. 条件随机场</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/28.%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8_38.html">26. 卡尔曼滤波器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/40.%E9%A1%B9%E7%9B%AE%E5%8F%8D%E5%BA%94%E7%90%86%E8%AE%BA_50.html">27. 项目反应理论</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/41.%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA_51.html">28. 贝叶斯知识追踪</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE.html">29. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../aigc/index.html">AI内容生成（ai-gc）</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html">1. 变分自编码器（Variational Autoencoder）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#evidence-lower-bound-elbo">1.1. 证据下界(Evidence Lower Bound,ELBO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id2">1.2. 编码-解码</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id3">1.3. 总结</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#em">1.3.1. 和EM算法的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#variational">1.3.2. 为什么叫变分（variational）？</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#vq-vae">1.4. VQ-VAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id4">1.5. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html">2. 扩散概率模型（diffusion probabilistic models）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#diffusion-probabilistic-model">2.1. 扩散概率模型（diffusion probabilistic model）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#markovian-hierarchical-variational-autoencoder-mhvae">2.1.1. 马尔科夫分层自编码器（Markovian Hierarchical Variational Autoencoder,MHVAE)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id6">2.1.2. 扩散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id8">2.1.3. 前向-后向</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#elbo">2.1.4. 目标函数（ELBO）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id14">2.1.5. 图片生成（采样）过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#denoising-diffusion-probabilistic-model-ddpm">2.2. 降噪扩散概率模型（Denoising diffusion probabilistic model,DDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#score-based-ddpm">2.3. 基于分数的解释（Score-based DDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id23">2.4. 扩散模型的三种等价表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#improved-denoising-diffusion-probabilistic-models-iddpm">2.5. 改进降噪扩散概率模型（Improved Denoising Diffusion Probabilistic Models,IDDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id24">2.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/ddim.html">3. 去噪扩散隐式模型（Denoising Diffusion Implicit Models,DDIM）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/ddim.html#id1">3.1. 扩散模型的回顾</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/ddim.html#id2">3.2. 非马尔科夫前向过程</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/ddim.html#id4">3.3. 加速采样</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/ddim.html#id5">3.4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html">4. 基于分数的生成模型（Score-based generative models）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id4">4.1. 基于分数的生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#score-matching">4.1.1. 分数匹配算法（Score Matching）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id8">4.1.2. 基于分数的生成模型面临的困难</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id12">4.1.3. 通过加噪的方法估计分布的近似分数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id13">4.1.4. 基于分数的改进采样算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id15">4.1.5. 改进的分数生成模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id17">4.2. 随机微分方程</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id18">4.2.1. 微分方程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id19">4.2.2. 随机微分方程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id20">4.2.3. 基于随机微分方程的生成模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id21">4.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/Guidance.html">5. 条件控制扩散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/Guidance.html#classifier-guidance">5.1. classifier guidance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/Guidance.html#classifier-free-guidance">5.2. Classifier-free guidance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/Guidance.html#clip-guidance">5.3. CLIP Guidance</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Guidance.html#id12">5.3.1. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/dalle2.html">6. DALL·E 2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/dalle2.html#glide">6.1. GLIDE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/dalle2.html#unclip">6.2. unCLIP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/dalle2.html#id3">6.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html">7. 稳定扩散模型（Stable diffusion model）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#latent-diffusion-model-ldm">7.1. 潜在扩散模型（Latent diffusion model,LDM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#stable-diffusion-sd">7.2. 稳定扩散模型（Stable diffusion,SD）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#id3">7.2.1. 推理过程代码</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#id4">7.2.2. 训练过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#id5">7.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/controlnet.html">8. 条件控制之ControlNet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/controlnet.html#id3">8.1. 算法原理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/controlnet.html#id6">8.2. 代码实现</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/controlnet.html#id7">8.3. 最后的总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/controlnet.html#id8">8.4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/dreamBooth.html">9. 条件控制之DreamBooth</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/dreamBooth.html#id2">9.1. DreamBooth 技术</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/dreamBooth.html#id3">9.2. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/imagen.html">10. Imagen</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/imagen.html#id2">10.1. 代码实现解读</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/imagen.html#id3">10.1.1. 第一阶段</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/imagen.html#id4">10.1.2. 第二阶段</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/imagen.html#id5">10.1.3. 第三阶段</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/imagen.html#id6">10.2. Imagen 总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/imagen.html#id7">10.3. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../deepspeed/index.html">deepspeed 详解-源码分析</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html">1. deepspeed - 预备知识</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html#torch-distribute">1.1. torch.distribute</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html#amp">1.2. 自动混合精度AMP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html#cuda-stream-and-event">1.3. cuda Stream and Event</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html#pin-memory">1.4. pin_memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html">2. deepspeed - 总入口</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#id1">2.1. 优化器的初始化</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#id2">2.1.1. 基础优化器</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#zero">2.1.2. 创建 ZeRO 优化器</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#f16">2.1.3. 创建 f16 半精度优化器</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deepspeed/%E6%80%BB%E5%85%A5%E5%8F%A3.html#bf16">2.1.4. 创建 bf16 半精度优化器</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../deepspeed/stage2-%E5%88%9D%E5%A7%8B%E5%8C%96.html">3. stage2 - 初始化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/stage2-%E5%88%9D%E5%A7%8B%E5%8C%96.html#id1">3.1. 配置项初始化</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/stage2-%E5%88%9D%E5%A7%8B%E5%8C%96.html#id2">3.2. 参数分割</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/stage2-%E5%88%9D%E5%A7%8B%E5%8C%96.html#cpu-offload">3.3. cpu offload</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html">4. Stage3 - 参数分割</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#deepspeedzerooptimizer-stage3">4.1. DeepSpeedZeroOptimizer_Stage3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#deepspeedzerooffload">4.2. DeepSpeedZeRoOffload</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#init">4.3. Init 模块</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#convert-to-deepspeed-param">4.3.1. _convert_to_deepspeed_param</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#partition">4.3.2. partition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../deepspeed/stage3-%E5%8F%82%E6%95%B0%E5%88%86%E5%89%B2.html#partition-param-sec">4.3.3. partition_param_sec</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../deepspeed/stage3-hook.html">5. Stage3 - hook 注册</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html">6. Stage3 - 前后向过程</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html#id1">6.1. 参数还原</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html#all-gather-params">6.1.1. __all_gather_params</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html#id2">6.2. 参数重新分割</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../deepspeed/stage3-%E5%89%8D%E5%90%8E%E5%90%91%E8%BF%87%E7%A8%8B.html#release-param">6.2.1. release_param</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../deepspeed/index.html#id2">参考文献</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../audio/index.html">语音技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../audio/feature.html">1. 音频特征</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id2">1.1. 认识声音</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id3">1.2. 认识声波</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id4">1.2.1. 物体的振动以及简谐振动</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id7">1.2.2. 什么是声波</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id8">1.2.3. 纯音和复合音</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#spectrum">1.2.4. 频谱 Spectrum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id10">1.2.5. 名词</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id13">1.3. 语音学</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id14">1.3.1. 发声原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id15">1.3.2. 听觉感应</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id16">1.4. 数字信号处理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id17">1.4.1. 模数转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#wav">1.4.2. 音频文件–WAV</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id18">1.5. 分帧与加窗</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id19">1.5.1. 预加重处理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id20">1.5.2. 分帧与加窗处理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id21">1.6. 声音的感官度量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#sound-pressure-level-spl">1.6.1. 声压与声压级(Sound Pressure Level,SPL)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#intensity-level-il">1.6.2. 声强与声强级(Intensity Level,IL）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id22">1.6.3. 声压与声强的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id23">1.6.4. 响度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id24">1.6.5. 音量计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id25">1.6.6. 频率与音高</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id26">1.7. 时域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id27">1.7.1. 短时能量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id28">1.7.2. 短时平均幅度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id29">1.7.3. 短时过零率</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id31">1.8. 频域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#spectrum-spectrogram">1.8.1. 声谱(spectrum)和时频谱(spectrogram)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#short-time-fourier-transform-stft">1.8.2. 短时傅里叶变换 Short-time Fourier transform (STFT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id33">1.8.3. 倒频谱</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id34">1.8.4. 色谱图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id35">1.9. 小波域特征</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id36">1.9.1. 离散小波域变换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id37">1.9.2. 小波域过零率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id38">1.9.3. 小波域质心</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id39">1.9.4. 小波域子带能量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#mfcc">1.10. 语音识别的音频特征–MFCC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id40">1.11. 参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../edm/index.html">教育领域数据挖掘</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../edm/bkt.html">1. 贝叶斯知识追踪(Bayesian Knowledge Tracing,BKT)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#id1">1.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#hidden-markov-model-hmm">1.2. 隐马尔科夫模型(Hidden Markov Model,HMM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#bayesian-knowledge-tracing">1.3. 贝叶斯知识追踪(Bayesian Knowledge Tracing)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#bkt">1.3.1. BKT的参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#item-response-theory-irt">1.4. 项目反映理论(Item Response Theory,IRT)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#bktirt">1.5. BKT结合IRT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#id5">1.6. 实验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id6">1.6.1. 数据集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id7">1.6.2. 实验方法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id8">1.6.3. 实验结果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id9">1.6.4. 项目代码</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#id10">1.7. 未来工作</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id11">1.7.1. 题目难度的计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#irt">1.7.2. 多参数IRT模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id12">1.7.3. 参数估计算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#id13">1.8. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../nlp/index.html">自然语言处理</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html">1. 文本去重</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id2">1.1. 背景</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id3">1.2. 技术思路</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id4">1.3. 相似（距离）算法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#euclidean-distance">1.3.1. 欧氏距离（Euclidean Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minkowski-distance">1.3.2. 闵科夫斯基距离（Minkowski Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#manhattan-distance">1.3.3. 曼哈顿距离（Manhattan Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#chebyshev-distance">1.3.4. 切比雪夫距离（Chebyshev Distance ）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#mahalanobis-distance">1.3.5. 马氏距离(Mahalanobis Distance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#cosine-similarity">1.3.6. 余弦夹角相似度(Cosine Similarity)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#hamming-distance">1.3.7. 汉明距离（Hamming Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#jaccard">1.3.8. Jaccard 系数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id5">1.3.9. 编辑距离</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id6">1.3.10. 最长公共字串</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id7">1.3.11. 最长公共子序列</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id8">1.4. 文本去重</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#kshingle">1.4.1. KShingle算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minhash">1.4.2. Minhash算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#simhash">1.4.3. simhash</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#ksentence">1.4.4. KSentence算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id9">1.5. 话术去重</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nlp/bert/content.html">2. Attention&amp;Transformer&amp;Bert 简介</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/bert/content.html#transformer">2.1. Transformer 从宏观到微观</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#seq2seq">2.1.1. seq2seq</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#id1">2.1.2. 模型的输入</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/bert/content.html#self-attention">2.2. Self-Attention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#id2">2.2.1. 什么是注意力？</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#id3">2.2.2. 加权求和</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#id4">2.2.3. 位置编码</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#multi-head">2.2.4. 多头注意力（Multi-head）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/bert/content.html#attention">2.3. Attention 机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/bert/content.html#id5">2.4. 其它参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../rst_tutorial/latex.html">latex demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../rst_tutorial/latex.html#latex">latex</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rst_tutorial/latex.html#how-to-write-an-m-x-n-matrix-in-latex">How to write an m x n matrix in LaTeX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-big-parentheses">With big parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-parentheses">With parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-brackets">With brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#latex-matrix-with-no-bracket">LateX matrix with no bracket</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-vertical-bar-brackets">With vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-curly-brackets">with curly brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-double-vertical-bar-brackets">with double vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#small-inline-matrix">small inline matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#examples-matrix-2-x-2-in-latex">Examples matrix 2 x 2 in LaTeX</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../rst_tutorial/graphviz.html">graphviz demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../rst_tutorial/graphviz.html#id1">布局</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rst_tutorial/graphviz.html#id2">其它</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">读书笔记</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">《统计因果推断推理入门》读书笔记</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html">1. 第三章 干预的效果</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id2">1.1. 第3.1节 干预</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id3">1.2. 第3.2节 校正公式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id8">1.3. 第3.3节 后门准则</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html">《深度学习推荐系统》读书笔记</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id2">重点</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id3">冷启动</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id4">探索与利用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id5">召回层的主要策略</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#embedding">协同过滤 &amp; Embedding 向量</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="移动版导航菜单" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">张振虎的博客</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="页面导航">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">广义线性模型</a></li>
      <li class="breadcrumb-item active"><span class="section-number">3. </span>推断与检验</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/glm/source/推断与检验/content.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="ch-influence-and-test">
<span id="id1"></span><h1><span class="section-number">3. </span>推断与检验<a class="headerlink" href="#ch-influence-and-test" title="此标题的永久链接"></a></h1>
<p>上一章我们介绍了统计推断中应用最广泛的最大似然估计，
然而当我们得到一个参数估计值后，
我们期望知道这个估计值靠不靠谱，它与参数的真实值又相差多少，
本章我们讨论如何评价一个参数估计值的好坏。
在正式讨论估计值评价方法之前，需要先熟悉统计推断中一些基本知识，
比如充分统计量、费歇尔信息、抽样分布等等，
要理解后面的内容需要对这些基础概念十分熟悉才行，
希望读者能花些精力理解这些基础知识，
如果仅靠本书的内容还不能理解，
请辅助参考其它概率与统计学的资料。</p>
<section id="id2">
<h2><span class="section-number">3.1. </span>统计量和充分统计量<a class="headerlink" href="#id2" title="此标题的永久链接"></a></h2>
<p>我们首先讨论统计量以及充分统计量的概念。</p>
<p>假设有一个独立同分布的观测样本集 <span class="math notranslate nohighlight">\(\mathcal{D} = \{x_1,x_2,\dots,x_N\}\)</span> ，
样本集中的样本都是从同一个概率分布
<span class="math notranslate nohighlight">\(P(X;\theta)\)</span> 采样得到，其中 <span class="math notranslate nohighlight">\(\theta\)</span> 是这个分布的未知参数，参数空间为 <span class="math notranslate nohighlight">\(\Theta\)</span> 。
上一章已经讲过，我们可以使用最大似然估计参数 <span class="math notranslate nohighlight">\(\theta\)</span>，
并且参数的估计值是一个关于样本的函数 <span class="math notranslate nohighlight">\(\hat{\theta} = g(\mathcal{D})\)</span> 。
在统计学中，把观测样本的函数称为 <strong>统计量</strong>。</p>
<dl class="simple glossary">
<dt id="term-0">统计量<a class="headerlink" href="#term-0" title="此术语词汇的永久链接"></a></dt><dd><p>正式地，任意观测样本的实值函数 <span class="math notranslate nohighlight">\(T=g(\mathcal{D})\)</span> 都称为一个 <strong>统计量(statistic)</strong> 。
一个统计量就是一个关于样本集的函数（允许是向量形式的函数)，<strong>在这个函数中不能有任何未知参数</strong> 。
比如，样本的均值 <span class="math notranslate nohighlight">\(\bar{x}=\frac{1}{N}\sum_i^N x_i\)</span> ，最大值 <span class="math notranslate nohighlight">\(max(\mathcal{D})\)</span>
， 中位数 <span class="math notranslate nohighlight">\(median(\mathcal{D})\)</span> 以及 <span class="math notranslate nohighlight">\(f(\mathcal{D})=4\)</span> 都是统计量。
但是 <span class="math notranslate nohighlight">\(x_1+\mu\)</span> （ <span class="math notranslate nohighlight">\(\mu\)</span> 是未知参数）就不是统计量。</p>
</dd>
</dl>
<p><strong>参数估计值是统计量</strong></p>
<p>在进行参数估计时，我们能利用的只有观测样本集，因此观测样本是我们进行参数估计的唯一信息源。
也就是说，我们能利用的有关参数的所有可用信息都包含在观察样本中。
<strong>因此，我们获得的参数估计量始终是观测值的函数，即参数估计量是统计量。</strong>
从某种意义上讲，该过程可以被认为是“压缩”原始观察数据：最初我们有N个数字，
但是经过这个“压缩”之后，我们只有1个数字 。
这种“压缩”总是使我们失去有关该参数的信息，决不能使我们获得更多的信息。
最好的情况是，该“压缩”结果包含的信息量与N个观测值中包含的信息量相同，
也就是该“压缩”结果包含的信息量已经是关于参数的信息的全部。</p>
<p><strong>统计量是随机变量</strong></p>
<p>观测样本集 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> 可以看成是 <span class="math notranslate nohighlight">\(N\)</span> 个服从相同概率分布的随机变量的独立采样，
每重新进行一次采样，都会得到不同的样本集，也就是说样本集 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>
本身也是随机（不同采样得到不一样的值）的，因此样本集 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>
可以看做是由 <span class="math notranslate nohighlight">\(N\)</span> 个随机变量组成，记作 <span class="math notranslate nohighlight">\(\mathcal{D}=\{X_1,X_2,\cdots,X_N\}\)</span>
，<strong>大写表示随机变量</strong> 。
统计量作为样本集 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> 的函数，也就相当于是 <span class="math notranslate nohighlight">\(N\)</span> 个随机变量的函数。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-0">
<span class="eqno">(3.1.1)<a class="headerlink" href="#equation-glm-source-content-0" title="此公式的永久链接"></a></span>\[T = g(\mathcal{D}) = g(X_1,X_2,\cdots,X_N)\]</div>
<p>在第一章我们就讲过，
<strong>随机变量的函数仍然是一个随机量，那么作为样本函数的统计量自然就是随机变量，而参数估计值是统计量，</strong>
<strong>因此参数估计值是一个随机变量，所以有时可以称参数估计值为参数估计量</strong>。</p>
<p><strong>充分统计量</strong></p>
<p>假设有一个统计量 <span class="math notranslate nohighlight">\(T(\mathcal{D})\)</span> ，并且 <span class="math notranslate nohighlight">\(t\)</span> 是 <span class="math notranslate nohighlight">\(T\)</span> 的一个特定值，
如果在给定 <span class="math notranslate nohighlight">\(T=t\)</span> 的条件下，我们就能计算出样本的联合概率 <span class="math notranslate nohighlight">\(P(X_1,X_2,\dots,X_N|T=t)\)</span> ，
而不再依赖参数 <span class="math notranslate nohighlight">\(\theta\)</span> ，这个统计量就是 <strong>充分统计量(sufficient statistic)</strong> 。</p>
<p>换种说法，在给定充分统计量 <span class="math notranslate nohighlight">\(T=t\)</span> 条件下，就能确定参数 <span class="math notranslate nohighlight">\(\theta\)</span> 的值，而不再需要额外的信息，
我们可以设想只保留 <span class="math notranslate nohighlight">\(T\)</span> 并丢弃所有 <span class="math notranslate nohighlight">\(X_i\)</span>，而不会丢失参数的任何信息！
从上面的直观分析中，我们可以看到充分统计量“吸收”了样本中包含的有关 <span class="math notranslate nohighlight">\(\theta\)</span> 的所有可用信息。
这个概念是R.A. Fisher在1922年提出的。</p>
<p>充分性的概念是为了回答以下问题而提出的：
是否存在一个统计量，即函数 <span class="math notranslate nohighlight">\(T(X_1,\dots,X_N)\)</span> ，其中包含样本中有关 <span class="math notranslate nohighlight">\(\theta\)</span> 的所有信息？
如果这样，则可以将原始数据减少或压缩到该统计信息而不会丢失信息。
例如，考虑一系列成功概率未知的独立伯努利试验。
我们可能有一种直觉的感觉，成功次数包含样本中有关 <span class="math notranslate nohighlight">\(\theta\)</span> 的所有信息，
而成功发生的顺序没有提供有关 <span class="math notranslate nohighlight">\(\theta\)</span> 的任何其他信息。
对于高斯分布，（样本）期望和（样本）协方差矩阵就是它的充分统计量，因为如果这两个参数已知，
就可以唯一确定一个高斯分布，而对于高斯分布的其他统计量，例如振幅、高阶矩等在这种时候都是多余的。</p>
<aside class="topic">
<p class="topic-title">示例：</p>
<p>假设 <span class="math notranslate nohighlight">\(X_1,X_2,\dots,X_N\)</span> 是 <span class="math notranslate nohighlight">\(N\)</span> 个独立同分布伯努利变量的采样样本，其中 <span class="math notranslate nohighlight">\(P(X_i=1)=\theta\)</span> 。
我们将验证 <span class="math notranslate nohighlight">\(T=\sum_{i=1}^N X_i\)</span> 是 <span class="math notranslate nohighlight">\(\theta\)</span> 的一个充分统计量。</p>
</aside>
<aside class="topic">
<p class="topic-title">证明：</p>
<p>由于 <span class="math notranslate nohighlight">\(X_i\)</span> 只能取值为 <span class="math notranslate nohighlight">\(0\)</span> 或者 <span class="math notranslate nohighlight">\(1\)</span> ，所以 <span class="math notranslate nohighlight">\(T=t\)</span> 可以看作是在N条样本中 <span class="math notranslate nohighlight">\(X_i=1\)</span> 的次数。
根据贝叶斯定理有：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-1">
<span class="eqno">(3.1.2)<a class="headerlink" href="#equation-glm-source-content-1" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}P(X_1,X_2,\dots,X_N|T=t) &amp;= \frac{P(X_1,\dots,X_N)}{P(T=t)}\\&amp;= \frac{\prod_i \theta^{X_i} (1-\theta)^{(1-X_i)} }{P(T=t)}\\&amp;= \frac{ \theta^t (1-\theta)^{N-t} }{P(T=t)}\end{aligned}\end{align} \]</div>
<p>现在看分母部分，<span class="math notranslate nohighlight">\(T\)</span> 的含义是在 <span class="math notranslate nohighlight">\(N\)</span> 次试验中 <span class="math notranslate nohighlight">\(1\)</span> 的数量，
很明显这是二项式分布，有 <span class="math notranslate nohighlight">\(N\)</span> 次试验，单次成功(为1)的概率为 <span class="math notranslate nohighlight">\(\theta\)</span> ，
一共成功 <span class="math notranslate nohighlight">\(t\)</span> 次（ <span class="math notranslate nohighlight">\(1\)</span> 的数量为 <span class="math notranslate nohighlight">\(t\)</span> ）的概率分布为 <span class="math notranslate nohighlight">\(T=\binom{N}{t}\theta^t (1-\theta)^{N-t}\)</span>
，其中 <span class="math notranslate nohighlight">\(\binom{N}{t}\)</span> 是组合数，从 <span class="math notranslate nohighlight">\(N\)</span> 个结果中任意选出 <span class="math notranslate nohighlight">\(t\)</span> 个的方法数。把分母代入上式：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-2">
<span class="eqno">(3.1.3)<a class="headerlink" href="#equation-glm-source-content-2" title="此公式的永久链接"></a></span>\[P(X_1,X_2,\dots,X_N|T=t) = \frac{ \theta^t (1-\theta)^{N-t} }{\binom{N}{t}\theta^t (1-\theta)^{N-t}}
= \frac{1}{\binom{N}{t}}\]</div>
<p>最终发现，样本在给定 <span class="math notranslate nohighlight">\(T=t\)</span> 的条件下的联合概率与参数 <span class="math notranslate nohighlight">\(\theta\)</span> 无关，也就是说在确定了 <span class="math notranslate nohighlight">\(T\)</span> 之后，
就可以直接得到样本的联合概率，而不再依赖参数 <span class="math notranslate nohighlight">\(\theta\)</span> 。</p>
</aside>
<p>在很多问题中，<strong>参数的最大似然估计量就是一个充分统计量</strong> ，比如，伯努利实验的参数估计量就是一个充分统计量
<span class="math notranslate nohighlight">\(\hat{\theta}_{ML}=\frac{1}{N}\sum_{i=1}^N X_i=\bar{X}\)</span> 。
同样，贝叶斯参数估计量也是一个充分统计量。最大似然估计量和贝叶斯估计量都是充分统计量的一个函数，
它们”吸收”了观测样本中关于参数的所有有用信息。</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>根据统计量的定义：样本的一个函数可以称为统计量，样本的求和 <span class="math notranslate nohighlight">\(\sum_{i=1}^N X_i\)</span> ，
样本的均值 <span class="math notranslate nohighlight">\(\frac{1}{N}\sum_{i=1}^N X_i\)</span> 都可以称为统计量。
所以，似然估计量 <span class="math notranslate nohighlight">\(\hat{\theta}_{ML}=\frac{1}{N}\sum_{i=1}^N X_i\)</span> 可以整体看做一个充分统计量（样本均值统计量），
也可以看做是充分统计量（求和统计量） <span class="math notranslate nohighlight">\(\sum_{i=1}^N X_i\)</span> 的一个函数。</p>
</div>
</section>
<section id="ch-sample-distribution">
<span id="id3"></span><h2><span class="section-number">3.2. </span>抽样分布<a class="headerlink" href="#ch-sample-distribution" title="此标题的永久链接"></a></h2>
<p>在统计学中，把需要调查或者研究的某一现象或者事物的全部数据称为统计总体，或简称 <strong>总体(population)</strong>。
比如，我们要研究中国人的身高分布，那么全国14亿人的身高数据就是总体(population)，
这14亿身高数据所属的数据分布称为 <strong>总体分布 (population distribution)</strong>，
其中每一个人的身高数据，即单个数据称为个体(individual)。
然而在实际中，我们不可能得到14亿的全部数据，也就是 <strong>总体数据通常是无法得知的</strong> 。
这时，可以选择抽样(sampling)，即从总体当中随机抽取出部分个体，然后得到这部分抽样个体的数据，
一次抽样的结果称为一份样本(sample)。
比如，从14亿的人群中随机抽取出1万的个体，然后去测量这1万人的身高数据，
这样就得到了一份包含1万个数据的样本，样本的容量(sample size)，或者说样本的大小，是1万。
注意样本(sample)和个体(individual)的区别，样本(sample)是一次抽样的结果，包含多个个体(individual)数据，
一份样本中包含的个体数据的数量称为本容量(sample size)。</p>
<p>随机变量抽样样本的函数称为统计量，统计量也是随机变量。
既然是随机变量，那统计量也定然会服从某种概率分布，
<strong>在统计学中，把统计量的概率分布统称为抽样分布（sample distribution）</strong>。
抽样分布也称统计量分布、随机变量函数分布，是指样本统计量的分布。
注意，抽样分布并不是某个具体的概率分布，而是一个统称，
其实就是”抽样样本的函数（统计量）的概率分布”的简称，
比较常见的抽样分布是正态分布、学生t分布、卡方分布、F分布等，
一个统计量具体是哪种抽样分布，这要取决于总体分布（抽样样本所属的概率分布）是什么以及统计量的函数是什么。</p>
<p>假设需要调查国人的身高情况，想要知道全国人民身高的均值和方差。
但是显然不可能测量得到全国人民的身高数据，然后计算得到均值和方差。
在统计学上，通常通过抽样解决这类问题。
根据经验，我们假设全国人民的身高数据服从正态分布，
记为 <span class="math notranslate nohighlight">\(X \sim N(\mu,\sigma^2)\)</span> ，
变量 <span class="math notranslate nohighlight">\(X\)</span> 就是总体正态变量，
<span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 分别表示总体的期望和方差。
然后从总体中，随机抽取一份包含1万个个体的样本，并且依次测量出这1万个个体的身高数据，
记为 <span class="math notranslate nohighlight">\(\mathcal{D}=\{X_1,X_2,\dots,X_N \},N=100000\)</span> ，
这就相当于从总体正态分布中取得一个独立同分布的采样。
现在我们要利用这个样本估计出总体的期望参数 <span class="math notranslate nohighlight">\(\mu\)</span> 和方差参数 <span class="math notranslate nohighlight">\(\sigma^2\)</span>
。</p>
<p>显然我们可以应用最大似然估计得到参数的估计值，这里我们直接使用 <a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-gaussian-ml"><span class="std std-numref">节 2.1.3</span></a> 的结论，
期望参数 <span class="math notranslate nohighlight">\(\mu\)</span> 和方差参数 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 的最大似然估计量分别是</p>
<div class="math notranslate nohighlight" id="equation-eq-estimator-eval-014">
<span class="eqno">(3.2.1)<a class="headerlink" href="#equation-eq-estimator-eval-014" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\hat{\mu} &amp;= \bar{X} = \frac{1}{N} \sum_{i=1}^N X_i\\\hat{\sigma}^2 &amp;= \frac{1}{N-1} \sum_{i=1}^N (X_i-\bar{X})^2\end{aligned}\end{align} \]</div>
<p><strong>显然参数的最大似然估计值是一个样本的函数（统计量），因此它是一个随机变量</strong>。
现在我们看下参数估计量 <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 和 <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span>
的抽样分布分别是什么。</p>
<section id="ch-sample-distribution-normal">
<span id="id4"></span><h3><span class="section-number">3.2.1. </span>正态分布<a class="headerlink" href="#ch-sample-distribution-normal" title="此标题的永久链接"></a></h3>
<p>我们先看期望参数估计量的抽样分布，
总体正态分布的期望（均值）参数的似然估计量
<span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 就等于样本的均值统计量。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-3">
<span class="eqno">(3.2.2)<a class="headerlink" href="#equation-glm-source-content-3" title="此公式的永久链接"></a></span>\[\hat{\mu} = \bar{X} = \frac{1}{N} \sum_{i=1}^N X_i\]</div>
<p>抽样样本集中每一条样本 <span class="math notranslate nohighlight">\(X_i\)</span> 都是正态分布随机变量，
根据第一章讲的正态分布的性质： <em>多个正态随机变量的线性组合结果仍然是一个正态随机变量</em> ，
显然估计量 <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 是 <strong>服从正态分布的</strong> 。
并且根据期望的计算性质，可知估计量的均值为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-4">
<span class="eqno">(3.2.3)<a class="headerlink" href="#equation-glm-source-content-4" title="此公式的永久链接"></a></span>\[\mathbb{E}[\hat{\mu}] =\mathbb{E}[ \frac{1}{N} \sum_{i=1}^N X_i ]
= \frac{1}{N} \sum_{i=1}^N \mathbb{E}[X_i]
= \frac{1}{N} \sum_{i=1}^N \mu
= \mu\]</div>
<p>参数估计量  <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 的期望就等于总体的期望 <span class="math notranslate nohighlight">\(\mu\)</span>
，这和我们的直观认知是一致的。
现在我们看下参数估计量  <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 的方差计算</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-5">
<span class="eqno">(3.2.4)<a class="headerlink" href="#equation-glm-source-content-5" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}V(\hat{\mu}) &amp;= V (\frac{1}{N} \sum_{i=1}^N X_i)\\&amp;= \frac{1}{N^2} \sum_{i=1}^N V(X_i)\\&amp;= \frac{N \sigma^2}{N^2}\\&amp;= \frac{\sigma^2}{N}\end{aligned}\end{align} \]</div>
<p>最终估计量 <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 的抽样分布是均值为 <span class="math notranslate nohighlight">\(\mu\)</span>，方差为 <span class="math notranslate nohighlight">\(\sigma^2/N\)</span> 的正态分布，
记作</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-6">
<span class="eqno">(3.2.5)<a class="headerlink" href="#equation-glm-source-content-6" title="此公式的永久链接"></a></span>\[\hat{\mu} \sim N(\mu,\frac{\sigma^2}{N})\]</div>
<p>也可以记作</p>
<div class="math notranslate nohighlight" id="equation-eq-estimator-eval-015">
<span class="eqno">(3.2.6)<a class="headerlink" href="#equation-eq-estimator-eval-015" title="此公式的永久链接"></a></span>\[Z = \frac{\hat{\mu} - \mu}{ \frac{\sigma}{\sqrt{N} }}  =  \frac{\bar{X} - \mu}{ \frac{\sigma}{\sqrt{N} }} \sim N(0,1)\]</div>
<p>通常会把这个统计量称为 <span class="math notranslate nohighlight">\(Z\)</span> 统计量，<span class="math notranslate nohighlight">\(Z\)</span> 统计量是服从标准正态分布的，
但是注意，要想得到这个统计量需要总体的标准差 <span class="math notranslate nohighlight">\(\sigma\)</span> 是已知的才行。</p>
</section>
<section id="t">
<span id="ch-sample-distribution-t"></span><h3><span class="section-number">3.2.2. </span>学生t分布<a class="headerlink" href="#t" title="此标题的永久链接"></a></h3>
<p>上一节我们讲到总体正态分布的期望估计量（样本均值统计量）的抽样分布是正态分布 <span class="math notranslate nohighlight">\(N(\mu,\frac{\sigma^2}{N})\)</span>
，抽样分布的方差是 <span class="math notranslate nohighlight">\(\frac{\sigma^2}{N}\)</span>。
其中含有总体的方差，然而很多时候总体方差 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 是未知的，
此时需要找一个总体方差 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 的替代值。</p>
<p>显然可以使用方差估计值（样本的方差统计量）作为总体方差 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 的近似替代，
样本的方差为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-7">
<span class="eqno">(3.2.7)<a class="headerlink" href="#equation-glm-source-content-7" title="此公式的永久链接"></a></span>\[S_N = \frac{1}{N} \sum_{i=1}^N (X_i-\bar{X})^2 = \hat{\sigma}^2\]</div>
<p>根据 <a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#ch-probability-t"><span class="std std-numref">节 1.8.7</span></a> 讲的t-分布的定义，
如下统计量 <span class="math notranslate nohighlight">\(T\)</span> 是服从自由度为 <span class="math notranslate nohighlight">\(N-1\)</span> 的t-分布。</p>
<div class="math notranslate nohighlight" id="equation-eq-estimator-eval-016">
<span class="eqno">(3.2.8)<a class="headerlink" href="#equation-eq-estimator-eval-016" title="此公式的永久链接"></a></span>\[ T = \frac{\hat{\mu} - \mu}{ \frac{\hat{\sigma}}{\sqrt{N} }} \sim T(N-1)\]</div>
<p>可以对比下 <a class="reference internal" href="#equation-eq-estimator-eval-015">公式(3.2.6)</a> 和 <a class="reference internal" href="#equation-eq-estimator-eval-016">公式(3.2.8)</a> 的区别，
当总体方差未知的时候，服从标准正态分布的 <span class="math notranslate nohighlight">\(Z\)</span> 统计量无法得到，
此时可以使用 <span class="math notranslate nohighlight">\(T\)</span> 统计量作为替代。</p>
<p>但是如果样本数量超过 <span class="math notranslate nohighlight">\(30\)</span>，就可以不使用 <span class="math notranslate nohighlight">\(T\)</span> 统计量，
而是直接使用 <span class="math notranslate nohighlight">\(Z\)</span> 统计量。
在 <a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#ch-probability-t"><span class="std std-numref">节 1.8.7</span></a> 节讲过，
当样本数量超过 <span class="math notranslate nohighlight">\(30\)</span> 的时候，t分布和标准正态分布基本是重合的，
两者没啥区别，也就是说此时使用方差估计值（样本方差）作为 <span class="math notranslate nohighlight">\(Z\)</span>
统计量中总体方差的替代也是可以。即当样本数量 <span class="math notranslate nohighlight">\(N&gt;30\)</span> 时，
如下 <span class="math notranslate nohighlight">\(Z\)</span> 统计量的抽样分布近似成立。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-8">
<span class="eqno">(3.2.9)<a class="headerlink" href="#equation-glm-source-content-8" title="此公式的永久链接"></a></span>\[Z = \frac{\hat{\mu} - \mu}{ \frac{\hat{\sigma}}{\sqrt{N} }} \sim N(0,1), \quad N &gt; 30\]</div>
</section>
<section id="id5">
<h3><span class="section-number">3.2.3. </span>卡方分布<a class="headerlink" href="#id5" title="此标题的永久链接"></a></h3>
<p>现在看下方差估计量 <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> 的抽样分布，
总体正态分布 <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,\sigma^2)\)</span> 方差参数的无偏计量为</p>
<div class="math notranslate nohighlight" id="equation-eq-estimator-eval-017">
<span class="eqno">(3.2.10)<a class="headerlink" href="#equation-eq-estimator-eval-017" title="此公式的永久链接"></a></span>\[ \hat{\sigma}^2 = \frac{1}{N-1} \sum_{i=1}^N (X_i-\bar{X})^2\]</div>
<p>方差估计量（样本方差）的抽样分布是和卡方分布的相关的，有如下（渐近）分布成立。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-9">
<span class="eqno">(3.2.11)<a class="headerlink" href="#equation-glm-source-content-9" title="此公式的永久链接"></a></span>\[\frac{(N-1) \hat{\sigma}^2}{\sigma^2}  \sim  \chi^2(N-1)\]</div>
<p><strong>证明过程如下：</strong></p>
<p>根据卡方分布的定义：多个标准正态分布的平方和服从卡方分布，可知如下变量 <span class="math notranslate nohighlight">\(Z\)</span> 是自由度为 <span class="math notranslate nohighlight">\(N\)</span> 的卡方随机变量</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-10">
<span class="eqno">(3.2.12)<a class="headerlink" href="#equation-glm-source-content-10" title="此公式的永久链接"></a></span>\[W = \sum_{i=1}^N \left ( \frac{X_i-\mu}{\sigma} \right )^2\]</div>
<p>分子部分加上同时减去一个 <span class="math notranslate nohighlight">\(\bar{X}\)</span>，<span class="math notranslate nohighlight">\(Z\)</span> 保持不变。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-11">
<span class="eqno">(3.2.13)<a class="headerlink" href="#equation-glm-source-content-11" title="此公式的永久链接"></a></span>\[W = \sum_{i=1}^N \left (  \frac{(X_i -\bar{X})+(\bar{X}-\mu)}{\sigma} \right )^2\]</div>
<p>然后把平方展开</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-12">
<span class="eqno">(3.2.14)<a class="headerlink" href="#equation-glm-source-content-12" title="此公式的永久链接"></a></span>\[W = \sum_{i=1}^N \left ( \frac{X_i -\bar{X}}{\sigma} \right )^2
+ \sum_{i=1}^N \left ( \frac{\bar{X}-\mu}{\sigma} \right )^2
+2 \left ( \frac{\bar{X}-\mu}{\sigma^2}  \right ) \sum_{i=1}^N (X_i -\bar{X})\]</div>
<p>上述等式右边的最后一项是 <span class="math notranslate nohighlight">\(0\)</span>。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-13">
<span class="eqno">(3.2.15)<a class="headerlink" href="#equation-glm-source-content-13" title="此公式的永久链接"></a></span>\[\sum_{i=1}^N (X_i -\bar{X}) = N \bar{X} - N \bar{X} = 0\]</div>
<p>因此 <span class="math notranslate nohighlight">\(Z\)</span> 简化成</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-14">
<span class="eqno">(3.2.16)<a class="headerlink" href="#equation-glm-source-content-14" title="此公式的永久链接"></a></span>\[W = \sum_{i=1}^N \left ( \frac{X_i -\bar{X}}{\sigma} \right )^2
+ N \left ( \frac{\bar{X}-\mu}{\sigma} \right )^2\]</div>
<p>然后可以把方差的估计量 <a class="reference internal" href="#equation-eq-estimator-eval-017">公式(3.2.10)</a> 代入到等式中。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-15">
<span class="eqno">(3.2.17)<a class="headerlink" href="#equation-glm-source-content-15" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned} W &amp;=   \frac{N-1}{(N-1) \sigma^2} \sum_{i=1}^N  \left ( X_i -\bar{X} \right )^2
+ \sum_{i=1}^N \left ( \frac{\bar{X}-\mu}{\sigma} \right )^2\\&amp;=   \frac{(N-1) \hat{\sigma}^2}{\sigma^2}  + \frac{ N (\bar{X}-\mu)^2 }{\sigma^2}\end{aligned}\end{align} \]</div>
<p>移项可得</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-16">
<span class="eqno">(3.2.18)<a class="headerlink" href="#equation-glm-source-content-16" title="此公式的永久链接"></a></span>\[\frac{(N-1) \hat{\sigma}^2}{\sigma^2} = W - \underbrace{ \frac{ N (\bar{X}-\mu)^2 }{\sigma^2}}_{\text{标准正态分布的平方}}\]</div>
<p>又因为有 <span class="math notranslate nohighlight">\(\bar{X} \sim \mathcal{N}(\mu,\sigma^2/N)\)</span>，等式右侧的第二项是一个标准正态分布的平方。
显然等式右侧变成一个自由度为 <span class="math notranslate nohighlight">\(N-1\)</span> 的卡方分布。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-17">
<span class="eqno">(3.2.19)<a class="headerlink" href="#equation-glm-source-content-17" title="此公式的永久链接"></a></span>\[\frac{(N-1) \hat{\sigma}^2}{\sigma^2} =\frac{ \sum_{i=1}^N (X_i-\bar{X})^2 }{\sigma^2}  \sim \chi^2(N-1)\]</div>
<p>注意，如果方差估计量用的似然估计量（有偏估计）</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-18">
<span class="eqno">(3.2.20)<a class="headerlink" href="#equation-glm-source-content-18" title="此公式的永久链接"></a></span>\[\hat{\sigma}^2_{ML} = \frac{1}{N} \sum_{i=1}^N (X_i-\bar{X})^2  \quad \text{有偏估计}\]</div>
<p>卡方统计量就变成</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-19">
<span class="eqno">(3.2.21)<a class="headerlink" href="#equation-glm-source-content-19" title="此公式的永久链接"></a></span>\[\frac{N \hat{\sigma}^2}{\sigma^2} =\frac{ \sum_{i=1}^N (X_i-\bar{X})^2 }{\sigma^2}  \sim \chi^2(N-1)\]</div>
</section>
</section>
<section id="id6">
<h2><span class="section-number">3.3. </span>极限理论<a class="headerlink" href="#id6" title="此标题的永久链接"></a></h2>
<p>我们已经理解了总体、样本、统计量、抽样分布的概念，
并且知道正态分布的样本均值统计量的抽样分布是正态分布。
抽样样本集 <span class="math notranslate nohighlight">\(\mathcal{D}=\{X_1,X_2,\cdots,X_N\}\)</span>
的另一个相关因子是样本的数量 <span class="math notranslate nohighlight">\(N\)</span>
，正所谓量变引起质变，
本节我们讨论当 <span class="math notranslate nohighlight">\(N\)</span> 极大时样本统计量会呈现出什么性质。</p>
<p>设 <span class="math notranslate nohighlight">\(X_1,X_2,\cdots,X_N\)</span> 为一个独立同分布的随机变量序列，
其公共分布的均值为 <span class="math notranslate nohighlight">\(\mu\)</span>， 方差为 <span class="math notranslate nohighlight">\(\sigma^2\)</span>。
定义</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-20">
<span class="eqno">(3.3.1)<a class="headerlink" href="#equation-glm-source-content-20" title="此公式的永久链接"></a></span>\[S_N = X_1+X_2+\cdots+X_N\]</div>
<p>为这个随机变量序列之和，本节的极限理论研究 <span class="math notranslate nohighlight">\(S_N\)</span>
以及与 <span class="math notranslate nohighlight">\(S_N\)</span> 相关的变量在 <span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span>
时的极限性质。</p>
<p>由随机变量序列的各项之间的相互独立性可知</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-21">
<span class="eqno">(3.3.2)<a class="headerlink" href="#equation-glm-source-content-21" title="此公式的永久链接"></a></span>\[V(S_N) = V(X_1) + V(X_2) + \cdots +V(X_N) = N \sigma^2\]</div>
<p>显然当 <span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span> 时，<span class="math notranslate nohighlight">\(S_N\)</span> 是发散的，不可能有极限。
但是 <em>样本均值统计量</em></p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-22">
<span class="eqno">(3.3.3)<a class="headerlink" href="#equation-glm-source-content-22" title="此公式的永久链接"></a></span>\[M_N = \frac{X_1+X_2+\cdots+X_N}{N} = \frac{S_N}{N}\]</div>
<p>却不同，经过简单计算可知</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-23">
<span class="eqno">(3.3.4)<a class="headerlink" href="#equation-glm-source-content-23" title="此公式的永久链接"></a></span>\[\mathbb{E}[M_N] = \mu, \quad V(M_N) = \frac{\sigma^2}{N}\]</div>
<p>当 <span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span> 时，
样本均值统计量 <span class="math notranslate nohighlight">\(M_N\)</span> 的方差趋近于 <span class="math notranslate nohighlight">\(0\)</span>
。方差趋近于 <span class="math notranslate nohighlight">\(0\)</span> 意味着 <span class="math notranslate nohighlight">\(M_N\)</span> 就与 <span class="math notranslate nohighlight">\(\mu\)</span> 特别接近。
这种现象就是大数定律的内容。
按通常的解释，当样本量 <span class="math notranslate nohighlight">\(N\)</span> 很大的时候，
从 <span class="math notranslate nohighlight">\(X\)</span> 抽取的样本平均值 <span class="math notranslate nohighlight">\(M_N\)</span> 就是变量 <span class="math notranslate nohighlight">\(X\)</span> 的平均值 <span class="math notranslate nohighlight">\(\mathbb{E}[X]\)</span>
。这里对 <span class="math notranslate nohighlight">\(X\)</span> 属于哪种概率分布并没有限制，非正态分布也符合这个定律。</p>
<p>下面考虑另一个随机变量，用 <span class="math notranslate nohighlight">\(S_N\)</span> 减去 <span class="math notranslate nohighlight">\(N\mu\)</span>，
可以得到零均值随机变量序列 <span class="math notranslate nohighlight">\(S_N - N\mu\)</span>，
然后再除以 <span class="math notranslate nohighlight">\(\sigma \sqrt{N}\)</span> ，
就得到随机变量序列</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-24">
<span class="eqno">(3.3.5)<a class="headerlink" href="#equation-glm-source-content-24" title="此公式的永久链接"></a></span>\[Z_N = \frac{S_N - N \mu}{\sigma \sqrt{N}} = \frac{\bar{X} - \mu}{\frac{\sigma}{ \sqrt{N}} }\]</div>
<p>易证明</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-25">
<span class="eqno">(3.3.6)<a class="headerlink" href="#equation-glm-source-content-25" title="此公式的永久链接"></a></span>\[\mathbb{E}[Z_N] = 0, \quad V(Z_N) = 1\]</div>
<p>因为 <span class="math notranslate nohighlight">\(Z_N\)</span> 的均值和方差不依赖于样本容量 <span class="math notranslate nohighlight">\(N\)</span>
，所以它的分布既不发散，也不收敛于一点。
<strong>中心极限定理</strong> 就研究 <span class="math notranslate nohighlight">\(Z_N\)</span> 的分布的渐近性质，并得出结论：
当 <span class="math notranslate nohighlight">\(N\)</span> 充分大的时候，<span class="math notranslate nohighlight">\(Z_N\)</span> 的分布就接近标准正态分布。</p>
<section id="id7">
<h3><span class="section-number">3.3.1. </span>马尔可夫和切比雪夫不等式<a class="headerlink" href="#id7" title="此标题的永久链接"></a></h3>
<p>我们首先介绍一些重要的不等式，这些不等式是大数定律和中心极限定理的基础。
这些不等式使用随机变量的均值和方差去分析事件的概率，
在随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的均值和方差易于计算，但分布不知道或不易计算时，
这些不等式就非常有用。</p>
<p>首先介绍 <strong>马尔可夫不等式</strong> 。
粗略的讲，该不等式是指，一个 <strong>非负</strong> 随机变量如果均值很小，
则该随机变量取大值的概率也非常小。
仔细想一想，这句话其实很好理解。</p>
<aside class="topic">
<p class="topic-title">马尔可夫不等式</p>
<p>设随机变量 <span class="math notranslate nohighlight">\(X\)</span> 只取非负值，则对任意 <span class="math notranslate nohighlight">\(a &gt;0\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-26">
<span class="eqno">(3.3.7)<a class="headerlink" href="#equation-glm-source-content-26" title="此公式的永久链接"></a></span>\[P(X \geq a) \leq \frac{\mathbb{E}[X]}{a}\]</div>
</aside>
<p>下面介绍 <strong>切比雪夫不等式</strong> ，
粗略的讲，切比雪夫不等式是指如果一个随机变量的方差非常小的话，
那么该随机变量取远离均值 <span class="math notranslate nohighlight">\(\mu\)</span> 的概率也非常小。
注意的是：<strong>切比雪夫不等式并不要求所涉及的随机变量非负</strong>。</p>
<aside class="topic">
<p class="topic-title">切比雪夫不等式</p>
<p>设随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的均值为 <span class="math notranslate nohighlight">\(\mu\)</span> ，方差为 <span class="math notranslate nohighlight">\(\sigma^2\)</span> ，则对任意 <span class="math notranslate nohighlight">\(c &gt;0\)</span>，</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-27">
<span class="eqno">(3.3.8)<a class="headerlink" href="#equation-glm-source-content-27" title="此公式的永久链接"></a></span>\[P(|X-\mu| \geq c) \leq \frac{\sigma^2}{c^2}\]</div>
</aside>
<p>切比雪夫不等式和马尔可夫不等式都是描述的随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的某部分概率的上界，
切比雪夫不等式比马尔可夫不等式更准确，<strong>即由切比雪夫不等式提供的概率的上界离概率的真值更近</strong>，
这是因为它利用了 <span class="math notranslate nohighlight">\(X\)</span> 的方差的信息。
当然一个随机变量的均值和方差也仅仅是粗略地描述了随机变量的性质，
所以由切比雪夫不等式提供的上界与精确概率也可能不是非常接近。</p>
</section>
<section id="id8">
<h3><span class="section-number">3.3.2. </span>弱大数定律<a class="headerlink" href="#id8" title="此标题的永久链接"></a></h3>
<p><em>弱大数定律</em> <strong>是指独立同分布的随机变量序列的样本均值，在大样本的情况下，以很大的概率与随机变量的均值非常接近</strong>。</p>
<p>下面考虑独立同分布的随机变量序列 <span class="math notranslate nohighlight">\(X_1,X_2,\cdots,X_N\)</span>，
它们的公共分布（总体分布）的均值为 <span class="math notranslate nohighlight">\(\mu\)</span>，
方差为 <span class="math notranslate nohighlight">\(\sigma^2\)</span>。定义样本均值</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-28">
<span class="eqno">(3.3.9)<a class="headerlink" href="#equation-glm-source-content-28" title="此公式的永久链接"></a></span>\[M_N = \frac{1}{N} \sum_{i=1}^N X_i\]</div>
<p>则</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-29">
<span class="eqno">(3.3.10)<a class="headerlink" href="#equation-glm-source-content-29" title="此公式的永久链接"></a></span>\[\mathbb{E}[M_N] = \frac{\mathbb{E}[X_1]+\mathbb{E}[X_2] + \cdots + \mathbb{E}[X_N] }{N}
= \frac{N \mu}{N} = \mu\]</div>
<p>再运用独立性可得</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-30">
<span class="eqno">(3.3.11)<a class="headerlink" href="#equation-glm-source-content-30" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}V(M_N) &amp;= \frac{V(X_1+X_2+\cdots+X_N)}{N^2}\\&amp;= \frac{V(X_1)+V(X_2)+\cdots + V(X_N)}{N^2}\\&amp;= \frac{N\sigma^2}{N^2} = \frac{\sigma^2}{N}\end{aligned}\end{align} \]</div>
<p>利用切比雪夫不等式可得</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-31">
<span class="eqno">(3.3.12)<a class="headerlink" href="#equation-glm-source-content-31" title="此公式的永久链接"></a></span>\[P(|M_N - \mu| \geq \epsilon) \leq \frac{\sigma^2}{N \epsilon^2} \quad \text{对任意的} \epsilon &gt;0 \text{成立}\]</div>
<p>注意，对任意固定的 <span class="math notranslate nohighlight">\(\epsilon &gt;0\)</span>，上面不等式的右边在 <span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span> 时趋近于 <span class="math notranslate nohighlight">\(0\)</span>，
于是就得到如下的弱大数定律。这里要提到的是：
当 <span class="math notranslate nohighlight">\(X_i\)</span> 的方差无界时，弱大数定律仍然成立，
但是需要更严格而精巧的证明，在此省略。
因此，在下面陈述的弱大数定律中，只需要一个假设，即 <span class="math notranslate nohighlight">\(\mathbb{E}[X_i]\)</span> 是有限的。</p>
<aside class="topic">
<p class="topic-title">弱大数定律</p>
<p>设 <span class="math notranslate nohighlight">\(X_1,X_2,\cdots,X_N\)</span> 独立同分布，其公共分布的均值为 <span class="math notranslate nohighlight">\(\mu\)</span>，则对任意的 <span class="math notranslate nohighlight">\(\epsilon &gt;0\)</span>，
当 <span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span> 时，</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-32">
<span class="eqno">(3.3.13)<a class="headerlink" href="#equation-glm-source-content-32" title="此公式的永久链接"></a></span>\[P(|M_N - \mu| \geq \epsilon)=P\left( \left | \frac{X_1+X_2+\cdots+X_N}{N} -\mu \right | \geq \epsilon \right ) \rightarrow 0\]</div>
</aside>
<p>弱大数定律是指对于充分大的 <span class="math notranslate nohighlight">\(N\)</span>，<span class="math notranslate nohighlight">\(M_N\)</span> 的分布的大部分都集中在 <span class="math notranslate nohighlight">\(\mu\)</span> 的 <strong>附近</strong> 。
设包含 <span class="math notranslate nohighlight">\(\mu\)</span> 的一个区间为 <span class="math notranslate nohighlight">\([\mu-\epsilon,\mu+\epsilon]\)</span>，
则 <span class="math notranslate nohighlight">\(M_N\)</span> 位于该区间的概率非常大。
当 <span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span> 时，该概率为 <span class="math notranslate nohighlight">\(1\)</span>。
当然当 <span class="math notranslate nohighlight">\(\epsilon\)</span> 非常小时，则需要更大的 <span class="math notranslate nohighlight">\(N\)</span>，
使得 <span class="math notranslate nohighlight">\(M_N\)</span> 以很大的概率落在这个区间。
弱大数定律的另一个理解就是在 <span class="math notranslate nohighlight">\(N\)</span> 充分大时， <span class="math notranslate nohighlight">\(M_N\)</span> 依概率收敛于 <span class="math notranslate nohighlight">\(\mu\)</span>。</p>
</section>
<section id="id9">
<h3><span class="section-number">3.3.3. </span>依概率收敛<a class="headerlink" href="#id9" title="此标题的永久链接"></a></h3>
<p>弱大数定律可以表述为” <span class="math notranslate nohighlight">\(M_N\)</span> 收敛于 <span class="math notranslate nohighlight">\(\mu\)</span> ”
。但是，既然 <span class="math notranslate nohighlight">\(M_1,M_2,\cdots\)</span> 是随机变量序列，
而不是数列，所以这里”收敛”的含义不同于数列的收敛，应该给予更明确的定义
。</p>
<aside class="topic">
<p class="topic-title">依概率收敛</p>
<p>设 <span class="math notranslate nohighlight">\(Y_1,Y_2,\cdots\)</span> 是随机变量序列（不必相互独立），<span class="math notranslate nohighlight">\(a\)</span> 为一个实数，如果对任意的 <span class="math notranslate nohighlight">\(\epsilon &gt;0\)</span> 都有</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-33">
<span class="eqno">(3.3.14)<a class="headerlink" href="#equation-glm-source-content-33" title="此公式的永久链接"></a></span>\[\lim_{N \rightarrow \infty} P(| Y_N - a| \geq \epsilon ) = 0\]</div>
<p>则称 <span class="math notranslate nohighlight">\(Y_N\)</span> 依概率收敛于 <span class="math notranslate nohighlight">\(a\)</span>。</p>
</aside>
<p>根据这个定义，<strong>弱大数定律就是说样本均值统计量依概率收敛于总体分布的真值</strong> <span class="math notranslate nohighlight">\(\mu\)</span>
。更一般地，利用切比雪夫不等式可以证明：如果所有的 <span class="math notranslate nohighlight">\(Y_N\)</span> 具有相同的期望，而方差 <span class="math notranslate nohighlight">\(V(Y_N)\)</span> 趋近于 <span class="math notranslate nohighlight">\(0\)</span>，
则 <span class="math notranslate nohighlight">\(Y_N\)</span> 依概率收敛于 <span class="math notranslate nohighlight">\(\mu\)</span>。</p>
<p>如果随机变量序列 <span class="math notranslate nohighlight">\(Y_1,Y_2,\cdots\)</span> 有概率质量函数或者概率密度函数，且依概率收敛于 <span class="math notranslate nohighlight">\(a\)</span>。
则根据依概率收敛的定义，对充分大的 <span class="math notranslate nohighlight">\(N\)</span>，<span class="math notranslate nohighlight">\(Y_N\)</span> 的概率质量或者密度函数的大部分”质量”集中在 <span class="math notranslate nohighlight">\(a\)</span>
的 <span class="math notranslate nohighlight">\(\epsilon\)</span> 邻域 <span class="math notranslate nohighlight">\([a-\epsilon,a+\epsilon]\)</span> 内。
所以依概率收敛的定义也可以这样描述：
对任意的 <span class="math notranslate nohighlight">\(\epsilon &gt;0\)</span> 和 <span class="math notranslate nohighlight">\(\delta &gt;0\)</span>，
存在 <span class="math notranslate nohighlight">\(N_0\)</span>，使得对所有的 <span class="math notranslate nohighlight">\(N \geq N_0\)</span> 都有</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-34">
<span class="eqno">(3.3.15)<a class="headerlink" href="#equation-glm-source-content-34" title="此公式的永久链接"></a></span>\[P(|Y_N - a| \geq \epsilon) \leq \delta\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\epsilon\)</span> 称为 <strong>精度</strong>，<span class="math notranslate nohighlight">\(\delta\)</span> 称为 <strong>置信水平</strong>。
依概率收敛的定义有如下形式：
<strong>任意给定精度和置信水平，在</strong> <span class="math notranslate nohighlight">\(N\)</span> <strong>充分大时</strong> <span class="math notranslate nohighlight">\(Y_N\)</span> <strong>等于</strong> <span class="math notranslate nohighlight">\(a\)</span> 。</p>
</section>
<section id="ch-clt">
<span id="id10"></span><h3><span class="section-number">3.3.4. </span>中心极限定理<a class="headerlink" href="#ch-clt" title="此标题的永久链接"></a></h3>
<p>根据弱大数定律，样本均值 <span class="math notranslate nohighlight">\(M_N=(x_1+x_2+\cdots+x_N)/N\)</span> 的分布随着 <span class="math notranslate nohighlight">\(N\)</span>
的增大，越来越集中在真值 <span class="math notranslate nohighlight">\(\mu\)</span> 的邻域内。特别地，
在我们的论证中，假定 <span class="math notranslate nohighlight">\(X_i\)</span> 的方差为有限的时候，
可以证明 <span class="math notranslate nohighlight">\(M_N\)</span> 的方差趋近于 <span class="math notranslate nohighlight">\(0\)</span>。
另一方面，前 <span class="math notranslate nohighlight">\(N\)</span> 项的和</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-35">
<span class="eqno">(3.3.16)<a class="headerlink" href="#equation-glm-source-content-35" title="此公式的永久链接"></a></span>\[S_N = X_1 + \cdots +X_N = N M_N\]</div>
<p>的方差趋近于 <span class="math notranslate nohighlight">\(\infty\)</span>，
所以 <span class="math notranslate nohighlight">\(S_N\)</span> 的分布不可能收敛。
换一个角度，我们考虑 <span class="math notranslate nohighlight">\(S_N\)</span>
与其均值 <span class="math notranslate nohighlight">\(N \mu\)</span> 的偏差 <span class="math notranslate nohighlight">\(S_N - N \mu\)</span>，
然后乘以正比于 <span class="math notranslate nohighlight">\(1/\sqrt{N}\)</span> 的刻度系数。
乘以刻度系数的目的就是使新的随机变量具有固定的方差。
中心极限定理指出这个新的随机变量的分布趋于标准正态分布。</p>
<p>具体地说，设 <span class="math notranslate nohighlight">\(X_1,X_2,\cdots\)</span> 是独立同分布的随机变量序列，
均值为 <span class="math notranslate nohighlight">\(\mu\)</span>，
方差为 <span class="math notranslate nohighlight">\(\sigma^2\)</span>。
定义</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-36">
<span class="eqno">(3.3.17)<a class="headerlink" href="#equation-glm-source-content-36" title="此公式的永久链接"></a></span>\[Z_N = \frac{S_N - N \mu}{\sigma \sqrt{N}} = \frac{X_1+\cdots+X_N - N\mu}{\sigma \sqrt{N}}\]</div>
<p>经过简单计算可以得到</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-37">
<span class="eqno">(3.3.18)<a class="headerlink" href="#equation-glm-source-content-37" title="此公式的永久链接"></a></span>\[\mathbb{E}[Z_N] = \frac{\mathbb{E}[X_1+\cdots+X_N] -N\mu }{\sigma \sqrt{N}} = 0\]</div>
<div class="math notranslate nohighlight" id="equation-glm-source-content-38">
<span class="eqno">(3.3.19)<a class="headerlink" href="#equation-glm-source-content-38" title="此公式的永久链接"></a></span>\[V(Z_N) = \frac{V(X_1+\cdots+X_N) }{N\sigma^2} = \frac{V(X_1)+\cdots+V(X_N)}{N\sigma^2}
= \frac{N\sigma^2}{N\sigma^2} = 1\]</div>
<aside class="topic">
<p class="topic-title">中心极限定理</p>
<p>设 <span class="math notranslate nohighlight">\(X_1,X_2,\cdots\)</span> 是独立同分布的随机变量序列，序列的每一项的均值为 <span class="math notranslate nohighlight">\(\mu\)</span>，
方差为 <span class="math notranslate nohighlight">\(\sigma^2\)</span>。记</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-39">
<span class="eqno">(3.3.20)<a class="headerlink" href="#equation-glm-source-content-39" title="此公式的永久链接"></a></span>\[Z_N  = \frac{X_1+\cdots+X_N - N\mu}{\sigma \sqrt{N}}\]</div>
<p>则 <span class="math notranslate nohighlight">\(Z_N\)</span> 的（累积）分布函数的极限分布为标准正态（累积）分布函数。
即</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-40">
<span class="eqno">(3.3.21)<a class="headerlink" href="#equation-glm-source-content-40" title="此公式的永久链接"></a></span>\[\lim_{N \rightarrow \infty} P(Z_N \leq x) = \Phi(x), \quad \text{对任意的}x\text{成立}\]</div>
<p>可以记作</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-41">
<span class="eqno">(3.3.22)<a class="headerlink" href="#equation-glm-source-content-41" title="此公式的永久链接"></a></span>\[Z_N \sim N(0,1)\]</div>
<p>或者</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-42">
<span class="eqno">(3.3.23)<a class="headerlink" href="#equation-glm-source-content-42" title="此公式的永久链接"></a></span>\[\frac{S_N - N\mu}{\sigma \sqrt{N}} \sim N(0,1)\]</div>
</aside>
<p>中心极限定理允许人们可以将 <span class="math notranslate nohighlight">\(Z_N\)</span> 的分布看成正态分布，从而可以计算与 <span class="math notranslate nohighlight">\(Z_N\)</span>
相关的随机变量的概率问题，因为正态分布在线性变换之下仍然是正态分布。
如果把 <span class="math notranslate nohighlight">\(Z_N\)</span> 的分子分母同时除以 <span class="math notranslate nohighlight">\(N\)</span>，就可以用均值统计量 <span class="math notranslate nohighlight">\(M_N\)</span> 表示。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-43">
<span class="eqno">(3.3.24)<a class="headerlink" href="#equation-glm-source-content-43" title="此公式的永久链接"></a></span>\[Z_N  = \frac{S_N - N\mu}{\sigma \sqrt{N}}
=  \frac{M_N - \mu}{\sigma /\sqrt{N}} \sim N(0,1)\]</div>
<p>再经过一些简单的变换，可以认为均值统计量的极限分布是均值为 <span class="math notranslate nohighlight">\(\mu\)</span> 方差为 <span class="math notranslate nohighlight">\(\sigma^2/N\)</span> 的正态分布。</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-glm-source-content-44">
<span class="eqno">(3.3.25)<a class="headerlink" href="#equation-glm-source-content-44" title="此公式的永久链接"></a></span>\[M_N \sim N( \mu,\frac{\sigma^2}{N})\]</div>
</div></blockquote>
<p>中心极限定理对 <span class="math notranslate nohighlight">\(X_i\)</span> 的分布并没有任何要求，
但是 <span class="math notranslate nohighlight">\(X_i\)</span> 的分布多少还是有一点不一样的地方。</p>
<ul class="simple">
<li><p>当总体分布 <span class="math notranslate nohighlight">\(X_i\)</span> 是正态分布 <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,\sigma^2)\)</span> 时，无论样本 <span class="math notranslate nohighlight">\(N\)</span> 是多少，
均值统计量 <span class="math notranslate nohighlight">\(M_N\)</span> 都服从正态分布 <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,\sigma^2/N)\)</span> 。</p></li>
<li><p>当总体分布 <span class="math notranslate nohighlight">\(X_i\)</span> 不是正态分布时，均值统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 渐近服从（极限分布）正态分布 <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,\sigma^2/N)\)</span> ，
<span class="math notranslate nohighlight">\(N\)</span> 越大越接近正态分布。至于 <span class="math notranslate nohighlight">\(N\)</span> 是多少才行，并没有一个准确的判断方法，这和 <span class="math notranslate nohighlight">\(X_i\)</span> 的分布有关。
<span class="math notranslate nohighlight">\(X_i\)</span> 的分布与正态分布相差越大，需要的 <span class="math notranslate nohighlight">\(N\)</span> 就越大；反之，<span class="math notranslate nohighlight">\(X_i\)</span> 的分布与正态分布越相似，需要的 <span class="math notranslate nohighlight">\(N\)</span> 越小。</p></li>
</ul>
<p>中心极限定理是一个非常具有一般性的定理。对于定理的条件，
除了序列为独立同分布的序列之外，还假设各项的均值和方差的有限性。
此外，对 <span class="math notranslate nohighlight">\(X_i\)</span> 的分布再也没有其它的要求。
<span class="math notranslate nohighlight">\(X_i\)</span> 的分布可以是离散的、连续的或是混合的。</p>
<p>这个定理不仅在理论上非常重要，而且在实践中也是如此。
从理论上看，该定理表明大样本的独立随机变量序列和大致是正态的。
所以当人们遇到的随机变量是由许多影响小但是独立的随机因素的总和的情况，
此时根据中心极限定理就可以判定这个随机量的分布是正态的。
例如在许多自然或工程系统中的白噪声就是这种情况。</p>
<p>从应用角度看，中心极限定理可以不必考虑随机变量具体服从什么概率分布，避免了概率质量函数和概率密度函数的繁琐计算。
而且，在具体计算的时候，人们只需均值和方差的信息以及简单查阅标准正态分布表即可。</p>
</section>
<section id="id11">
<h3><span class="section-number">3.3.5. </span>强大数定理<a class="headerlink" href="#id11" title="此标题的永久链接"></a></h3>
<p>强大数定律与弱大数定律一样，都是指样本均值统计量收敛于真值 <span class="math notranslate nohighlight">\(\mu\)</span>。
但是它们强调的是不同的收敛类别，
下面是强大数定律的一般陈述。</p>
<aside class="topic">
<p class="topic-title">强大数定律</p>
<p>设 <span class="math notranslate nohighlight">\(X_1,X_2,\cdots\)</span> 是均值为 <span class="math notranslate nohighlight">\(\mu\)</span> 的独立同分布随机序列，则样本均值 <span class="math notranslate nohighlight">\(M_N=(X_1,X_2,\cdots+X_N)/N\)</span>
<strong>以概率</strong> <span class="math notranslate nohighlight">\(\pmb{1}\)</span> 收敛于 <span class="math notranslate nohighlight">\(\mu\)</span>，即</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-45">
<span class="eqno">(3.3.26)<a class="headerlink" href="#equation-glm-source-content-45" title="此公式的永久链接"></a></span>\[P\left ( \lim_{N \rightarrow \infty} \frac{X_1+X_2+\cdots+X_N}{N} = \mu \right ) = 1\]</div>
</aside>
<p>强大数定律与弱大数定律的区别是细微的，需要仔细说明。
弱大数定律是指 <span class="math notranslate nohighlight">\(M_N\)</span> 显著性偏离 <span class="math notranslate nohighlight">\(\mu\)</span> 的事件的概率 <span class="math notranslate nohighlight">\(P(|M_N -\mu|)\geq \epsilon\)</span>
在 <span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span> 时区域 <span class="math notranslate nohighlight">\(0\)</span>。
但是对任意有限的 <span class="math notranslate nohighlight">\(N\)</span>，这个概率可以是正的（大于零）。
所以可以想象的是，在 <span class="math notranslate nohighlight">\(M_N\)</span> 这个无穷的序列中，
常常有 <span class="math notranslate nohighlight">\(M_N\)</span> 显著偏离 <span class="math notranslate nohighlight">\(\mu\)</span>。
弱大数定律不能提供到底有多少会显著性偏离 <span class="math notranslate nohighlight">\(\mu\)</span>，但是强大数定律却可以。
根据强大数定律， <span class="math notranslate nohighlight">\(M_N\)</span> 以概率 <span class="math notranslate nohighlight">\(1\)</span> 收敛于 <span class="math notranslate nohighlight">\(\mu\)</span>。
这意味，对任意的 <span class="math notranslate nohighlight">\(\epsilon &gt;0\)</span>，偏离 <span class="math notranslate nohighlight">\(|M_N-\mu|\)</span> 超过 <span class="math notranslate nohighlight">\(\epsilon\)</span> 的只能发生有限次。</p>
<p>强大数定律中的收敛与弱大数定律中的收敛是两个不同的概念，现在给出以概率 <span class="math notranslate nohighlight">\(1\)</span> 收敛的定义。</p>
<aside class="topic">
<p class="topic-title">以概率 <span class="math notranslate nohighlight">\(1\)</span> 收敛</p>
<p>设 <span class="math notranslate nohighlight">\(Y_1,Y_2,\cdots\)</span> 是某种概率模型下的随机变量序列（不必独立），<span class="math notranslate nohighlight">\(c\)</span> 是某个实数，
如果</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-46">
<span class="eqno">(3.3.27)<a class="headerlink" href="#equation-glm-source-content-46" title="此公式的永久链接"></a></span>\[P(\lim_{N \rightarrow \infty} Y_N=c) = 1\]</div>
<p>则称 <span class="math notranslate nohighlight">\(Y_N\)</span> <strong>以概率</strong> <span class="math notranslate nohighlight">\(\pmb{1}\)</span> （或 <strong>几乎处处</strong> ）收敛于 <span class="math notranslate nohighlight">\(c\)</span> 。</p>
</aside>
<p>类似于前面的讨论，我们应该正确理解以概率 <span class="math notranslate nohighlight">\(1\)</span> 这种收敛类型，
这种收敛也是在由无穷数列组成的样本空间中建立的：
若某随机变量序列以概率 <span class="math notranslate nohighlight">\(1\)</span> 收敛于常数 <span class="math notranslate nohighlight">\(c\)</span>，
则在样本空间中，全部的概率集中在满足极限等于 <span class="math notranslate nohighlight">\(c\)</span> 的无穷数列的子集上。
但这并不意味其他的无穷序列是不可能的，只是他们是非常不可能的，即他们的概率是 <span class="math notranslate nohighlight">\(0\)</span>。</p>
</section>
</section>
<section id="id12">
<h2><span class="section-number">3.4. </span>似然估计量<a class="headerlink" href="#id12" title="此标题的永久链接"></a></h2>
<p>前几节我们已经把评价一个参数估计量所需的基础知识讨论的差不多了的，
参数估计量一定是一个关于样本的函数，
而样本的函数定义为统计量，
因此参数估计量是统计量。
统计量也是一个随机变量，
统计量的分布统称为抽样分布。
大数定律给出了均值统计量的极限收敛性质，
中心极限定理进一步强化，给出了均值统计量的极限分布。
概率分布的均值参数的最大似然估计量就等于样本的均值估计量，
因此我们可以运用中心极限定理对均值参数的似然估计量进行分析。</p>
<p>设 <span class="math notranslate nohighlight">\(X_1,X_2,\cdots\)</span> 是独立同分布的随机变量序列，亦可以看做是某个总体变量 <span class="math notranslate nohighlight">\(X\)</span>
的独立同分布的观测样本。
<span class="math notranslate nohighlight">\(\theta\)</span> 是变量 <span class="math notranslate nohighlight">\(X\)</span> 所属分布的一个参数，
它的最大似然估计量记作 <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>，
假设参数的真实值是 <span class="math notranslate nohighlight">\(\theta_{true}\)</span>。</p>
<p>我们知道估计量 <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> 是一个随机量，它不能精确等于参数真实值 <span class="math notranslate nohighlight">\(\theta_{true}\)</span>。
但是如果当样本数量 <span class="math notranslate nohighlight">\(N\)</span> 足够大时，估计量 <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> 可以依概率收敛于参数的真实值 <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>，
那么我们就说这个估计量是 <strong>一致性估计量</strong> 。</p>
<aside class="topic">
<p class="topic-title">一致性估计量(Consistent Estimator)</p>
<p>当样本数量趋近于无穷大时，估计量 <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> <strong>以概率</strong> 收敛于参数的真实值 <span class="math notranslate nohighlight">\(\theta_{\text{true}}\)</span>，</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-47">
<span class="eqno">(3.4.1)<a class="headerlink" href="#equation-glm-source-content-47" title="此公式的永久链接"></a></span>\[\lim_{ N \rightarrow \infty} P( | \hat{\theta}_N- \theta_{\text{true}}| \geq \epsilon ) = 0,
\quad \text{对任意} \epsilon &gt;0 \text{成立}\]</div>
<p>则称这个估计量 <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> 就是一致性估计量（Consistent Estimator）。</p>
</aside>
<p>在统计学中，一致估计量(Consistent Estimater)、渐进一致估计量，亦称相合估计量、相容估计量。
其所表征的一致性或（相合性）同渐进正态性是大样本估计中两大最重要的性质。随着样本量无限增加，
估计误差在一定意义下可以任意地小。也即估计量的分布越来越集中在所估计的参数的真实值附近，
使得估计量依概率收敛于参数真值。
这里定义的一致性称弱相合性。如果将概率收敛的方式改为以概率 <span class="math notranslate nohighlight">\(1\)</span> 收敛就称为强相合性。</p>
<p>为什么是 <em>依概率</em> 收敛，而不是 <em>确定性</em> 收敛？
因为参数估计量本身是一个随机变量，服从某种概率分布，只能是以某种概率得到某个确定性的值，
所以这里是依概率收敛到真实值。
一致性是对参数估计的基本要求，一个参数估计要是不满足一致性基本无用。</p>
<p>一致性估计量是 <em>依概率</em> 收敛到真实值的，并不是一定收敛到真实值，
所示我们实际上得到的参数估计量和真实值之间还是会存在一定误差的。
我们需要对这个误差进行量化评估，以便能评估一个估计量的好坏。</p>
<p>最直接的误差就是估计量和真实值之间的差值， <span class="math notranslate nohighlight">\(d=\hat{\theta}-\theta\)</span> ，
但是差值 <span class="math notranslate nohighlight">\(d\)</span> 有正有负，不易使用，
因此我们采用它的平方，
定义参数估计量和参数真实值之间的平方误差（Squared Error,SE）为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-48">
<span class="eqno">(3.4.2)<a class="headerlink" href="#equation-glm-source-content-48" title="此公式的永久链接"></a></span>\[SE = (\hat{\theta}-\theta_{true})^2\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> 是一个随机量，导致 <code class="docutils literal notranslate"><span class="pre">SE</span></code> 也是一个随机量，
我们用它的期望值作为最终的评价误差，平方误差的期望称之为均方误差（mean square error,MSE）
。</p>
<div class="math notranslate nohighlight" id="equation-eq-2-39">
<span class="eqno">(3.4.3)<a class="headerlink" href="#equation-eq-2-39" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}MSE &amp;= \mathbb{E} [(\hat{\theta}- \theta_{\text{true}} )^2]\\&amp;= \mathbb{E}[ \hat{\theta}^2-2 \hat{\theta} \theta_{\text{true}} + \theta_{\text{true}}^2    ]\\&amp;= \left ( \mathbb{E}[\hat{\theta}^2]- \mathbb{E}[\hat{\theta}]^2 \right )
+\left ( \mathbb{E}[\hat{\theta}]^2    -2\mathbb{E}[\hat{\theta}] \theta_{\text{true}} + \theta_{\text{true}}^2   \right )\\&amp;= \underbrace{V (\hat{\theta})}_{\text{估计量的方差部分}}
 +
\underbrace{\left ( \mathbb{E}[\hat{\theta}]- \theta_{\text{true}}\right)^2}_{\text{偏差部分}}\end{aligned}\end{align} \]</div>
<p>显然一个参数估计量和参数真实值之间的误差由两部分组成：<strong>估计量的方差</strong> 和 <strong>偏差</strong> ，
其中方差部分是估计量的方差，不是观测变量 <span class="math notranslate nohighlight">\(X\)</span> 的方差。
两部分都是非负的，
因此一个好的估计量要求两部分都必须小。</p>
<section id="id13">
<h3><span class="section-number">3.4.1. </span>估计量的偏差与方差<a class="headerlink" href="#id13" title="此标题的永久链接"></a></h3>
<p>一个估计量的偏差（bias）被定义成估计量的期望和参数真实值之间的差值，</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-49">
<span class="eqno">(3.4.4)<a class="headerlink" href="#equation-glm-source-content-49" title="此公式的永久链接"></a></span>\[b(\hat{\theta}) = \mathbb{E}[\hat{\theta} ]
- \theta_{\text{true}}\]</div>
<p>当偏差为 <span class="math notranslate nohighlight">\(0\)</span> 时，就称这个估计量是 <strong>无偏估计量</strong>。</p>
<aside class="topic">
<p class="topic-title">无偏估计量（unbiased estimator）</p>
<p>当一个估计量满足 <span class="math notranslate nohighlight">\(b(\hat{\theta})=0\)</span> 时，也就是满足 <strong>估计量的期望值等于参数的真实值</strong> ，就称这个估计量为无偏估计。</p>
<ul class="simple">
<li><p>若 <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\theta}]=\theta_{\text{true}}\)</span> 对 <span class="math notranslate nohighlight">\(\theta\)</span> 所有可能取值都成立，
则称 <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> 为 <strong>无偏估计</strong>。</p></li>
<li><p>若 <span class="math notranslate nohighlight">\(\lim_{N \rightarrow \infty} \mathbb{E}[\hat{\theta}]=\theta_{\text{true}}\)</span>
对 <span class="math notranslate nohighlight">\(\theta\)</span> 所有可能取值都成立，则称 <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> 为 <strong>渐近无偏估计</strong>。</p></li>
</ul>
</aside>
<p>我们不可能指望作为随机量的估计量正好和未知的参数真值相等，因此估计误差一般非零。
另一方面，对于 <span class="math notranslate nohighlight">\(\theta\)</span> 所有可能的取值，如果平均估计误差是零，则得到一个无偏的估计量。
渐进无偏只需要随着观测样本数量 <span class="math notranslate nohighlight">\(N\)</span> 的增加，估计量变得无偏即可。</p>
<p>除了偏差，我们还对误差中方差部分的大小感兴趣，
现在我们看下估计量的方差部分，
估计量的方差也是存在下界的，这可以通过一个定理给出。</p>
<aside class="topic">
<p class="topic-title">Cramer-Rao Lower Bound (CRLB) 定理</p>
<p>Cram´er–Rao Lower Bound (CRLB)定理描述了一个确定性参数(deterministic parameter) <span class="math notranslate nohighlight">\(\theta\)</span>
的估计量的方差的下界</p>
<div class="math notranslate nohighlight" id="equation-eq-2-50">
<span class="eqno">(3.4.5)<a class="headerlink" href="#equation-eq-2-50" title="此公式的永久链接"></a></span>\[V(\hat{\theta}) \ge \frac{(\frac{\partial}{\partial \theta} \mathbb{E}[\hat{\theta}] )^2}{I(\theta)}\]</div>
</aside>
<p>其中分子部分是估计量的期望对参数真实值的一阶导的平方，
如果一个估计量是无偏估计，那么有 <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\theta}]=\theta_{\text{true}}\)</span>
，这时分子就等于 <span class="math notranslate nohighlight">\(1\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-50">
<span class="eqno">(3.4.6)<a class="headerlink" href="#equation-glm-source-content-50" title="此公式的永久链接"></a></span>\[(\frac{\partial}{\partial \theta} \mathbb{E}[\hat{\theta}] )^2
= ( \frac{\partial}{\partial \theta}  \theta)^2
= 1\]</div>
<p>因此 <strong>对于无偏估计量</strong>，<a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#equation-eq-2-50">公式(2.7.20)</a> 可以简化为：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-51">
<span class="eqno">(3.4.7)<a class="headerlink" href="#equation-glm-source-content-51" title="此公式的永久链接"></a></span>\[V(\hat{\theta}) \ge \frac{1}{I(\theta)}\]</div>
<p><span class="math notranslate nohighlight">\(I(\theta)\)</span> 是费歇尔信息(Fisher-Information)矩阵。
根据 <code class="docutils literal notranslate"><span class="pre">CRLB</span></code> 定理，可以看出一个估计量的方差是存在下界的，
<strong>并且对于无偏估计量，估计量的方差的最小值是费歇尔信息的倒数</strong>，
显然当一个估计量的方差为下界时，这个估计量是最稳定的。</p>
<p>通常会用如下方式衡量一个 <em>无偏估计量</em> 的 <strong>有效性（efficiency）</strong>，</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-52">
<span class="eqno">(3.4.8)<a class="headerlink" href="#equation-glm-source-content-52" title="此公式的永久链接"></a></span>\[\mathcal{E}(\hat{\theta}) = \frac{1/I(\theta)}{V(\hat{\theta})}\]</div>
<p>当 <span class="math notranslate nohighlight">\(\mathcal{E}(\hat{\theta})=1\)</span> 时，称此估计量为有效估计（efficient estimator）。</p>
<aside class="topic">
<p class="topic-title">有效估计量(efficient estimator)</p>
<p>任意一个估计量，如果其方差为 <code class="docutils literal notranslate"><span class="pre">CRLB</span></code> 的下限，那么这个估计量是有效估计量。</p>
</aside>
<p>估计量的均方误差由方差和偏差组成，最好的估计量应该是偏差和方差都尽可能的小，偏差最小为无偏估计，
所以我们定义出最小方差无偏估计。</p>
<aside class="topic">
<p class="topic-title">最小方差无偏估计</p>
<p>当参数 <span class="math notranslate nohighlight">\(\theta\)</span> 存在多个无偏估计时，其中方差最小的估计量就称为最小方差无偏估计
(Minimum Variance Unbiased Estimator,MVUE)。
显然，<code class="docutils literal notranslate"><span class="pre">MVUE</span></code> 是使得 <code class="docutils literal notranslate"><span class="pre">MSE</span></code> 最小的估计量。
然而，最小方差无偏估计量并不总是存在的，
即使存在，我们也可能找不到，没有任何一种方法会始终产生 <code class="docutils literal notranslate"><span class="pre">MVUE</span></code> 。
查找 <code class="docutils literal notranslate"><span class="pre">MVUE</span></code> 的一种有用方法是为参数找到充分统计量。</p>
</aside>
<p>最后我们总结下，</p>
<ul class="simple">
<li><p>如果估计量依概率收敛于参数真值，则称这个估计量具有相合性，或者说一致性。</p></li>
<li><p>如果估计量的期望等于参数真值，则这个估计量是无偏估计。</p></li>
<li><p>对于无偏估计量，估计量的方差的最小值是费歇尔信息的倒数。</p></li>
</ul>
</section>
<section id="ch-2-fisher-information">
<span id="id14"></span><h3><span class="section-number">3.4.2. </span>信息量<a class="headerlink" href="#ch-2-fisher-information" title="此标题的永久链接"></a></h3>
<p>在参数估计问题中，我们从目标概率分布的观测样本中获取有关参数的信息。
这里有一个很自然的问题是：数据样本可以提供多少关于未知参数信息？
本节我们介绍这种信息量的度量方法。
我们还可以看到，
该信息量度可用于查找估计量方差的界限，
并可用于近似估计从大样本中获得的估计量的抽样分布，
并且如果样本较大，则进一步用于获得近似置信区间。</p>
<p>假设有一个随机变量 <span class="math notranslate nohighlight">\(X\)</span>
，其概率质量（密度）函数为 <span class="math notranslate nohighlight">\(P(X;\theta)\)</span>
， <span class="math notranslate nohighlight">\(\theta\)</span> 是模型未知参数，并且其值未知。
概率质量（密度）函数描述了在给定 <span class="math notranslate nohighlight">\(\theta\)</span> 时，
获取一个 <span class="math notranslate nohighlight">\(X\)</span> 的观测值的概率。
这里我们先看只有一条观测样本的情况，稍后再说有多条观测样本的情况。</p>
<p>随机变量 <span class="math notranslate nohighlight">\(X\)</span>  <strong>单条观测样本</strong> 对数似然函数为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-53">
<span class="eqno">(3.4.9)<a class="headerlink" href="#equation-glm-source-content-53" title="此公式的永久链接"></a></span>\[\ell(\theta;X) = log P(X;\theta)\]</div>
<p>当利用最大似然估计进行参数估计时，我们需要求对数似然函数的一阶偏导数</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-54">
<span class="eqno">(3.4.10)<a class="headerlink" href="#equation-glm-source-content-54" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\ell'(\theta;X) &amp;= \frac{\partial  \ell(\theta;X) }{\partial \theta}\\&amp;=  \frac{\partial }{\partial \theta} \log P(X;\theta)\\&amp;=\frac{P'(X;\theta)}{P(X;\theta)}\end{aligned}\end{align} \]</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>这里利用了对数函数的求导公式:</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-55">
<span class="eqno">(3.4.11)<a class="headerlink" href="#equation-glm-source-content-55" title="此公式的永久链接"></a></span>\[\nabla \log f(x) = \frac{1}{f(x)} \nabla f(x)\]</div>
</div>
<p>其中 <span class="math notranslate nohighlight">\(P'(X;\theta)\)</span> 表示函数 <span class="math notranslate nohighlight">\(P(X;\theta)\)</span>
关于 <span class="math notranslate nohighlight">\(\theta\)</span> 的一阶导数，同理，
符号 <span class="math notranslate nohighlight">\(P''(x;\theta)\)</span> 表示二阶导数。
如果参数 <span class="math notranslate nohighlight">\(\theta\)</span> 是一个标量参数，关于参数的一阶导数和二阶导师也是一个标量。
如果参数 <span class="math notranslate nohighlight">\(\theta\)</span> 是一个参数向量，关于参数的一阶偏导数就是一个向量，二阶偏导数是一个矩阵。</p>
<aside class="topic">
<p class="topic-title">Score function</p>
<p>对数似然函数关于参数的一阶导数称为得分函数(Score function)，通常用符号 <span class="math notranslate nohighlight">\(S\)</span> 表示。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-56">
<span class="eqno">(3.4.12)<a class="headerlink" href="#equation-glm-source-content-56" title="此公式的永久链接"></a></span>\[S(\theta)= \frac{\partial \ell(\theta;X)}{\partial \theta}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\theta\)</span> 是模型的参数，当模型存在多个参数时，<span class="math notranslate nohighlight">\(\theta\)</span> 是参数向量，
<span class="math notranslate nohighlight">\(S(\theta)\)</span> 也是一个向量。 <span class="math notranslate nohighlight">\(S(\theta)\)</span> 是一个关于 <strong>观测样本和参数</strong> 的函数。
通常如果似然函数是凹(concave)的，我们可以通过令 <span class="math notranslate nohighlight">\(S(\theta)=0\)</span> 求得参数的最优解。</p>
</aside>
<p><span class="math notranslate nohighlight">\(S(\theta)\)</span> 是对数似然函数的一阶导数，一阶导数描述的是函数在这一点的切线的斜率，
导数越大切线斜率越大，所以 <span class="math notranslate nohighlight">\(S(\theta)\)</span> 表示的是对数似然函数在某个 <span class="math notranslate nohighlight">\(\theta\)</span>
值时模型的敏感度(sensitive)。</p>
<p><span class="math notranslate nohighlight">\(S(\theta)\)</span> 是关于 <span class="math notranslate nohighlight">\(X\)</span> 的一个函数，
所以 <span class="math notranslate nohighlight">\(S(\theta)\)</span> 也是一个随机变量，
我们可以研究它的期望与方差。
首先来看一下 <span class="math notranslate nohighlight">\(S(\theta)\)</span>  的期望，
在开始之前，先给出有关积分计算的一些技巧。</p>
<p>一个函数的积分和求导是可以互换的，并且概率质量（密度）函数的积分一定是等于 <span class="math notranslate nohighlight">\(1\)</span> 的，
所以有如下等式成立。</p>
<div class="math notranslate nohighlight" id="equation-eq-2-33">
<span class="eqno">(3.4.13)<a class="headerlink" href="#equation-eq-2-33" title="此公式的永久链接"></a></span>\[\int f'(x;\theta) dx= \frac{\partial}{\partial \theta} \int f(x;\theta) dx
=\frac{\partial}{\partial \theta} 1
=0\]</div>
<p>类似地有：</p>
<div class="math notranslate nohighlight" id="equation-eq-2-34">
<span class="eqno">(3.4.14)<a class="headerlink" href="#equation-eq-2-34" title="此公式的永久链接"></a></span>\[\int f''(x;\theta) dx= \frac{\partial^2}{\partial \theta^2} \int f(x;\theta) dx
=\frac{\partial}{\partial \theta} 1
=0\]</div>
<p><span class="math notranslate nohighlight">\(S(\theta)\)</span> 关于样本变量的期望一定是等于0的，
结合 <a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#equation-eq-2-33">公式(2.6.4)</a> 可以推导出 <span class="math notranslate nohighlight">\(S(\theta)\)</span> 的期望为：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-57">
<span class="eqno">(3.4.15)<a class="headerlink" href="#equation-glm-source-content-57" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mathop{\mathbb{E}}_{X} \left[ S(\theta) \right] &amp;= \mathop{\mathbb{E}}_{X} \left[ \nabla \ell(\theta;X) \right]\\&amp;= \int [\nabla \ell(\theta;X)] \, P(X ; \theta) \, \text{d}x\\&amp;= \int [\nabla \log P(X ; \theta)] \, P(X ; \theta) \, \text{d}x\\&amp;= \int \frac{\nabla P(X ; \theta)}{P(X ; \theta)} P(X ; \theta) \, \text{d} x\\&amp;= \int \nabla P(X ; \theta) \, \text{d} x\\&amp;= \nabla \int P(X; \theta) \, \text{d} x\\&amp;= \nabla 1\\&amp;= 0\end{aligned}\end{align} \]</div>
<p><span class="math notranslate nohighlight">\(S(\theta)\)</span> 的二阶矩(second moment)，也就是其方差(Variance)，被称为 <code class="docutils literal notranslate"><span class="pre">Fisher</span> <span class="pre">information</span></code>，
中文常翻译成费歇尔信息，
通常用符号 <span class="math notranslate nohighlight">\(I(\theta)\)</span> 表示，
<span class="math notranslate nohighlight">\(I(\theta)\)</span> 是一个方阵，通常称为信息矩阵(information matrix)。</p>
<div class="math notranslate nohighlight" id="equation-eq-2-35">
<span class="eqno">(3.4.16)<a class="headerlink" href="#equation-eq-2-35" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}I(\theta) &amp;=\mathop{V(S(\theta))}_{X}\\&amp;=
\mathop{\mathbb{E}}_{X} [(S(\theta)- \mathop{\mathbb{E}}_{X} [S(\theta)] )^2]\\&amp;= \mathop{\mathbb{E}}_{X} [S(\theta)^2]\\&amp;= \mathop{\mathbb{E}}_{X} [S(\theta)S(\theta)^T]\end{aligned}\end{align} \]</div>
<p>实际上， <span class="math notranslate nohighlight">\(I(\theta)\)</span> 和对数似然函数的二阶导数的期望值是有关系的，
我们先来看下对数似然函数的二阶导数，
二阶导数可以在一阶导数的基础上再次求导得到。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-58">
<span class="eqno">(3.4.17)<a class="headerlink" href="#equation-glm-source-content-58" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\ell''(\theta;X) &amp;= \frac{\partial}{\partial \theta} \ell'(\theta;X)\\&amp;= \frac{\partial}{\partial \theta} \left [  \frac{P'(X;\theta)}{P(X;\theta)} \right ]\\&amp;= \frac{P''(X;\theta)P(X;\theta)-[P'(X;\theta)]^2}{[P(X;\theta)]^2}\\&amp;= \frac{P''(X;\theta)P(X;\theta)}{[P(X;\theta)]^2} - \left[ \frac{P'(X;\theta)}{P(X;\theta)} \right]^2\\&amp;= \frac{P''(X;\theta)}{P(X;\theta)} - [\ell'(\theta;X)]^2\end{aligned}\end{align} \]</div>
<p>然后我们看下对数似然函数二阶导数的期望值：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-59">
<span class="eqno">(3.4.18)<a class="headerlink" href="#equation-glm-source-content-59" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mathop{\mathbb{E}}_{X} \left[ \ell''(\theta;X) \right] &amp;=
\int \left [  \frac{P''(X;\theta)}{P(X;\theta)} - [\ell'(\theta;X)]^2  \right ]  P(X;\theta) dx\\&amp;= \int P''(X;\theta) dx -\int [\ell'(\theta;X)]^2  P(X;\theta) dx\\&amp;= 0 - \int [S(\theta)]^2  P(X;\theta)  dx\\&amp;=  - \mathop{\mathbb{E}}_{X} [ [S(\theta)]^2 ]\\&amp;=  - I(\theta)\end{aligned}\end{align} \]</div>
<p>因此，<code class="docutils literal notranslate"><span class="pre">Fisher</span> <span class="pre">information</span></code> <strong>就等于对数似然函数二阶导数的期望的负数</strong>。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-60">
<span class="eqno">(3.4.19)<a class="headerlink" href="#equation-glm-source-content-60" title="此公式的永久链接"></a></span>\[I(\theta) = - \mathop{\mathbb{E}}_{X} \left[ \ell''(\theta;X) \right]\]</div>
<p>但参数 <span class="math notranslate nohighlight">\(\theta\)</span> 是一个参数向量时，对数似然函数的二阶偏导数就是一个矩阵（方阵），
这个二阶偏导数矩阵称为 <strong>海森矩阵（Hessian matrix）</strong>，
通常用符号 <span class="math notranslate nohighlight">\(H\)</span> 表示，
因此 <span class="math notranslate nohighlight">\(I(\theta)\)</span> 经常也被表示成海森矩阵的期望的负数，
当然此时 <span class="math notranslate nohighlight">\(I(\theta)\)</span> 也是一个矩阵，称为 <strong>信息矩阵（information matrix）</strong> 。</p>
<div class="math notranslate nohighlight" id="equation-eq-34-41">
<span class="eqno">(3.4.20)<a class="headerlink" href="#equation-eq-34-41" title="此公式的永久链接"></a></span>\[I(\theta) = - \mathop{\mathbb{E}}_{X}[H(\theta)]\]</div>
<p>我们看到，无论是通过 <code class="docutils literal notranslate"><span class="pre">score</span> <span class="pre">function</span></code> 的方差计算，还是通过 <code class="docutils literal notranslate"><span class="pre">Hessian</span></code> 矩阵计算，
<span class="math notranslate nohighlight">\(I(\theta)\)</span> <strong>都是一个期望值，所以经常被称为期望化信息(expected information)</strong>。
信息量 <span class="math notranslate nohighlight">\(I(\theta)\)</span> 是关于随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的期望的函数，
已经对 <span class="math notranslate nohighlight">\(X\)</span> 求了期望，所以信息量 <span class="math notranslate nohighlight">\(I(\theta)\)</span> 最终的表达式中不再有随机变量（样本） <span class="math notranslate nohighlight">\(X\)</span>
，它仅仅是一个关于参数 <span class="math notranslate nohighlight">\(\theta\)</span> 的函数。</p>
<p>以上单条观测样本的信息量称为单位费歇尔信息量（unit Fisher information），
如果有 <span class="math notranslate nohighlight">\(N\)</span> 个独立不同分布的 <span class="math notranslate nohighlight">\(N\)</span> 条独立观测样本，它们的信息量就是 <span class="math notranslate nohighlight">\(N\)</span> 条单位信息量的求和。
如果有 <span class="math notranslate nohighlight">\(N\)</span> 条独立同分布的观测样本，它们的信息量就是 <span class="math notranslate nohighlight">\(N\)</span> 倍的单位信息量。
<strong>因为单位信息量是变量的期望，与具体的观测样本无关的，所以当有多条观测样本时，累加就可以了</strong>。</p>
<p>对于独立同分布的观测样本集 <span class="math notranslate nohighlight">\(\mathcal{D}=\{X_1,X_2,\cdots,X_N\}\)</span>
，它的信息量为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-61">
<span class="eqno">(3.4.21)<a class="headerlink" href="#equation-glm-source-content-61" title="此公式的永久链接"></a></span>\[I_{\mathcal{D}}(\theta) = N I_{X}(\theta)\]</div>
<p><span class="math notranslate nohighlight">\(I_{\mathcal{D}}(\theta)\)</span> 是正比于 <span class="math notranslate nohighlight">\(N\)</span> 的，也就是说样本越多，我们的到关于参数的信息量就越大。</p>
<p><code class="docutils literal notranslate"><span class="pre">Fisher</span></code> 信息是一种测量可观测随机变量 <span class="math notranslate nohighlight">\(X\)</span> 携带其概率所依赖的未知参数 <span class="math notranslate nohighlight">\(\theta\)</span> 的信息量的方式。
<code class="docutils literal notranslate"><span class="pre">Fisher</span></code> 是 <code class="docutils literal notranslate"><span class="pre">score</span> <span class="pre">function</span></code> （似然函数一阶偏导数）方差，也是似然函数二阶偏导数期望的负数，
<code class="docutils literal notranslate"><span class="pre">Fisher</span></code> 信息越大似然函数的曲线越尖锐，越容易得到参数的最优解。
根据 <code class="docutils literal notranslate"><span class="pre">CRLB</span></code> 定理，基于独立同分布观测样本 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> 的无偏参数估计量的方差的最小值为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-62">
<span class="eqno">(3.4.22)<a class="headerlink" href="#equation-glm-source-content-62" title="此公式的永久链接"></a></span>\[V(\hat{\theta}) \geq \frac{1}{  I_{\mathcal{D}} (\theta)} = \frac{1}{ N I_{X} (\theta)}\]</div>
<p>从这个也可以看出，当 <code class="docutils literal notranslate"><span class="pre">Fisher</span></code> 信息越大的时候，参数估计量的方差越小，方差越小自然就容易得到一个接近参数真值的估计值。
同时它是正比于样本数量 <span class="math notranslate nohighlight">\(N\)</span> 的，意味着随着样本的增加，估计量的方差越来越小。</p>
<p>在 <code class="docutils literal notranslate"><span class="pre">Fisher</span></code> 信息量的实际应用中，当需要计算一个独立同分布的观测样本对于参数的信息量 <span class="math notranslate nohighlight">\(I_{\mathcal{D}} (\theta)\)</span>
时，如果按照上面讲的求了观测变量的期望，那么</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-63">
<span class="eqno">(3.4.23)<a class="headerlink" href="#equation-glm-source-content-63" title="此公式的永久链接"></a></span>\[I_{\mathcal{D}} (\theta) =  N I_{X} (\theta)\]</div>
<p>此时就称 <span class="math notranslate nohighlight">\(I_{\mathcal{D}} (\theta)\)</span> 是期望（expected）信息（矩阵）。
也就是不求期望，直接就按照观测样本值计算，
此时得到的就是观测（observed）信息（矩阵）。
使用期望信息矩阵和观测信息矩阵分别计算出的估计量的方差会有些差别，
这在之后的广义线性模型的内容中会用到。</p>
</section>
<section id="ch-2-mle-estimator">
<span id="id15"></span><h3><span class="section-number">3.4.3. </span>最大似然估计的特性<a class="headerlink" href="#ch-2-mle-estimator" title="此标题的永久链接"></a></h3>
<p>现在我们来看下最大似然估计量具有哪些特点，首先回顾一下分布的均值参数和方差参数的最大似然估计量。</p>
<p>已知随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的期望参数为 <span class="math notranslate nohighlight">\(\mu\)</span>，方差参数为 <span class="math notranslate nohighlight">\(\sigma^2\)</span>，
两个参数的似然估计量分别记作 <span class="math notranslate nohighlight">\(\hat{\mu}_{ML}\)</span> 和 <span class="math notranslate nohighlight">\(\hat{\sigma}^2_{ML}\)</span>
。</p>
<p><strong>均值参数的似然估计量</strong></p>
<p>均值参数的最大似然估计量就等于样本的均值统计量，</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-64">
<span class="eqno">(3.4.24)<a class="headerlink" href="#equation-glm-source-content-64" title="此公式的永久链接"></a></span>\[\hat{\mu}_{ML} = \bar{X} = \frac{X_1+X_2+\cdots+X_N}{N}\]</div>
<p>并且估计量 <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 的期望和方差分别为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-65">
<span class="eqno">(3.4.25)<a class="headerlink" href="#equation-glm-source-content-65" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mathbb{E}[\hat{\mu}_{ML}] = \mu\\V(\hat{\mu}_{ML}) = \frac{\sigma^2}{N}\end{aligned}\end{align} \]</div>
<p>显然，对于均值参数的似然估计量有</p>
<ul class="simple">
<li><p>根据弱大数定律，它相合估计，或者说一致性估计。</p></li>
<li><p>它是无偏估计量，它的偏差为 <span class="math notranslate nohighlight">\(0\)</span> ，因此它的均方误差是 <span class="math notranslate nohighlight">\(MSE=\sigma^2/N\)</span> 。</p></li>
<li><p>它的方差符合 <code class="docutils literal notranslate"><span class="pre">CRLB</span></code> 的下界，因此它是最小方差无偏估计，或者说是有效估计。</p></li>
<li><p>根据中心极限定理，它有 <strong>渐近正态性（asymptotic normality）</strong> ，其渐进服从正态分布 <span class="math notranslate nohighlight">\(N(\mu,\frac{\sigma^2}{N})\)</span> 。</p></li>
</ul>
<p><strong>方差参数的似然估计量</strong></p>
<p>随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的方差参数的似然估计量就是样本的方差，即</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-66">
<span class="eqno">(3.4.26)<a class="headerlink" href="#equation-glm-source-content-66" title="此公式的永久链接"></a></span>\[\hat{\sigma}^2_{ML}  = \frac{\sum_{i=1}^N (X_i - \bar{X})^2 }{N}\]</div>
<p>现在我们也来看下方差估计量的期望值，在计算前，先给出如下几个事实。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-67">
<span class="eqno">(3.4.27)<a class="headerlink" href="#equation-glm-source-content-67" title="此公式的永久链接"></a></span>\[\mathbb{E}[\bar{X}] = \mu,\quad
\mathbb{E}[X_i^2] = \mu^2 + \sigma^2,\quad
\mathbb{E}[\bar{X}^2] = \mu^2 + \frac{\sigma^2}{N}\]</div>
<p>估计量 <span class="math notranslate nohighlight">\(\hat{\sigma}^2_{ML}\)</span> 的期望为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-68">
<span class="eqno">(3.4.28)<a class="headerlink" href="#equation-glm-source-content-68" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mathbb{E}[\hat{\sigma}^2_{ML}] &amp;=  \mathbb{E} \left [ \frac{\sum_{i=1}^N (X_i - \bar{X})^2 }{N} \right ]\\&amp;= \frac{1}{N}  \mathbb{E} \left [\sum_{i=1}^N (X_i - \bar{X})^2 \right ]\\&amp;= \frac{1}{N}  \mathbb{E} \left [\sum_{i=1}^N ( X_i^2 -  2 X_i \bar{X} + \bar{X}^2 ) \right ]\\&amp;= \frac{1}{N}  \mathbb{E} \left [\sum_{i=1}^N  X_i^2 -  2  \bar{X} \sum_{i=1}^N X_i + N \bar{X}^2   \right ]\\&amp;= \mathbb{E} \left [ \frac{1}{N}  \sum_{i=1}^N  X_i^2 -  \frac{2  \bar{X} \sum_{i=1}^N X_i}{N} +  \bar{X}^2   \right ]\\&amp;= \mathbb{E} \left [ \frac{1}{N}  \sum_{i=1}^N  X_i^2 -  2  \bar{X}^2 +  \bar{X}^2   \right ]\\&amp;= \mathbb{E} \left [ \frac{1}{N}  \sum_{i=1}^N  X_i^2 -  \bar{X}^2   \right ]\\&amp;= \frac{N(\mu^2 +\sigma^2)}{N}  - \left (  \mu^2 + \frac{\sigma^2}{N} \right )\\&amp;= \frac{N-1}{N} \sigma^2\end{aligned}\end{align} \]</div>
<p>可以看到 <strong>方差似然估计量的期望不等于方差参数真值，因此它是一个有偏估计量</strong>。
但是当 <span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span> 时，它们是相等的，因此 <strong>方差似然估计量是渐近无偏的</strong>，
同时它也是也是渐近正态性的。</p>
<p>虽然方差的似然估计量是有偏的，但是可以做一个简单的变换得到一个无偏的估计量，
显然只需要乘上 <span class="math notranslate nohighlight">\(N/(N-1)\)</span> 即可。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-69">
<span class="eqno">(3.4.29)<a class="headerlink" href="#equation-glm-source-content-69" title="此公式的永久链接"></a></span>\[\hat{\sigma}^2_{\text{无偏}} = \frac{ N}{N-1} \hat{\sigma}^2_{ML} = \frac{\sum_{i=1}^N (X_i - \bar{X})^2 }{N-1}\]</div>
<p>当样本数量 <span class="math notranslate nohighlight">\(N\)</span> 足够大时 <span class="math notranslate nohighlight">\(\hat{\sigma}^2_{\text{无偏}}\)</span> 与 <span class="math notranslate nohighlight">\(\hat{\sigma}^2_{ML}\)</span>
其实没有太大的区别。</p>
<p>最大似然估计还有一个特别的性质，它遵循 <strong>不变原理</strong>：
如果 <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> 是 <span class="math notranslate nohighlight">\(\theta\)</span> 的最大似然估计，
那么对于任意关于 <span class="math notranslate nohighlight">\(\theta\)</span> 的一一映射函数 <span class="math notranslate nohighlight">\(h\)</span>， <span class="math notranslate nohighlight">\(H=h(\theta)\)</span> 的最大似然估计是 <span class="math notranslate nohighlight">\(h(\hat{\theta})\)</span>
。对于独立同分布的观测，在一些适合的假设条件下，最大似然估计量是相合的或者说一致的。</p>
<p>另一个有趣的性质是当 <span class="math notranslate nohighlight">\(\theta\)</span> 是标量参数的时候，在某些合适的条件下，
最大似然估计量具有 <strong>渐近正态</strong> 性质。
特别地，可以看到 <span class="math notranslate nohighlight">\((\hat{\theta}-\theta)/V(\hat{\theta})\)</span>
的分布接近标准正态分布。
因此，如果我们还能够估计出 <span class="math notranslate nohighlight">\(V(\hat{\theta}\)</span>
就能进一步得到基于正态近似的误差方差估计。
当 <span class="math notranslate nohighlight">\(\theta\)</span> 是向量参数，针对每个分量都可以得到类似结论。</p>
<aside class="topic">
<p class="topic-title">渐近正态性(Asymptotic normality)</p>
<p>我们说一个估计量是渐近正态性的，如果满足：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-70">
<span class="eqno">(3.4.30)<a class="headerlink" href="#equation-glm-source-content-70" title="此公式的永久链接"></a></span>\[\sqrt{N}(\hat{\theta}- \theta_{\text{true}}) \rightarrow^d \mathcal{N}(0,\frac{1}{I(\theta)})\]</div>
<p>或者</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-71">
<span class="eqno">(3.4.31)<a class="headerlink" href="#equation-glm-source-content-71" title="此公式的永久链接"></a></span>\[\hat{\theta} \rightarrow^d \mathcal{N}(\theta_{\text{true}},\frac{1}{N I(\theta)})\]</div>
</aside>
<p><strong>渐近正态性对应着中心极限定理，最大似然估计是满足渐近正态性的。</strong>
似然估计量不仅是渐近服从正态分布，而且是以参数真实值为期望的正态分布，
这表明似然估计量依概率(正态分布)收敛于参数的真实值，这符合一致性的定义。
显然极大似然估计是一致性估计。
由于似然估计是一致性估计，似然估计量是渐近收敛于参数真实值的，也就是估计量的偏差渐近为 <span class="math notranslate nohighlight">\(0\)</span>，
因此可以得出似然估计量是 <strong>渐近无偏估计</strong> 。</p>
<p>我们知道估计量的 <code class="docutils literal notranslate"><span class="pre">MSE</span></code> 是由偏差和方差组成的（ <a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#equation-eq-2-39">公式(2.7.2)</a> ），无偏性是对估计量的偏差的评价，
而估计量的方差影响着估计值的稳定性，方差越小估计量就越稳定，
并且最大似然估计量的方差符合 <code class="docutils literal notranslate"><span class="pre">CRLB</span></code> 的下界，就等于费歇尔信息的倒数，
如果 <span class="math notranslate nohighlight">\(\theta\)</span> 是参数向量，估计量的方差为协方差矩阵，
此时费歇尔信息 <span class="math notranslate nohighlight">\(I(\theta)\)</span>
为信息矩阵(Information matrix)。</p>
<div class="math notranslate nohighlight" id="equation-eq-2-60">
<span class="eqno">(3.4.32)<a class="headerlink" href="#equation-eq-2-60" title="此公式的永久链接"></a></span>\[\text{Cov}(\hat{\theta}_{ML}) = [I(\theta)]^{-1}\]</div>
<p>这里我们省略证明过程，有兴趣的读者可以参考其他资料。
显然似然估计不仅仅是渐近无偏估计，而且估计量的方差就等于 <code class="docutils literal notranslate"><span class="pre">CRLB</span></code> 定理的下界，
因此似然估计量是不仅仅是有效估计量，而是其最小方差无偏估计(Minimum Variance Unbiased Estimator,MVUE)，
<strong>并且我们可以通过</strong> <span class="math notranslate nohighlight">\(I(\theta)\)</span> <strong>量化衡量MLE估计量的方差。</strong></p>
<p>当我们用最大似然估计出一个参数的估计值后，我们期望能量化评估出这个参数估计值的好坏，
大家常用的方法是计算观测值的误差：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-72">
<span class="eqno">(3.4.33)<a class="headerlink" href="#equation-glm-source-content-72" title="此公式的永久链接"></a></span>\[\text{MSE} = \frac{1}{N} \sum_i^N (y_i-\hat{y}_i)^2\]</div>
<p>这种方法衡量的是整个模型的预测效果，并不能衡量出参数的估计值和参数的最优值之间的误差，
我们已经知道最大似然估计是无偏估计，那么最终最大似然估计量的误差就可以用如下公式衡量：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-73">
<span class="eqno">(3.4.34)<a class="headerlink" href="#equation-glm-source-content-73" title="此公式的永久链接"></a></span>\[\text{Standard Errors} = \sqrt{V(\hat{\theta}_{ML})} = \sqrt{ \text{diag} ([I(\theta)]^{-1})}\]</div>
<p><span class="math notranslate nohighlight">\([I(\theta)]^{-1}\)</span> 是协方差矩阵，其对角线元素是每个参数方差，开根号后得到每个参数的标准差。</p>
<p>最后我们总结下最大似然估计拥有的特性：</p>
<ul class="simple">
<li><p>最大似然估计量符合大数定律，它是一致性(consistency)估计，或者数相合估计。</p></li>
<li><p>由于满足一致性，渐近收敛于参数真实值，渐近偏差为 <span class="math notranslate nohighlight">\(0\)</span>，因此它满足渐近无偏性(unbiased)。</p></li>
<li><p>根据中心极限定理，它有 <strong>渐近正态性（asymptotic normality）</strong> ，其渐进服从正态分布 <span class="math notranslate nohighlight">\(N(\mu,\frac{\sigma^2}{N})\)</span> 。</p></li>
<li><p>最大似然估计量的方差符合 <code class="docutils literal notranslate"><span class="pre">CRLB</span></code> 定理的下限，所以是有效估计(Efficient Estimator)，并且是最小方差无偏估计。</p></li>
</ul>
<p>最大似然估计量这些特性都是 <strong>渐近的</strong>，即当观测样本数量 <span class="math notranslate nohighlight">\(N\)</span> 足够大时才能显现出来，
好处是对观测变量所属的分布没有任何要求，即不管观测变量服从什么概率分布，最大似然估计都有这些渐近特性。
有一个例外是，如果观测变量的概率分布是正态分布，则 <strong>不再是渐近的</strong>，而是 <strong>精确的</strong>。</p>
</section>
</section>
<section id="ch-influence-test-interval">
<span id="id16"></span><h2><span class="section-number">3.5. </span>置信区间<a class="headerlink" href="#ch-influence-test-interval" title="此标题的永久链接"></a></h2>
<p>在统计学中，由样本数据估计总体分布所含未知参数的真实值，所得到的值，称为估计值。
最大似然为我们提供了一个利用样本估计总体未知参数的良好方法，
通过上一节的内容，我们已经知道最大似然估计量拥有非常好的性质，
多数情况下，能为我们提供一个良好的参数估计值。
<strong>我们把这种估计结果使用一个点的数值表示“最佳估计值”方法，称为点估计（point estimation）</strong>。</p>
<p>我们知道估计量是一个随机变量，通过现有的样本算出一个具体的估计值，不同的样本算出的估计值也是不同的。
最大似然估计量理论上也只是 <strong>依概率</strong> 收敛于参数真值的，
因此通常我们使用某个具体的样本算出的估计值和真实值还是不一样的，
估计值和真实值之间到底相差多少，
点估计并没有给出。
本节我们讨论统计学中另一种参数估计方法：<strong>区间估计(interval estimate)</strong>，
也叫 <strong>置信区间（confidence interval）</strong>，
相比于点估计，它能给出有关估计值的更多信息。</p>
<p>样本的点估计值并不是完全等于总体参数真实值的，虽然很接近，但还是存在一定误差的。
在统计学，我们不能用”可能”、”大概”、”也许吧”这样的字眼去描述这个误差，
而是需要给出去一个量化的描述方法，
就像本章开篇引言说的那样。</p>
<blockquote>
<div><div class="line-block">
<div class="line">统计推断除了结论之外，还需要说明结论的不确定程度。–《统计学的世界》</div>
</div>
</div></blockquote>
<p>我们已经知道，样本的均值统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 可以作为总体均值参数 <span class="math notranslate nohighlight">\(\mu\)</span> 的点估计量，
而统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 是一个随机量，即不同的样本会得到不同的值，其渐近服从正态分布。</p>
<div class="math notranslate nohighlight" id="equation-eq-estimator-eval-085">
<span class="eqno">(3.5.1)<a class="headerlink" href="#equation-eq-estimator-eval-085" title="此公式的永久链接"></a></span>\[\hat{\mu} = \bar{X} \sim \mathcal{N}(\mu,\sigma^2/N)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 分布是总体的均值参数和方差参数，
<span class="math notranslate nohighlight">\(N\)</span> 是样本的容量。</p>
<figure class="align-center" id="id25">
<span id="pic-influence"></span><a class="reference internal image-reference" href="../../../_images/均值统计量正态分布.jpg"><img alt="../../../_images/%E5%9D%87%E5%80%BC%E7%BB%9F%E8%AE%A1%E9%87%8F%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83.jpg" src="../../../_images/%E5%9D%87%E5%80%BC%E7%BB%9F%E8%AE%A1%E9%87%8F%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 3.5.1 </span><span class="caption-text">均值统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 服从正态分布，样本容量 <span class="math notranslate nohighlight">\(N\)</span> 越大其标准差越小。</span><a class="headerlink" href="#id25" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>从 <a class="reference internal" href="#pic-influence"><span class="std std-numref">图 3.5.1</span></a> 可以看出，
均值统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 的值有 <span class="math notranslate nohighlight">\(68.2\%\)</span> 的概率落在区间 <span class="math notranslate nohighlight">\(\mu \pm \sigma/\sqrt{N}\)</span>
范围内，我们知道 <span class="math notranslate nohighlight">\(\mu\)</span> 是总体的真实均值参数，也就是说样本估计值 <span class="math notranslate nohighlight">\(\bar{X}\)</span>
有68.2%的概率和总体真实值 <span class="math notranslate nohighlight">\(\mu\)</span> 之间的误差在一个标准差的范围  <span class="math notranslate nohighlight">\(\pm \sigma/\sqrt{N}\)</span> 内。</p>
<p>但是总体均值参数 <span class="math notranslate nohighlight">\(\mu\)</span> 是未知的，无法得知区间 <span class="math notranslate nohighlight">\(\mu \pm \sigma/\sqrt{N}\)</span> 的具体范围，
这时可以反转一下。
既然 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 有 <span class="math notranslate nohighlight">\(68.2\%\)</span> 的概率落在区间 <span class="math notranslate nohighlight">\(\mu \pm \sigma/\sqrt{N}\)</span>
，反过来就是，<span class="math notranslate nohighlight">\(\mu\)</span> 有 <span class="math notranslate nohighlight">\(68.2\%\)</span> 的概率在区间 <span class="math notranslate nohighlight">\(\hat{\mu} \pm \sigma/\sqrt{N}\)</span> 内，
如 <a class="reference internal" href="#fg-influence"><span class="std std-numref">图 3.5.2</span></a> 所示，
<span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 是总体均值 <span class="math notranslate nohighlight">\(\mu\)</span> 的一个具体估计值
（均值统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 的一个具体值），
<span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 落在区间
<span class="math notranslate nohighlight">\([\mu-\sigma/\sqrt{N},\mu+\sigma/\sqrt{N}]\)</span>
也可以看成是 <span class="math notranslate nohighlight">\(\mu\)</span> 在区间
<span class="math notranslate nohighlight">\([\hat{\mu}-\sigma/\sqrt{N},\hat{\mu}+\sigma/\sqrt{N}]\)</span>
。</p>
<figure class="align-center" id="id26">
<span id="fg-influence"></span><a class="reference internal image-reference" href="../../../_images/置信区间.jpg"><img alt="../../../_images/%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4.jpg" src="../../../_images/%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 3.5.2 </span><span class="caption-text">估计值 <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 与总体期望 <span class="math notranslate nohighlight">\(\mu\)</span> 位置是相对的，黄色曲线是蓝色曲线的一个平移。</span><a class="headerlink" href="#id26" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>上面的例子中，我们给出的置信区间是上下一个标准差的范围，置信区间的范围可以根据实际情况调整。
我们用 <span class="math notranslate nohighlight">\(\delta\)</span> 表示区间的距离中心点的距离，
则这个区间可以表示成 <span class="math notranslate nohighlight">\([\hat{\mu}-\delta,\hat{\mu}+\delta]\)</span>，
这个区间的概率记为 <span class="math notranslate nohighlight">\(1-\alpha\)</span>，
则可以记为</p>
<div class="math notranslate nohighlight" id="equation-eq-estimator-eval-086">
<span class="eqno">(3.5.2)<a class="headerlink" href="#equation-eq-estimator-eval-086" title="此公式的永久链接"></a></span>\[P( \hat{\mu} - \delta \leq \mu \leq \hat{\mu}+a) = 1-\alpha\]</div>
<p>区间 <span class="math notranslate nohighlight">\([\text{估计值} \pm \text{误差范围}]\)</span> 称为 <strong>置信区间（confidence interval）</strong>，
<span class="math notranslate nohighlight">\(1-\alpha\)</span> 称为 <strong>置信度（confidence level），也叫置信系数（confidence coefficient）</strong>。
而 <span class="math notranslate nohighlight">\(\alpha(0&lt;\alpha&lt;1)\)</span> 则被称为 <strong>显著（性）水平（level of significance）</strong>，
<span class="math notranslate nohighlight">\(\alpha\)</span> 的值通常是事先就确定好的，显然 <span class="math notranslate nohighlight">\(\alpha\)</span> 越大，
置信区间的范围就越小，一般会选择 <span class="math notranslate nohighlight">\(0.01,0.05\)</span> 这样的值。
置信区间的端点被称为 <strong>置信限（confidence limits）或者临界值（critical values）</strong>，
<span class="math notranslate nohighlight">\(\hat{\mu} - \delta\)</span> 称为置信下限（lower confidence limit）,
而 <span class="math notranslate nohighlight">\(\hat{\mu} + \delta\)</span> 称为置信上限（upper confidence limit）。</p>
<p>在 <span class="math notranslate nohighlight">\(\alpha\)</span> 确定的条件下，置信区间的范围和 <span class="math notranslate nohighlight">\(\delta\)</span> 相关，
而 <span class="math notranslate nohighlight">\(\delta\)</span> 和估计量的标准误差相关，标准误越大，置信区间越宽。
换句话说，估计量的标准误越大，对未知参数的真值进行估计的不确定性越大。
因此，估计量的标准误常被喻为估计量的 <strong>精度</strong>，
即用估计量去测定真实的总体值有多精确。</p>
<p>相比于原本的似然估计，置信区间给出了一个参数估计区间，因此也称作 <strong>区间估计（interval estimate）</strong>，
顾名思义，区间估计给出的是一个可能包含参数真值的区间。
与之相对的，点估计给出是一个具体的估计（点）值，
相比单纯的点估计，区间估计提供了更加丰富的信息。</p>
<p>置信区间利用了参数估计量的抽样分布，当估计量是无偏估计量时，估计量的期望就是总体参数的真实值，
注意置信区间是根据参数估计量给出的，因此这个区间是随机（量）的，
而参数的真值是一个固定的数值，不是随机值。
因此置信区间解读成：<strong>随机（置信）区间包含参数真值的概率是</strong> <span class="math notranslate nohighlight">\(1-\alpha\)</span>。
不能说成： <em>参数真值落在这个区间的概率是</em> <span class="math notranslate nohighlight">\(1-\alpha\)</span> 。</p>
<p>概率分布的常见参数有均值参数 <span class="math notranslate nohighlight">\(\mu\)</span> 和方差参数 <span class="math notranslate nohighlight">\(\sigma\)</span>
，这两个参数会一直贯穿本书的全部内容，
为了让大家更深刻的理解，这里我们分别给出两个参数的区间估计的过程。
比较特殊的一点是，均值参数估计量 <span class="math notranslate nohighlight">\(\hat{\mu}=\bar{X}\)</span>
的抽样分布有两种情况，
当已知总体方差 <span class="math notranslate nohighlight">\(\sigma\)</span> 或者样本数量足够大时，
<span class="math notranslate nohighlight">\(\hat{\mu}\)</span>  的抽样分布可以选择标准正态分布（ <span class="math notranslate nohighlight">\(Z\)</span> 统计量）
，反之需要使用学生t分布（<span class="math notranslate nohighlight">\(T\)</span> 统计量），
两种情况我们都简要介绍一下。</p>
<section id="z">
<h3><span class="section-number">3.5.1. </span>均值参数的 Z 区间估计<a class="headerlink" href="#z" title="此标题的永久链接"></a></h3>
<p>虽然已知均值参数估计量的抽样分布是（渐近）正态分布 <a class="reference internal" href="#equation-eq-estimator-eval-085">公式(3.5.1)</a>，
但是在计算技术普及前，非标准正态分布的概率值不是很方便计算，
所以通常会转成服从标准正态分布的 <span class="math notranslate nohighlight">\(Z\)</span> 统计量。</p>
<div class="math notranslate nohighlight" id="equation-eq-estimator-eval-085x">
<span class="eqno">(3.5.3)<a class="headerlink" href="#equation-eq-estimator-eval-085x" title="此公式的永久链接"></a></span>\[Z = \frac{\hat{\mu} - \mu}{ \frac{\sigma}{\sqrt{N}}  } \sim \mathcal{N}(0,1)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mu\)</span> 是总体均值参数的真值，<span class="math notranslate nohighlight">\(\sigma\)</span> 是总体方差参数的真值，
<span class="math notranslate nohighlight">\(N\)</span> 是观测样本的数量。
根据置信区间的公式（ <a class="reference internal" href="#equation-eq-estimator-eval-086">公式(3.5.2)</a>），需要找到一个概率为 <span class="math notranslate nohighlight">\(1-\alpha\)</span>
的区间。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-74">
<span class="eqno">(3.5.4)<a class="headerlink" href="#equation-glm-source-content-74" title="此公式的永久链接"></a></span>\[P( \delta_1 \leq Z \leq \delta_2 ) = 1-\alpha\]</div>
<p>由于 <span class="math notranslate nohighlight">\(Z\)</span> 是标准正态分布，区间 <span class="math notranslate nohighlight">\([\delta_1,\delta_2]\)</span> 是以 <span class="math notranslate nohighlight">\(0\)</span> 点为中心左右对称的，
可以记为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-75">
<span class="eqno">(3.5.5)<a class="headerlink" href="#equation-glm-source-content-75" title="此公式的永久链接"></a></span>\[P( -\delta \leq \frac{\hat{\mu} - \mu}{ \frac{\sigma}{\sqrt{N}}} \leq \delta ) = 1-\alpha\]</div>
<p><span class="math notranslate nohighlight">\(\alpha\)</span> 的值是事前指定的，假设为 <span class="math notranslate nohighlight">\(5\%\)</span>
，则 <span class="math notranslate nohighlight">\(1-\alpha=95\%\)</span>，
根据标准正态分布概率密度的划分情况，可以近似认为在两个标准差的范围，而 <span class="math notranslate nohighlight">\(Z\)</span> （标准正态分布）的标准差是 <span class="math notranslate nohighlight">\(1\)</span>，
因此有 <span class="math notranslate nohighlight">\(\delta=2\)</span>。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-76">
<span class="eqno">(3.5.6)<a class="headerlink" href="#equation-glm-source-content-76" title="此公式的永久链接"></a></span>\[P( -2 \leq \frac{\hat{\mu} - \mu}{ \frac{\sigma}{\sqrt{N}}} \leq 2) =  0.95\]</div>
<p>进一步移项可得</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-77">
<span class="eqno">(3.5.7)<a class="headerlink" href="#equation-glm-source-content-77" title="此公式的永久链接"></a></span>\[P( \hat{\mu} - \frac{2\sigma}{\sqrt{N}}  \leq \mu \leq \hat{\mu} + \frac{2\sigma}{\sqrt{N}}) =  0.95\]</div>
<p><span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 是先一步利用最大似然估计得到的估计值，在这里是已知的。
如果总体的方差参数 <span class="math notranslate nohighlight">\(\sigma\)</span> 是已知的，这里就已经结束了，已经得到了 <span class="math notranslate nohighlight">\(95\%\)</span> 的置信的区间。
然而实际应用中，<span class="math notranslate nohighlight">\(\sigma\)</span> 通常是未知的，此时如果你的样本数量足够多，
就可以使用 <span class="math notranslate nohighlight">\(\sigma\)</span> 的一个无偏估计值替代。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-78">
<span class="eqno">(3.5.8)<a class="headerlink" href="#equation-glm-source-content-78" title="此公式的永久链接"></a></span>\[\hat{\sigma} = \frac{\sum_{i=1}  (X_i - \bar{X})^2  }{N-1}\]</div>
<p>最后，利用 <span class="math notranslate nohighlight">\(Z\)</span> 统计量得到的均值参数的 <span class="math notranslate nohighlight">\(95\%\)</span> 置信区间为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-79">
<span class="eqno">(3.5.9)<a class="headerlink" href="#equation-glm-source-content-79" title="此公式的永久链接"></a></span>\[\left [ \hat{\mu} - \frac{2 \hat{\sigma}}{\sqrt{N}} ,\hat{\mu} + \frac{2 \hat{\sigma} }{\sqrt{N}} \right ]\]</div>
<figure class="align-center" id="id27">
<span id="id17"></span><a class="reference internal image-reference" href="../../../_images/标准正态分布置信区间.jpg"><img alt="../../../_images/%E6%A0%87%E5%87%86%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4.jpg" src="../../../_images/%E6%A0%87%E5%87%86%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 3.5.3 </span><span class="caption-text">标准正态分布95%置信区间 <span class="math notranslate nohighlight">\([-2,2]\)</span></span><a class="headerlink" href="#id27" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
</section>
<section id="id18">
<h3><span class="section-number">3.5.2. </span>均值参数的 T 区间估计<a class="headerlink" href="#id18" title="此标题的永久链接"></a></h3>
<p>在 <a class="reference internal" href="#ch-sample-distribution-t"><span class="std std-numref">节 3.2.2</span></a> 讲过，
当总体的方差参数未知或者样本数量小于 <span class="math notranslate nohighlight">\(30\)</span> 的时候，
均值统计量的抽样分布可以用学生t分布替代。
这时在得到对均值参数的置信区间时就要使用学生t分布代替标准正态分布，
实现起来比较简单，是需要把 <span class="math notranslate nohighlight">\(Z\)</span> 统计量换成 <span class="math notranslate nohighlight">\(T\)</span> 统计量。</p>
<div class="math notranslate nohighlight" id="equation-eq-estimator-eval-090">
<span class="eqno">(3.5.10)<a class="headerlink" href="#equation-eq-estimator-eval-090" title="此公式的永久链接"></a></span>\[T = \frac{\hat{\mu} - \mu}{ \frac{ \hat{\sigma}}{\sqrt{N}}  } \sim T(N-1)\]</div>
<div class="math notranslate nohighlight" id="equation-glm-source-content-80">
<span class="eqno">(3.5.11)<a class="headerlink" href="#equation-glm-source-content-80" title="此公式的永久链接"></a></span>\[P_{T}( -\delta \leq \frac{\hat{\mu} - \mu}{ \frac{\hat{\sigma}}{\sqrt{N}}} \leq \delta ) = 95\%\]</div>
<p>对于t分布，它的概率区间就不是用标准差来分割了，需要查询t分布临界表或者用计算机去计算得到，
t分布的概率是和自由度（<span class="math notranslate nohighlight">\(N-1\)</span>）相关的，
假设 <span class="math notranslate nohighlight">\(N=30\)</span>，通过查表可得自由度为 <span class="math notranslate nohighlight">\(29\)</span> 的t分布 <span class="math notranslate nohighlight">\(95\%\)</span> 的区间为边界
<span class="math notranslate nohighlight">\(\delta=2.045\)</span>，
这比标准正态分布（<span class="math notranslate nohighlight">\(Z\)</span>）的 <span class="math notranslate nohighlight">\(2\)</span> 稍微大了一点。
最后，利用 <span class="math notranslate nohighlight">\(T\)</span> 统计量得到的均值参数的 <span class="math notranslate nohighlight">\(95\%\)</span> 置信区间为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-81">
<span class="eqno">(3.5.12)<a class="headerlink" href="#equation-glm-source-content-81" title="此公式的永久链接"></a></span>\[\left [ \hat{\mu} - \frac{2.045 \hat{\sigma}}{\sqrt{N}} ,\hat{\mu} + \frac{2.045 \hat{\sigma} }{\sqrt{N}} \right ]\]</div>
<figure class="align-center" id="id28">
<span id="fg-influence-t"></span><a class="reference internal image-reference" href="../../../_images/t分布置信区间.jpg"><img alt="../../../_images/t%E5%88%86%E5%B8%83%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4.jpg" src="../../../_images/t%E5%88%86%E5%B8%83%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 3.5.4 </span><span class="caption-text">自由度为29的t分布的95%置信区间 <span class="math notranslate nohighlight">\([-2.045,2.045]\)</span>，相比标准正态分布的95%区间 <span class="math notranslate nohighlight">\([-2,2]\)</span> 稍微大了一些</span><a class="headerlink" href="#id28" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
</section>
<section id="id19">
<h3><span class="section-number">3.5.3. </span>方差参数的区间估计<a class="headerlink" href="#id19" title="此标题的永久链接"></a></h3>
<p>我们已经知道方差参数的无偏估计量 <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span>
是和卡方分布相关的，如下统计量服从自由度为 <span class="math notranslate nohighlight">\(N-1\)</span> 的卡方分布。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-82">
<span class="eqno">(3.5.13)<a class="headerlink" href="#equation-glm-source-content-82" title="此公式的永久链接"></a></span>\[\chi^2 = \frac{N\hat{\sigma}^2}{\sigma^2} \sim \chi^2(N-1)\]</div>
<p>获取方差估计量置信区间的过程和上面的均值参数的基本是一样的，
唯一注意的地方是，卡方分布概率密度函数不再是对称的，上界和下界不再对称。
<span class="math notranslate nohighlight">\(1-\alpha\)</span> 的概率区间，相当于是在左右两边各扣除 <span class="math notranslate nohighlight">\(\alpha/2\)</span> 的概率区间，
也就是在分布的左边去掉 <span class="math notranslate nohighlight">\(\alpha/2\)</span> 的概率区间，在分布的右边也去掉 <span class="math notranslate nohighlight">\(\alpha/2\)</span>
的概率区间</p>
<div class="math notranslate nohighlight" id="equation-eq-estimator-eval-095">
<span class="eqno">(3.5.14)<a class="headerlink" href="#equation-eq-estimator-eval-095" title="此公式的永久链接"></a></span>\[P( \delta_1 \leq  \frac{N\hat{\sigma}^2}{\sigma^2} \leq \delta_2 ) = 1-\alpha\]</div>
<figure class="align-center" id="id29">
<span id="id20"></span><a class="reference internal image-reference" href="../../../_images/卡方分布置信区间.jpg"><img alt="../../../_images/%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4.jpg" src="../../../_images/%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83%E7%BD%AE%E4%BF%A1%E5%8C%BA%E9%97%B4.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 3.5.5 </span><span class="caption-text">卡方分布置信区间，两侧各有 <span class="math notranslate nohighlight">\(\alpha/2\)</span> 的区域被剔除。</span><a class="headerlink" href="#id29" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>然后通过查询卡方分布临界表得到分别得到左边界 <span class="math notranslate nohighlight">\(\delta_1\)</span>
和有边界 <span class="math notranslate nohighlight">\(\delta_2\)</span> 的值，也可以利用 <code class="docutils literal notranslate"><span class="pre">python</span></code>
中 <code class="docutils literal notranslate"><span class="pre">scipy</span></code> 数学工具包计算得到。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>
<span class="c1"># 显著水平为5%</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="c1"># 自有度为 15</span>
<span class="n">df</span> <span class="o">=</span> <span class="mi">15</span>
<span class="c1"># 左侧边界</span>
<span class="n">delta_1</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span><span class="mi">15</span><span class="p">)</span>
<span class="c1"># 右侧边界</span>
<span class="n">delta_2</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
<p>最后调整下 <a class="reference internal" href="#equation-eq-estimator-eval-095">公式(3.5.14)</a> 得到方差参数的置信区间。</p>
<div class="math notranslate nohighlight" id="equation-eq-estimator-eval-096">
<span class="eqno">(3.5.15)<a class="headerlink" href="#equation-eq-estimator-eval-096" title="此公式的永久链接"></a></span>\[P \left [  (N-1) \frac{\hat{\sigma}^2}{\delta_1} \leq \sigma^2 \leq  (N-1) \frac{\hat{\sigma}^2}{\delta_2} \right ] = 1-\alpha\]</div>
</section>
</section>
<section id="ch-influence-test-test">
<span id="id21"></span><h2><span class="section-number">3.6. </span>简单假设检验<a class="headerlink" href="#ch-influence-test-test" title="此标题的永久链接"></a></h2>
<p>统计推断是利用样本数据来对总体得出结论。
点估计是使用样本统计量来估计总体参数，但点估计量并不是准确无误的。
置信区间，又叫区间估计，给出了点估计量的不确定性程度。本节介绍统计推断中另一种推断的方法，假设检验(hypothesis testing)。
假设检验(hypothesis testing)，或者显著性检验(significance testing)是用来处理有关总体参数或者总体分布的断言。
<strong>不同于点估计和区间估计，假设检验不是用来估计总体参数的，而是用来判断对于总体(参数)的某个假设是否成立。</strong></p>
<p>在日常生活中，经常会遇到这样一种情况，我们已经对总体有一个了猜测或者断言，需要去验证这个猜测是不是”正确”的，
或者说这个猜测有多大可能性是正确的。然而总体的真实情况我们是无法得知的，这时就只能通过样本去验证这个猜测，
这就是假设检验做的事情。</p>
<p>举个例子说明下，假设有一个学者发表了一篇关于国人身高的论文，论文中声称国人的平均身高为165cm。
你对这个值有些怀疑，你想验证下这个值是否可信。
然而你又不可能统计出全国所有人民的身高去验证专家的结论是否正确。
通常的做法是，自己随机选择一些身高数据作为样本，然后算出样本的平均值，假设你算出来是160cm，
和学者公布的165cm有些差异。然而这个差异能说明专家声明的165cm是错误的么?
我们知道样本的均值统计量是一个随机量，不同的采样会得到不同的统计值。
那么，这5cm的差异是由于样本的随机性导致的，还是专家的声明是错误的呢？
这可以通过假设检验给出结论。</p>
<p>假设检验(hypothesis testing)，又叫显著性检验(significance testing)，
检验的过程一般可以抽象成四个步骤。</p>
<p><strong>步骤1. 陈述假设</strong></p>
<p>通常我们把对总体的假设称为零假设(null hypothesis)，通常用符号 <span class="math notranslate nohighlight">\(H_0\)</span> 表示，
读作”H零”。<span class="math notranslate nohighlight">\(H_0\)</span> 是对总体的一个假设或者说断言，它是一个虚拟的假设。
比如在我们的例子中，零假设就是：假设专家的声明是正确的，即国人身高的总体均值是165cm。
这是我们做出的一个虚拟假设，
用符号表示记作：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-83">
<span class="eqno">(3.6.1)<a class="headerlink" href="#equation-glm-source-content-83" title="此公式的永久链接"></a></span>\[H_0: \mu = 165\]</div>
<aside class="topic">
<p class="topic-title">零假设 <span class="math notranslate nohighlight">\(H_0\)</span></p>
<p>在统计学显著性检验中，被检验的断言叫作”零假设”(null hypothesis)，也叫初始假设。
<strong>检验主要评估否定零假设的证据有多强</strong>。</p>
</aside>
<p>和零假设相反的结论称为备择假设(alternative hypothesis)，也可以叫做对立假设，
通常用符号 <span class="math notranslate nohighlight">\(H_a\)</span> 表示。如果零假设 <span class="math notranslate nohighlight">\(H_0\)</span> 不成立，
就意味着备择假设 <span class="math notranslate nohighlight">\(H_a\)</span> 是成立的，通常二者是对立的。
在我们的例子中，备择假设为：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-84">
<span class="eqno">(3.6.2)<a class="headerlink" href="#equation-glm-source-content-84" title="此公式的永久链接"></a></span>\[H_a: \mu \neq 165\]</div>
<p>假设检验的过程，就是先假设 <span class="math notranslate nohighlight">\(H_0\)</span> 是正确的，然后在这个前提下寻找 <strong>否定</strong> <span class="math notranslate nohighlight">\(H_0\)</span> 的证据，
如果找到”证据”，并且这个证据足够强烈，就拒绝(reject) <span class="math notranslate nohighlight">\(H_0\)</span> ，接受 <span class="math notranslate nohighlight">\(H_a\)</span> ；
如果没有足够的”证据”，就接受(accept) <span class="math notranslate nohighlight">\(H_0\)</span> 。</p>
<p>经过前面的熏陶，我们已经了解到，在统计学中没有什么是绝对的，一切都是通过概率来描述。
这意味用来否定 <span class="math notranslate nohighlight">\(H_0\)</span> 的”证据”也不是绝对的，亦然是”概率”的，
所以用的是接受(accept)、拒绝(reject)这样的词，而不是其他准确判定的词。
因为我们找到的证据并不是百分百的证明 <span class="math notranslate nohighlight">\(H_0\)</span> 是错误的，
只能是从概率上认为 <span class="math notranslate nohighlight">\(H_0\)</span> 成立的可能性”比较小”，
所及拒绝了 <span class="math notranslate nohighlight">\(H_0\)</span> 选择了  <span class="math notranslate nohighlight">\(H_a\)</span> ，
<strong>假设检验只是一种从概率上选择最有可能的结果，而不是像数学上的证明一样给出绝对的对错</strong>。</p>
<p><strong>步骤2. 设定决策标准</strong></p>
<p>假设检验是要找到否定 <span class="math notranslate nohighlight">\(H_0\)</span> 的”证据”，
通常这种”证据”就是在 <span class="math notranslate nohighlight">\(H_0\)</span> 成立的条件下发生了一件”不可能”发生的事件，
所谓的”不可能事件”，就是一件概率很小的事件。
那么这个”不可能”的程度是多少，”概率很小”又有多小？这就需要给出一个标准。
这个标准称为显著水平(level of significance)。</p>
<aside class="topic">
<p class="topic-title">显著水平</p>
<p>显著水平(level of significance, or significance level)，
是判断小概率事件有多小的标准，显著水平值越小，意味这个事件发生概率越小，越极端。
通常用符号 <span class="math notranslate nohighlight">\(\alpha\)</span> 表示。</p>
</aside>
<p>显著水平 <span class="math notranslate nohighlight">\(\alpha\)</span> 通常会设置成5%、2%、1%等值，其含义是只要一个事件发生的概率小于等于
<span class="math notranslate nohighlight">\(\alpha\)</span> 就认为这是一件极端的小概率事件。
在假设检验中，如果在 <span class="math notranslate nohighlight">\(H_0\)</span> 成立的条件下，发生了一件概率小于等于 <span class="math notranslate nohighlight">\(\alpha\)</span>
的事件，认为 <span class="math notranslate nohighlight">\(H_0\)</span> 很可能是错误的，此时会拒绝 <span class="math notranslate nohighlight">\(H_0\)</span>。</p>
<p><strong>步骤3. 计算检验统计量</strong></p>
<p>有了检验标准后，就需要计算出一个值和这个标准比较，
计算这个值的统计量就称为检验统计量，检验统计量有很多种，
一般会根据实际的问题场景选择合适的检验统计量，
然后计算出这个检验统计量的值以及理论上得到这个值的可能性(概率)，
这个概率值称为P值(P-value)，最后把这个P值和检验标准值 <span class="math notranslate nohighlight">\(\alpha\)</span>
进行比较，并根据比较结果给出结论。</p>
<p>样本均值统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 的抽样分布是正态分布 <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,\sigma^2/N)\)</span> ，
通过样本统计量的抽样分布就能计算出样本统计值的发生概率。
在我们的例子中，在 <span class="math notranslate nohighlight">\(H_0\)</span> 成立的前提下，身高总体分布的均值（期望）就是 <span class="math notranslate nohighlight">\(\mu=165\)</span> ，
总体的方差未知，暂时用符号 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 表示，则抽样样本的均值统计量  <span class="math notranslate nohighlight">\(\bar{X}\)</span>
的抽样分布是</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-85">
<span class="eqno">(3.6.3)<a class="headerlink" href="#equation-glm-source-content-85" title="此公式的永久链接"></a></span>\[\bar{X} \sim \mathcal{N}(\mu=165,\sigma^2/N)\]</div>
<p>以上抽样分布的方差 <span class="math notranslate nohighlight">\(Var(\bar{X} )=\sigma^2/N\)</span> 是未知的，其中 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 是总体方差参数，
<span class="math notranslate nohighlight">\(N\)</span> 是抽样样本容量，通常样本容量 <span class="math notranslate nohighlight">\(N\)</span> 是已知的，假设抽样样本容量是100。
这时还需要得到总体方差参数 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 才可以，根据点估计的知识，可以用样本方差近似估计总体方差</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-86">
<span class="eqno">(3.6.4)<a class="headerlink" href="#equation-glm-source-content-86" title="此公式的永久链接"></a></span>\[\hat{\sigma}^2 = \frac{\sum_{i=1}^N (\bar{x}-x_i )^2 }{N-1}\]</div>
<p>这里我们假设算出来的总体方差估计值是 <span class="math notranslate nohighlight">\(\hat{\sigma}^2=36.0\)</span>，
则样本均值统计量的抽样分布的方差为 <span class="math notranslate nohighlight">\(\hat{\sigma}^2/N=36.0/100=0.36\)</span>
，在 <span class="math notranslate nohighlight">\(H_0\)</span> 成立的条件下，样本均值统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 的抽样分布就为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-87">
<span class="eqno">(3.6.5)<a class="headerlink" href="#equation-glm-source-content-87" title="此公式的永久链接"></a></span>\[\bar{X} \sim \mathcal{N}(165,0.36)\]</div>
<p>理论上样本统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 的期望是 <span class="math notranslate nohighlight">\(165\)</span>，方差是 <span class="math notranslate nohighlight">\(0.36\)</span>。
然后我们发现，从抽样样本计算得到样本均值为 <span class="math notranslate nohighlight">\(\bar{X}=160\)</span>。
理论上样本结果越接近 <span class="math notranslate nohighlight">\(165\)</span>，专家( <span class="math notranslate nohighlight">\(H_0\)</span> )是正确的可能性就越大；
样本均值结果偏离 <span class="math notranslate nohighlight">\(165\)</span> 越远，专家( <span class="math notranslate nohighlight">\(H_0\)</span> )是错误的可能性就越大。</p>
<p>那么样本统计值偏离期望值多远才叫小概率事件呢？总要有个判断标准。
这个标准就是我们在上个步骤中制定的显著水平 <span class="math notranslate nohighlight">\(\alpha\)</span> 。
<a class="reference internal" href="#pic-influence-me-100"><span class="std std-numref">图 3.6.1</span></a> 是均值统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 的抽样分布(正态分布)的概率分布曲线。
我们把曲线下方的面积分成两个区域，紧邻期望值两侧的中间区域称为 <strong>置信区间</strong>，
其面积是 <span class="math notranslate nohighlight">\(1-\alpha\)</span> ，<span class="math notranslate nohighlight">\(1-\alpha\)</span> 是这个区域的面积，也是 <span class="math notranslate nohighlight">\(\bar{X}\)</span>
落在这个区间的概率值，称为 <strong>置信水平</strong>。在假设检验中这个区域也叫作 <strong>接受域</strong>，表示我们接受零假设 <span class="math notranslate nohighlight">\(H_0\)</span>
的区域，样本统计值落在接受域的概率是 <span class="math notranslate nohighlight">\(1-\alpha\)</span>。
接受域两侧的阴影区域称为 <strong>拒绝域</strong>，表示拒绝零假设 <span class="math notranslate nohighlight">\(H_0\)</span> 的区域，其面积总和是 <span class="math notranslate nohighlight">\(\alpha\)</span> ，
样本统计值落在这个区间的概率是 <span class="math notranslate nohighlight">\(\alpha\)</span> 。
显著水平 <span class="math notranslate nohighlight">\(\alpha\)</span> 就是对这个”极端小概率”事件的一个标准，
如果统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 的值落在这个区间，我们就认为发生了小概率事件，此时选择拒绝零假设 <span class="math notranslate nohighlight">\(H_0\)</span>
。</p>
<figure class="align-center" id="id30">
<span id="pic-influence-me-100"></span><a class="reference internal image-reference" href="../../../_images/假设检验区域划分.jpg"><img alt="../../../_images/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E5%8C%BA%E5%9F%9F%E5%88%92%E5%88%86.jpg" src="../../../_images/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E5%8C%BA%E5%9F%9F%E5%88%92%E5%88%86.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 3.6.1 </span><span class="caption-text">标准正态分布的区域划分。阴影部分是拒绝域，左右两部分的概率和为 <span class="math notranslate nohighlight">\(\alpha\)</span>。
中间区域是接受域，它的概率是 <span class="math notranslate nohighlight">\(1-\alpha\)</span>。</span><a class="headerlink" href="#id30" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>下一步就是要算样本统计值 <span class="math notranslate nohighlight">\(160\)</span> 落在了抽样分布 <span class="math notranslate nohighlight">\(\mathcal{N}(165,0.36)\)</span>
的哪个区域，是落在了接受域还是拒绝域，落在不同的区域会导致我们对 <span class="math notranslate nohighlight">\(H_0\)</span>
做出不一样的选择。
然而在计算机普及之前，要计算出 <span class="math notranslate nohighlight">\(160\)</span> 在正态分布 <span class="math notranslate nohighlight">\(\mathcal{N}(165,0.36)\)</span>
哪个区域不是一件简单的事情，
因此通常并不直接使用均值统计量进行检验（验证）而是使用 Z 统计量，
Z 统计量就是均值统计量转化成标准正态分布。</p>
<aside class="topic">
<p class="topic-title">Z统计量</p>
<p>如下统计量称为 Z 统计量，Z 统计量的抽样分布是 <strong>标准正态分布</strong>。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-88">
<span class="eqno">(3.6.6)<a class="headerlink" href="#equation-glm-source-content-88" title="此公式的永久链接"></a></span>\[Z = \frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{N}}} \sim \mathcal{N}(0,1)\]</div>
</aside>
<p>有关 Z 统计量的推导我们在
<a class="reference internal" href="#ch-sample-distribution-normal"><span class="std std-numref">节 3.2.1</span></a>
和 <a class="reference internal" href="#ch-clt"><span class="std std-numref">节 3.3.4</span></a> 都有讲到过，可以回顾一下相关内容。
Z 统计量其实就是把服从非标准正态分布的样本均值统计量转换为一个服从标准正态的分布的统计量，
标准正态的分布方便进行检验计算，可以通过查表的方式得出 P 值。
Z 统计量也可以用来衡量样本均值结果值距离期望值有多少个标准差的距离，有时也叫作标准分。</p>
<p>现在我们把样本均值转成成 <span class="math notranslate nohighlight">\(Z\)</span> 的值，
通过计算可得 <span class="math notranslate nohighlight">\(Z=\frac{163-165}{\sqrt{0.36}}=-2/0.6=-3.34\)</span>
，意味着我们的检验统计量的样本值偏离其抽样分布理论期望值 <span class="math notranslate nohighlight">\(3.34\)</span>
个标准差（<span class="math notranslate nohighlight">\(Z\)</span> 的期望值为 <span class="math notranslate nohighlight">\(0\)</span>，标准差为 <span class="math notranslate nohighlight">\(1\)</span>）远，
负号代表是负偏离，小于期望值。如果是正数，就是大于期望值，是正偏离。</p>
<figure class="align-center" id="id31">
<span id="pic-influence-093"></span><a class="reference internal image-reference" href="../../../_images/Z检验-身高.jpg"><img alt="../../../_images/Z%E6%A3%80%E9%AA%8C-%E8%BA%AB%E9%AB%98.jpg" src="../../../_images/Z%E6%A3%80%E9%AA%8C-%E8%BA%AB%E9%AB%98.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 3.6.2 </span><span class="caption-text">当 <span class="math notranslate nohighlight">\(\alpha=0.05\)</span> 时，左右边界分别是 <span class="math notranslate nohighlight">\(-1.96\)</span> 和 <span class="math notranslate nohighlight">\(1.96\)</span>，
计算出的 <span class="math notranslate nohighlight">\(z=-3.34\)</span> 正好落在了左侧拒绝域内。</span><a class="headerlink" href="#id31" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p><strong>步骤4. 做出决策</strong></p>
<p>样本均值 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 结果值163偏离理论期望值165的原因可能有两种，
第一个可能的原因是，正常的随机结果，因为统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 本就是一个随机量，不同样本会得到不同值，
出现不一致是正常的随机现象。
第二个可能原因就是，<span class="math notranslate nohighlight">\(H_0\)</span> 是错误的，<span class="math notranslate nohighlight">\(H_1\)</span> 才是对的，
总体期望不是165，也会导致样本结果偏离理论期望值。
那么如何判断是哪个原因导致现在这个结果呢？
很遗憾，并没有准确的判断方法。我们只能根据概率”接受”其中的一个，这也是假设检验的本质。</p>
<p>理论上，样本结果值偏离理论期望值越远，第二个原因的可能性越大。
换句话说，检验统计量Z的值越大，<span class="math notranslate nohighlight">\(H_0\)</span> 错误的可能性越大。
我们知道统计量 <span class="math notranslate nohighlight">\(\bar{X}\)</span> 是服从正态分布的，
在一个正态分布中，越偏离中心位置的值概率越小，
得到一个远离中心的值是一个概率很小很极端的事件。
因此如果我们通过样本计算得到值在正态分布上是一个很小概率的事件，
就意味发生了一件很极端(概率很小)的事件，
而我们认为通常不会这么”巧合”。
如果在 <span class="math notranslate nohighlight">\(H_0\)</span> 是正确的前提下，
发生了一件极端的事件，我们更倾向于认为 <span class="math notranslate nohighlight">\(H_0\)</span> 是错误的。
本例中计算的到 <span class="math notranslate nohighlight">\(Z=-3.34\)</span>，
那么要得到这样一个样本结果值 <span class="math notranslate nohighlight">\(|Z| \ge 3.34\)</span> 的概率是多少呢？</p>
<p>在正态分布中，采样值落到区间 <span class="math notranslate nohighlight">\([\mu, \mu \pm \sigma)\)</span> 的概率大约是68.27%，
落到2个标准差区间 <span class="math notranslate nohighlight">\([\mu, \mu \pm 2\sigma)\)</span> 的概率大约是95.46%，
落到3个标准差区间 <span class="math notranslate nohighlight">\([\mu, \mu \pm 3\sigma)\)</span> 的概率大约是99.73%，
参考 <a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#fg-probability-009"><span class="std std-numref">图 1.8.6</span></a> 。</p>
<p>我们的例子中计算得到 <span class="math notranslate nohighlight">\(Z=-3.34\)</span>，
通过查正态分布表可以得到 <span class="math notranslate nohighlight">\(P(|Z| \ge 3.34)=0.08\%\)</span>
，其含义是，正态分布得到一个偏离期望值 <strong>至少</strong> 3.34的标准差距离的值的概率是 <span class="math notranslate nohighlight">\(0.08\%\)</span>。
这个概率值在假设检验中称作P值(P-value)。</p>
<aside class="topic">
<p class="topic-title">P值</p>
<p>统计检验的P值(P-value)是在 <span class="math notranslate nohighlight">\(H_0\)</span> 为真的假设下，
所得到的样本统计值结果会像实际观察结果那么极端或者更极端的概率。
P值越小，说明越极端，否定 <span class="math notranslate nohighlight">\(H_0\)</span> 的证据就越强。注意，
P值算的不是一个点的概率 <span class="math notranslate nohighlight">\(P(|Z|=z)\)</span> ，而是这个点以及比这个点更极端的概率
<span class="math notranslate nohighlight">\(P(|Z| \ge z)\)</span> 。</p>
</aside>
<figure class="align-center" id="id32">
<span id="pic-influence-me-105"></span><a class="reference internal image-reference" href="../../../_images/me_105.jpg"><img alt="../../../_images/me_105.jpg" src="../../../_images/me_105.jpg" style="width: 382.9px; height: 193.2px;" /></a>
<figcaption>
<p><span class="caption-number">图 3.6.3 </span><span class="caption-text">双侧检验 P值和 <span class="math notranslate nohighlight">\(\alpha\)</span> 的关系</span><a class="headerlink" href="#id32" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>注意我们的备择假设是 <span class="math notranslate nohighlight">\(H_1:\mu \neq 165\)</span>，不等于意味着大于或小于，
也就是本例中 <span class="math notranslate nohighlight">\(H_1\)</span> 是包含负偏离和正偏离两个的，
需要计算检验统计量Z落在分布两侧的概率之和，这种正负偏离一起算的检验称为双边检验。
如果把备择假设 <span class="math notranslate nohighlight">\(H_1\)</span> 改成 <span class="math notranslate nohighlight">\(\mu &lt;165\)</span> ，就变成了单边检验，在单边检验中拒绝域只有一侧，
此时只能计算 <span class="math notranslate nohighlight">\(Z \le -3.4\)</span> 的概率。</p>
<figure class="align-center" id="id33">
<span id="pic-influence-me-106"></span><a class="reference internal image-reference" href="../../../_images/me_106.jpg"><img alt="../../../_images/me_106.jpg" src="../../../_images/me_106.jpg" style="width: 383.59999999999997px; height: 235.89999999999998px;" /></a>
<figcaption>
<p><span class="caption-number">图 3.6.4 </span><span class="caption-text">单侧检验 P值和 <span class="math notranslate nohighlight">\(\alpha\)</span> 的关系</span><a class="headerlink" href="#id33" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>我们已经计算了P值0.08%，那这个P值是大还是小，算不算极端事件，需要有一个判断标准。
这个标准就是步骤2中设置的显著性水平 <span class="math notranslate nohighlight">\(\alpha\)</span>，
如果 <span class="math notranslate nohighlight">\(P \le \alpha\)</span> ，则认为发生了极端事件，此时我们拒绝零假设 <span class="math notranslate nohighlight">\(H_0\)</span>，
接受备择假设  <span class="math notranslate nohighlight">\(H_1\)</span>；
如果 <span class="math notranslate nohighlight">\(P &gt; \alpha\)</span> ，则认为没有发生极端事件，样本统计值的偏离是正常的随机误差造成的，
此时接受零假设 <span class="math notranslate nohighlight">\(H_0\)</span>。
假设本例中，我们设置显著水平 <span class="math notranslate nohighlight">\(\alpha=1.0\%\)</span> ，
显然P值 <span class="math notranslate nohighlight">\(0.08\)</span> 小于显著水平 <span class="math notranslate nohighlight">\(0.1\)</span>，因此我们拒绝零假设 <span class="math notranslate nohighlight">\(H_0\)</span> ，
我们有理由认为专家是在胡扯。</p>
<p><strong>决策错误</strong></p>
<div class="admonition important">
<p class="admonition-title">重要</p>
<p>假设检验对总体断言的决策并不是百分百正确的，对于零假设的接受或拒绝的决策是基于概率的，
所以是有可能做出错误的决策的，显著水平 <span class="math notranslate nohighlight">\(\alpha\)</span> 就是做出错误决策的概率的上限。</p>
</div>
<p>回顾整个检验过程，从始至终我们都是不知道总体的真实情况的，仅仅根据一份样本统计值做出的决策
，而决策的判定又是基于概率的，因此假设检验给出的结论也有错误的可能。
零假设的的真实情况和检验结论之间存在四种可能结果。</p>
<span id="tb-influence-03"></span><table class="docutils align-default" id="id34">
<caption><span class="caption-number">表 3.6.1 </span><span class="caption-text">假设检验的四种决策结果</span><a class="headerlink" href="#id34" title="此表格的永久链接"></a></caption>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>接受零假设</p></th>
<th class="head"><p>拒绝零假设</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>零假设为真</p></td>
<td><p>正确 <span class="math notranslate nohighlight">\(1-\alpha\)</span></p></td>
<td><p>Type I 错误 <span class="math notranslate nohighlight">\(\alpha\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>零假设为假</p></td>
<td><p>Type II 错误 <span class="math notranslate nohighlight">\(\beta\)</span></p></td>
<td><p>正确 <span class="math notranslate nohighlight">\(1-\beta\)</span></p></td>
</tr>
</tbody>
</table>
<p><a class="reference internal" href="#tb-influence-03"><span class="std std-numref">表 3.6.1</span></a> 是用表格的形式给出4种情况，
其中两种结果是正确的，另两种结果是错误的。</p>
<ul class="simple">
<li><p>零假设为真，并且决策结果是接受，此时决策结果是正确的，这个结果的概率是 <span class="math notranslate nohighlight">\(1-\alpha\)</span>。</p></li>
<li><p>零假设为真，然而决策结果是拒绝，此时决策结果是错误的，这个结果的概率是 <span class="math notranslate nohighlight">\(\alpha\)</span>。</p></li>
<li><p>零假设为假，然而决策结果是接受，此时决策结果是错误的，这个结果的概率是 <span class="math notranslate nohighlight">\(\beta\)</span>，此时称为Type I 错误。</p></li>
<li><p>零假设为假，并且决策结果是拒绝，此时决策结果是正确的，这个结果的概率是 <span class="math notranslate nohighlight">\(1-\beta\)</span>，此时称为Type II 错误。</p></li>
</ul>
<p><strong>Type II 错误</strong></p>
<p>如果检验的决策是接受零假设，那么这个结果有可能是正确的也可能是错误的。
如果零假设实际上是错误的，那么我们就做了一个错误的决策，此时称为
Type II 错误，又叫 <span class="math notranslate nohighlight">\(\beta\)</span> 错误，<span class="math notranslate nohighlight">\(\beta\)</span> 表示做出错误决策的概率，
当然这个 <span class="math notranslate nohighlight">\(\beta\)</span> 的值我们是无法得知的。</p>
<p>假设检验的零假设通常是对事物或者总体已有的一个认知或者结论，我们通过假设检验去论证这个认知是否正确，
如果假设检验的决策是 <span class="math notranslate nohighlight">\(\beta\)</span> 错误，相当于我们的检验过程其实没有贡献什么，
并没有判断出来这个零假设是错误的。更可悲的是，我们自己并不知道发生了 <span class="math notranslate nohighlight">\(\beta\)</span> 错误。</p>
<p><strong>Type I 错误</strong></p>
<p>同样的，如果检验的决策是拒绝零假设，也有可能是错误的决策。
零假设是真实的，但决策结果是拒绝零假设，我们把这类型的错误称为 Type I 错误。
幸运的是，我们能掌控犯 Type I 错误的概率上限，
Type I 错误发生的概率上限就是显著水平 <span class="math notranslate nohighlight">\(\alpha\)</span>
。我们通过比较P值和  <span class="math notranslate nohighlight">\(\alpha\)</span> 做出拒绝零假设决策，
因此 <span class="math notranslate nohighlight">\(\alpha\)</span> 就是代表着我们做出 <strong>拒绝零假设</strong> 决策的概率，
也就是犯 Type I 错误的 <strong>概率上限</strong> ，
注意 <span class="math notranslate nohighlight">\(\alpha\)</span> 不是 Type I 错误的概率，而是其理论上限，
可以通过减小显著水平 <span class="math notranslate nohighlight">\(\alpha\)</span> 的值，来降低 Type I 错误的概率。
但是，并不能一味的降低 <span class="math notranslate nohighlight">\(\alpha\)</span> 的值，
随着  <span class="math notranslate nohighlight">\(\alpha\)</span> 的降低，我们拒绝零假设的条件就更加严苛，减少了拒绝零假设的可能性，
因此也就减少了检验出错误零假设的能力（power）。</p>
<p>假设检验的关键思想在于一个检验统计量（test statistic）及其在虚拟假设下的抽样分布，
根据观测数据算出检验统计量值决定是否接受 <span class="math notranslate nohighlight">\(H_0\)</span>。
其过程概括起来就是</p>
<ol class="arabic simple">
<li><p>对总体某个参数的值做出一个虚拟的假设，称为零假设，记作 <span class="math notranslate nohighlight">\(H_0\)</span>。与 <span class="math notranslate nohighlight">\(H_0\)</span> 不同的结果是对立假设，记作 <span class="math notranslate nohighlight">\(H_a\)</span>。</p></li>
<li><p>选择一个和这个参数相关的检验统计量，并根据样本和虚拟假设的参数值计算出这个检验统计量的值，然后算出检验统计量值对应的 <span class="math notranslate nohighlight">\(P\)</span> 值。
所谓 <span class="math notranslate nohighlight">\(P\)</span> 值就是，在检验统计量的抽样分布下，得到检验统计量值及其更极端值的概率。</p></li>
<li><p>根据 <span class="math notranslate nohighlight">\(P\)</span> 值和显著水平 <span class="math notranslate nohighlight">\(\alpha\)</span> 做出接受还是拒绝零假设的决策。</p></li>
</ol>
<p>习惯上会根据检验统计量（抽样分布）对检验过程进行命名，
比如利用 <span class="math notranslate nohighlight">\(Z\)</span> 统计量进行假设检验就称为 <span class="math notranslate nohighlight">\(Z`检验，
利用 :math:`T\)</span> 统计量进行假设检验就称为 <span class="math notranslate nohighlight">\(T\)</span> 检验，
利用 <span class="math notranslate nohighlight">\(\chi^2\)</span> 统计量进行检验就称为 <span class="math notranslate nohighlight">\(\chi^2\)</span> 检验，
下面我们分别对这些检验进行简单的介绍。</p>
<section id="id22">
<h3><span class="section-number">3.6.1. </span>Z检验<a class="headerlink" href="#id22" title="此标题的永久链接"></a></h3>
<p>和均值参数相关的统计量有两个 <span class="math notranslate nohighlight">\(Z\)</span> 统计量和 <span class="math notranslate nohighlight">\(T\)</span> 统计量，
当总体方差已知或者抽样样本足够多时，使用 <span class="math notranslate nohighlight">\(Z\)</span> 统计量即可，
当总体方差未知并且抽样样本比较少时，建议使用 <span class="math notranslate nohighlight">\(T\)</span> 统计量。
本节我们先介绍用 <span class="math notranslate nohighlight">\(Z\)</span> 统计量对均值参数进行检验，
下一节讨论如何用 <span class="math notranslate nohighlight">\(T\)</span> 统计量对均值参数进行检验。</p>
<p>假设我们要对某个总体的分布的均值参数 <span class="math notranslate nohighlight">\(\mu\)</span> 进行检验，
总体的方差参数 <span class="math notranslate nohighlight">\(\sigma^1\)</span> 认为是已知的。
我们对均值参数 <span class="math notranslate nohighlight">\(\mu\)</span> 做出一个虚拟的假设，
假设它的真实值为 <span class="math notranslate nohighlight">\(\mu^*\)</span>
，零假设就是</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-89">
<span class="eqno">(3.6.7)<a class="headerlink" href="#equation-glm-source-content-89" title="此公式的永久链接"></a></span>\[H_0 : \mu = \mu^*\]</div>
<p>与零假设结果相反的对立假设为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-90">
<span class="eqno">(3.6.8)<a class="headerlink" href="#equation-glm-source-content-90" title="此公式的永久链接"></a></span>\[H_a: \mu \neq \mu^*\]</div>
<p>然后我们得到一个容量为 <span class="math notranslate nohighlight">\(N\)</span> 的抽样样本（观测样本），
利用这个样本可以得到 <span class="math notranslate nohighlight">\(\mu\)</span> 的一个估计值，记作 <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>
，估计量 <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> 的标准误差为
<span class="math notranslate nohighlight">\(\sigma/\sqrt{N}\)</span>，
然后就可以计算出 <span class="math notranslate nohighlight">\(Z\)</span> 统计量的一个值，记作 <span class="math notranslate nohighlight">\(z\)</span>
。</p>
<div class="math notranslate nohighlight" id="equation-eq-estimator-eval-101">
<span class="eqno">(3.6.9)<a class="headerlink" href="#equation-eq-estimator-eval-101" title="此公式的永久链接"></a></span>\[Z = \frac{\hat{\mu} - \mu^* }{\frac{\sigma}{\sqrt{N}}}\]</div>
<p>最后看 <span class="math notranslate nohighlight">\(z\)</span> 落在了哪个区域，如果落在接受域，则接受零假设，即认为零假设是正确的。
反之，如果落在拒绝域，就拒绝零假设。
判断 <span class="math notranslate nohighlight">\(z\)</span> 落在哪个区域有两种方法。</p>
<p>第一种方法，在给定显著水平 <span class="math notranslate nohighlight">\(\alpha\)</span> 的值后，计算出临界值 <span class="math notranslate nohighlight">\(\delta_1,\delta_2\)</span>
，这和上一节置信区间的方法是一样的，可以算出接受域（置信）区间 <span class="math notranslate nohighlight">\([\delta_1,\delta_2]\)</span>
，然后判断 <span class="math notranslate nohighlight">\(z\)</span> 值是否在区间 <span class="math notranslate nohighlight">\([\delta_1,\delta_2]\)</span> 内即可得出结论。
从这里可以看出，建设检验和置信区间本质上（区间估计）是一样的。</p>
<p>第二种方法，先计算出 <span class="math notranslate nohighlight">\(P\)</span> 值，
即 <span class="math notranslate nohighlight">\(P(|Z|&gt;z)\)</span>，
如果 <span class="math notranslate nohighlight">\(P \leq \alpha\)</span> 则说明 <span class="math notranslate nohighlight">\(z\)</span> 值落在了拒绝域，
反之，如果 <span class="math notranslate nohighlight">\(P&gt;\alpha\)</span> 则说明 <span class="math notranslate nohighlight">\(z\)</span> 值落在了接受域。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-91">
<span class="eqno">(3.6.10)<a class="headerlink" href="#equation-glm-source-content-91" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}P\text{值} &amp;= P(Z \geq z) + P(Z \leq -z)\\&amp;= 2 \Phi(-z)\end{aligned}\end{align} \]</div>
<p>公式中 <span class="math notranslate nohighlight">\(\Phi\)</span> 是标准正态的分布的累积分布函数，由于标注正态分布是对称的，
因此有 <span class="math notranslate nohighlight">\(P(Z \geq z)=P(Z \leq -z)\)</span></p>
<figure class="align-center" id="id35">
<span id="pic-influence-111"></span><a class="reference internal image-reference" href="../../../_images/Z检验.jpg"><img alt="../../../_images/Z%E6%A3%80%E9%AA%8C.jpg" src="../../../_images/Z%E6%A3%80%E9%AA%8C.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 3.6.5 </span><span class="caption-text">双侧Z检验</span><a class="headerlink" href="#id35" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
</section>
<section id="id23">
<h3><span class="section-number">3.6.2. </span>T检验<a class="headerlink" href="#id23" title="此标题的永久链接"></a></h3>
<p>当总体方差参数 <span class="math notranslate nohighlight">\(\sigma\)</span> 未知并且抽样样本数量比较少时，
就用 <span class="math notranslate nohighlight">\(T\)</span> 检验替代 <span class="math notranslate nohighlight">\(Z\)</span> 。
<span class="math notranslate nohighlight">\(T\)</span> 检验和 <span class="math notranslate nohighlight">\(Z\)</span> 检验的过程是完全一样的，
甚至二者的统计量值计算公式都是相似的，不一样的地方在于用 <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> 代替 <span class="math notranslate nohighlight">\(\sigma\)</span>。</p>
<div class="math notranslate nohighlight" id="equation-eq-estimator-eval-102">
<span class="eqno">(3.6.11)<a class="headerlink" href="#equation-eq-estimator-eval-102" title="此公式的永久链接"></a></span>\[T = \frac{\hat{\mu} - \mu^* }{\frac{\hat{\sigma}}{\sqrt{N}}}\]</div>
<p>不一样的地方仅在于计算 <span class="math notranslate nohighlight">\(P\)</span> 值的时候，要使用学生t分布的累积分布函数。
我们用符号 <span class="math notranslate nohighlight">\(\mathcal{T}_{n}\)</span> 表示自由度为 <span class="math notranslate nohighlight">\(n\)</span>
的学生t分布的累积分布函数，则 <span class="math notranslate nohighlight">\(P\)</span> 值的计算方法为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-92">
<span class="eqno">(3.6.12)<a class="headerlink" href="#equation-glm-source-content-92" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}P\text{值} &amp;= P(T \geq t) + P(T \leq -t)\\&amp;= 2 \mathcal{T}_d(-t)\end{aligned}\end{align} \]</div>
<p><span class="math notranslate nohighlight">\(T\)</span> 统计量（<a class="reference internal" href="#equation-eq-estimator-eval-102">公式(3.6.11)</a>）与 <span class="math notranslate nohighlight">\(Z\)</span> 统计量（<a class="reference internal" href="#equation-eq-estimator-eval-101">公式(3.6.9)</a>）的公式看上去是一样的，
二者之间的差别就在于分母部分，如果其中 <span class="math notranslate nohighlight">\(\sigma\)</span> 是总体分布的真实方差得到的就是 <span class="math notranslate nohighlight">\(Z\)</span> 统计量，服从标准正态分布；
如果用的是方差估计值 <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> 得到的就是 <span class="math notranslate nohighlight">\(T\)</span> 统计量，服从学生 t 分布。
当然如果样本数量 <span class="math notranslate nohighlight">\(N\)</span> 无穷大，<span class="math notranslate nohighlight">\(T\)</span> 统计量就近似等于 <span class="math notranslate nohighlight">\(Z\)</span> 统计量。</p>
</section>
<section id="id24">
<h3><span class="section-number">3.6.3. </span>卡方检验<a class="headerlink" href="#id24" title="此标题的永久链接"></a></h3>
<p>卡方检验常用于对方差参数进行检验，
零假设是对方差参数的一个虚拟假设
，假设方差参数的值为 <span class="math notranslate nohighlight">\(\sigma^*\)</span>
，然后通过卡方检验决定是否接受这个假设。</p>
<p>设方差参数的零假设和对立假设分别为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-93">
<span class="eqno">(3.6.13)<a class="headerlink" href="#equation-glm-source-content-93" title="此公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}H_0: \sigma^2 =\sigma^*\\H_a: \sigma^2 \neq \sigma^*\end{aligned}\end{align} \]</div>
<p>然后利用容量为 <span class="math notranslate nohighlight">\(N\)</span> 的样本得到方差参数的一个无偏估计值 <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span>
，有了 <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> 和 <span class="math notranslate nohighlight">\(\sigma^*\)</span> 后，
可以计算出卡方统计量的值。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-94">
<span class="eqno">(3.6.14)<a class="headerlink" href="#equation-glm-source-content-94" title="此公式的永久链接"></a></span>\[x = \frac{N \hat{\sigma}^2}{\sigma^*}\]</div>
<figure class="align-center" id="id36">
<span id="pic-influence-112"></span><a class="reference internal image-reference" href="../../../_images/双侧卡方检验.jpg"><img alt="../../../_images/%E5%8F%8C%E4%BE%A7%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C.jpg" src="../../../_images/%E5%8F%8C%E4%BE%A7%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 3.6.6 </span><span class="caption-text">双侧 <span class="math notranslate nohighlight">\(\chi^2\)</span> 检验</span><a class="headerlink" href="#id36" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>这里要注意 <span class="math notranslate nohighlight">\(\chi^2\)</span> 分布不再是对称结构，
如果是双边检验，无法直接计算出 <span class="math notranslate nohighlight">\(P\)</span> 值，
此时可以先算出接受（置信）域区间 <span class="math notranslate nohighlight">\([\delta_1,\delta_2]\)</span>，
如 <a class="reference internal" href="#pic-influence-112"><span class="std std-numref">图 3.6.6</span></a> 所示，
然后根据 <span class="math notranslate nohighlight">\(\chi^2\)</span> 值是否落在这个区间做出决策。</p>
<figure class="align-center" id="id37">
<span id="pic-influence-113"></span><a class="reference internal image-reference" href="../../../_images/单侧卡方检验.jpg"><img alt="../../../_images/%E5%8D%95%E4%BE%A7%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C.jpg" src="../../../_images/%E5%8D%95%E4%BE%A7%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 3.6.7 </span><span class="caption-text">单侧 <span class="math notranslate nohighlight">\(\chi^2\)</span> 检验</span><a class="headerlink" href="#id37" title="此图像的永久链接"></a></p>
</figcaption>
</figure>
<p>事实上，由于卡方分布是左偏的，整个图形期望值距离左侧很近，而右侧是一条长尾，
所以多数情况下，卡方检验使用的是单（右）侧检验，
如 <a class="reference internal" href="#pic-influence-113"><span class="std std-numref">图 3.6.7</span></a> 所示。
此时可以计算出 <span class="math notranslate nohighlight">\(P\)</span> 值，
比较 <span class="math notranslate nohighlight">\(P\)</span> 值和显著水平 <span class="math notranslate nohighlight">\(\alpha\)</span>
大小做出决策。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-95">
<span class="eqno">(3.6.15)<a class="headerlink" href="#equation-glm-source-content-95" title="此公式的永久链接"></a></span>\[P \text{值} = P(\chi^2 \geq x )\]</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="页脚">
        <a href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html" class="btn btn-neutral float-left" title="2. 最大似然估计" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html" class="btn btn-neutral float-right" title="4. 贝叶斯估计" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2018, zhangzhenhu(acmtiger@outlook.com) 禁止一切形式的转载！.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用的 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a> 开发.
  


    <script src="https://utteranc.es/client.js"
            repo="zhangzhenhu/blog"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>



</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>