<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>10. 模型检验 &mdash; 张振虎的博客 张振虎 文档</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
    <link rel="canonical" href="https://www.zhangzhenhu.com/glm/source/模型评估/influence.html" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/translations.js"></script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"macros": {"RR": "{\\bf R}", "bold": ["{\\bf #1}", 1], "KL": ["{D_\\textrm{KL}\\left ( #1 \\| #2 \\right )}", 2], "EE": ["{\\mathbb{E}_{#1} \\left [ #2 \\right ]}", 2, ""], "scalemath": ["{\\scalebox{#1}{\\mbox{\\ensuremath{\\displaystyle #2}}}}", 2], "argmin": ["{\\operatorname*{\\arg\\min}}"], "ind": ["{\\perp\\!\\!\\!\\!\\perp}"]}}}</script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" />
    <link rel="next" title="11. 高斯模型" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html" />
    <link rel="prev" title="9. 模型评估" href="content.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> 张振虎的博客
          </a>
              <div class="version">
                acmtiger@outlook.com
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">目录:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">广义线性模型</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id2">1.1. 概率模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id3">1.1.1. 概率律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id4">1.1.2. 离散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id5">1.1.3. 连续模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id6">1.2. 条件概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id7">1.3. 联合概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id8">1.4. 全概率与贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id9">1.5. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id10">1.6. 随机变量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id11">1.6.1. 离散随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id12">1.6.2. 连续随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id13">1.6.3. 累积分布函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id14">1.6.4. 随机变量的函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id15">1.6.5. 期望与方差</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id16">1.7. 边缘化</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id17">1.8. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#ch-basic-bernoulli">1.8.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id19">1.8.2. 二项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id20">1.8.3. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id21">1.8.4. 多项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id22">1.8.5. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id23">1.8.6. 卡方分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#t">1.8.7. t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#f">1.8.8. F分布</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html">2. 最大似然估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-liklihood">2.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id3">2.2. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id4">2.3. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-2-gaussian-ml">2.4. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id6">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html">3. 推断与检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id2">3.1. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution">3.2. 抽样分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution-normal">3.2.1. 正态分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#t">3.2.2. 学生t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id5">3.2.3. 卡方分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id6">3.3. 极限理论</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id7">3.3.1. 马尔可夫和切比雪夫不等式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id8">3.3.2. 弱大数定律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id9">3.3.3. 依概率收敛</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-clt">3.3.4. 中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id11">3.3.5. 强大数定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id12">3.4. 似然估计量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id13">3.4.1. 估计量的偏差与方差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-fisher-information">3.4.2. 信息量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-mle-estimator">3.4.3. 最大似然估计的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-interval">3.5. 置信区间</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#z">3.5.1. 均值参数的 Z 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id18">3.5.2. 均值参数的 T 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id19">3.5.3. 方差参数的区间估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-test">3.6. 简单假设检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id22">3.6.1. Z检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id23">3.6.2. T检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id24">3.6.3. 卡方检验</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html">4. 贝叶斯估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id2">4.1. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id3">4.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id4">4.1.2. 类别分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id5">4.2. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id6">4.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id7">4.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id8">4.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html">5. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-1">5.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id3">5.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id4">5.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id5">5.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id6">5.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id7">5.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-moments">5.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id9">5.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#kl">5.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html">6. 线性回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id2">6.1. 最小二乘</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id3">6.1.1. 最小误差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id4">6.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id5">6.2. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id6">6.2.1. 高斯假设</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id7">6.2.2. 参数估计</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html">7. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id2">7.1. 指数族分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id3">7.1.1. 自然指数族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id4">7.1.2. 示例：高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id5">7.1.3. 示例：伯努利分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id6">7.2. 广义线性模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id7">7.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html">8. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate">8.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id3">8.2. 泰勒级数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id4">8.3. 梯度下降法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id6">8.4. 牛顿法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id7">8.4.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id8">8.4.2. 标准连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id9">8.4.3. 迭代初始值的设定</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#irls">8.5. 迭代重加权最小二乘(IRLS)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id10">8.5.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id11">8.5.2. 算法过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id12">8.6. 估计量的标准误差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate-phi">8.7. 分散参数的估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="content.html">9. 模型评估</a><ul>
<li class="toctree-l3"><a class="reference internal" href="content.html#id2">9.1. 拟合优度</a><ul>
<li class="toctree-l4"><a class="reference internal" href="content.html#id3">9.1.1. 嵌套模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="content.html#likelihood-ratio">9.1.2. 对数似然比(Likelihood ratio)</a></li>
<li class="toctree-l4"><a class="reference internal" href="content.html#deviance">9.1.3. 偏差(deviance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="content.html#r-2">9.1.4. 决定系数 <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="content.html#ch-glm-gof-chi">9.1.5. 广义皮尔逊卡方统计量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="content.html#residual-analysis">9.2. 残差分析(Residual analysis)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="content.html#response-residuals">9.2.1. Response residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="content.html#working-residuals">9.2.2. Working residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="content.html#partial-residuals">9.2.3. Partial residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="content.html#pearson-residuals">9.2.4. Pearson residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="content.html#deviance-residuals">9.2.5. Deviance residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="content.html#score-residuals">9.2.6. Score residuals</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="content.html#model-selection">9.3. 模型选择(model selection)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="content.html#aic">9.3.1. AIC</a></li>
<li class="toctree-l4"><a class="reference internal" href="content.html#bic">9.3.2. BIC</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">10. 模型检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">10.1. 拉格朗日乘子检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">10.1.1. 得分统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">10.1.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#wald">10.2. wald 检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id5">10.2.1. 参数估计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">10.2.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id7">10.3. 似然比检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id8">10.3.1. 抽样分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id9">10.3.2. 模型比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id10">10.3.3. 偏差统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#f">10.3.4. F 检验</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id11">10.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">11. 高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">11.1. 传统线性回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">11.2. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">11.3. 高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">11.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">11.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">11.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">11.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id10">11.5. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">12. 逆高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">12.1. 逆高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">12.2. 逆高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">12.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">12.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">12.3.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">12.3.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">12.4. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">13. 二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">13.1. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">13.2. 逻辑回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">13.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">13.2.2. 参数估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#odds-logit">13.2.3. odds 与 logit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">13.3. 二项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">13.4. 二项式回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">13.4.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">13.4.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">13.5. 其它连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">13.5.1. 恒等连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#probit">13.5.2. probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#log-log-clog-log">13.5.3. log-log 和 clog-log</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">13.6. 分组数据与比例数据</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html">14. 泊松模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#poisson">14.1. 泊松(Poisson)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id2">14.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id3">14.1.2. 泊松分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id5">14.2. 泊松回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id6">14.3. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id7">14.4. 拟合统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id8">14.5. 频率模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id9">14.6. 泊松模型的局限性</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html">15. 指数模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#exponential">15.1. 指数(exponential)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id2">15.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id3">15.1.2. 分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id6">15.2. 指数回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id7">15.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id8">15.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id9">15.3.2. 拟合优度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#irls">15.3.3. IRLS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html">16. Gamma 模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id1">16.1. Gamma 函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id2">16.2. Gamma 分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id3">16.3. Gamma 回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id4">16.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id5">16.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#irls">16.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id6">16.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id7">16.5. 其他连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id8">16.5.1. 对数 Gamma 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#identity-gamma">16.5.2. 恒等(identity) Gamma 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html">17. 过度分散</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id2">17.1. 什么是过度分散</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id3">17.2. 过度分散的检测</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id4">17.3. 过度分散的影响</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id5">17.4. 标准误差的修正</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">18. 负二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">18.1. 负二项式分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">18.1.1. 从二项式分布推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">18.1.2. 泊松-伽马混合分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#alpha">18.1.3. 辅助参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的影响</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">18.2. 负二项回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">18.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#irls">18.3.1. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">18.3.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">18.4. 负二项式模型扩展</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">18.4.1. 对数连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">18.4.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">18.4.3. 几何模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">18.4.4. 广义负二项式模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html">19. 零计数问题</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id2">19.1. 零截断模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id3">19.1.1. 零截断泊松模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id4">19.1.2. 零截断负二项式模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id5">19.2. 零膨胀模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#hurdle">19.2.1. Hurdle 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#zero-inflate">19.2.2. Zero-inflate 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">20. 多项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">20.1. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#softmax">20.2. softmax 回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">20.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">20.3. 多项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id6">20.4. 多项式回归模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">21. 有序离散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">21.1. 有序逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">21.2. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">21.3. 连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#logit">21.3.1. logit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#probit">21.3.2. probit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#clog-log">21.3.3. clog-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#log-log">21.3.4. log-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#cauchit">21.3.5. cauchit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">21.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html">附录</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id2">标准正态累积分布表</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id3">卡方分布临界值表</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/content.html">参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../probability_model/index_html.html">概率图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id2">1.1. 概率分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id3">1.2. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#marginalization">1.3. 边缘化(marginalization)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id6">1.4. 贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id7">1.5. 期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id8">1.6. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id9">1.6.1. 离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id10">1.6.2. 连续变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id11">1.6.3. 计数变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id12">1.7. 大数定律</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id13">1.7.1. 独立同分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id14">1.7.2. 中心极限定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id15">1.8. 信息论基础</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id16">1.8.1. 信息熵</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#kl">1.8.2. KL散度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id18">1.8.3. 互信息</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html">2. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-liklihood">2.1. 极大似然估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id3">2.1.1. 二值离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id4">2.1.2. 一般离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-gaussian-ml">2.1.3. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id6">2.1.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-bayesian-estimation">2.2. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id8">2.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id9">2.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id10">2.3. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id11">2.3.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id12">2.3.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id13">2.4. 最大似然估计与贝叶斯估计的对比</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id14">2.5. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#fisher-information">2.6. Fisher Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id15">2.7. 估计量的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id16">2.7.1. 估计量的方差与偏差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id17">2.7.2. 大数定律和中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-mle-estimator">2.7.3. 最大似然估计的特性</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html">3. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-1">3.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id3">3.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id4">3.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id5">3.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id6">3.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id7">3.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-moments">3.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id9">3.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#kl">3.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/19.%E5%A4%9A%E7%BB%B4%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_26.html">4. 多维高斯分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html">5. 有向图(Directed Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id1">5.1. 有向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id2">5.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id3">5.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html">6. 无向图(Undirected Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id1">6.1. 无向图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id2">6.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id3">6.3. 图的分解</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#vs">6.4. 有向图 vs 无向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id4">6.5. 树</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id5">6.6. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html">7. 因子图</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id2">7.1. 因子图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id3">7.2. 图模型之间的转换</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id4">7.2.1. 转换为因子图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id5">7.2.2. 因子图转换为有向图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id6">7.3. 图模型的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#i-map">7.3.1. I-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#d-map">7.3.2. D-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#p-map">7.3.3. P-map</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html">8. 模型推断：消元法</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id2">8.1. 什么是模型的推断</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id3">8.2. 消元法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id4">8.2.1. 有向图消元算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#ch-condition-margin">8.2.2. 条件概率和边缘概率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id6">8.2.3. 无向图的消元法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id7">8.3. 图消除</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id9">8.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html">9. 加和乘积算法(sum-product algorithm)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id1">9.1. 树结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id2">9.2. 从消元法到信息传播</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id3">9.3. 树模型的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id4">9.4. 因子图的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id5">9.5. 类树结构图模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#polytrees">9.6. 多重树(polytrees)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id6">9.7. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html">10. 最大后验估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id2">10.1. 最大后验概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id3">10.2. 最大化后验的状态</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id4">10.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html">11. 完整观测的参数学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id2">11.1. 有向图的参数学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id3">11.2. 无向图的参数学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id4">11.2.1. 成对二值变量模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id5">11.2.2. 一般二值变量模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html">12. 不完整观测的学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#id2">12.1. 隐变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#em">12.2. 期望最大化算法(EM)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/14.%E5%9B%BE%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E5%AD%A6%E4%B9%A0_lecture_23.html">13. 有向图结构学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/16.%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD_lecture_17.html">14. 变分推断</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html">15. 马尔科夫蒙特卡洛</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#why-sampling">15.1. Why sampling？</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#monte-carlo">15.2. 蒙特卡罗(Monte Carlo)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain">15.3. 马尔科夫链(Markov Chain)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id2">15.3.1. 一个例子</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#time-reversibility">15.3.2. 时间可逆性(Time Reversibility)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id3">15.3.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain-monte-carlo">15.4. Markov Chain Monte Carlo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#metropolis-hastings">15.4.1. Metropolis-Hastings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id4">15.4.2. 例子：正态分布的采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id5">15.4.3. 多变量采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#gibbs">15.4.4. Gibbs 采样</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#mixing-time">15.5. Mixing Time</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#approximate-map-and-partitioning">15.6. Approximate MAP and Partitioning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html">16. 贝叶斯分类器</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id2">16.1. 朴素贝叶斯模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id3">16.1.1. 模型表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id4">16.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id5">16.2. 高斯判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id6">16.2.1. 一元高斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id7">16.2.2. 多元高斯模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id8">16.3. 逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id9">16.4. 生成模型和判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id10">16.5. 多分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id11">16.6. 其它扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html">17. 回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id2">17.1. 机器学习的概率解释</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id3">17.2. 经典线性回归</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id4">17.2.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id5">17.3. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id6">17.3.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id7">17.4. 凸函数最优化问题</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id8">17.5. 岭回归</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html">18. 分类模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id2">18.1. 生成模型与判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id3">18.2. 线性回归与线性分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id4">18.3. 生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id5">18.3.1. 高斯判别模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id6">18.3.2. 朴素贝叶斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id7">18.3.3. 指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id8">18.4. 判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id9">18.4.1. 逻辑回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id10">18.4.2. 多分类</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id11">18.4.3. 最大熵模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#probit">18.4.4. Probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#noisy-or">18.4.5. Noisy-OR 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id12">18.4.6. 其它指数模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html">19. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id2">19.1. 定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id3">19.1.1. 指数族分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id4">19.1.2. 链接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id5">19.1.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id6">19.2. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id7">19.2.1. 梯度下降法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id8">19.2.2. 牛顿法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#irls">19.2.3. 迭代重加权最小二乘(IRLS)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#goodness-of-fit">19.3. goodness of fit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id9">19.4. 连续值响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id10">19.4.1. 高斯族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#gamma">19.4.2. Gamma族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id12">19.5. 二项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id13">19.6. 多项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id14">19.7. 计数响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id15">19.7.1. 泊松分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id16">19.8. GLM扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html">20. 混合模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id2">20.1. 一般混合模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id3">20.1.1. 模型的有向图表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id5">20.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id8">20.2. 高斯混合模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id9">20.2.1. 模型的表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id10">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#k-means">20.3. K-means</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/32.%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90_42.html">21. 因子分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E4%BA%8C%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B.html">22. 二变量模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/33.LDA_43.html">23. 主题模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/33.LDA_43.html#plsa">23.1. PLSA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/33.LDA_43.html#lda">23.2. LDA</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html">24. 隐马尔可夫模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id2">24.1. 隐马尔可夫模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id3">24.1.1. 马尔可夫模型和朴素贝叶斯模型的关系</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id4">24.2. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/27.%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA_37.html">25. 条件随机场</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/28.%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8_38.html">26. 卡尔曼滤波器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/40.%E9%A1%B9%E7%9B%AE%E5%8F%8D%E5%BA%94%E7%90%86%E8%AE%BA_50.html">27. 项目反应理论</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/41.%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA_51.html">28. 贝叶斯知识追踪</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE.html">29. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../aigc/index.html">AI内容生成（ai-gc）</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html">1. 变分自编码器（Variational Autoencoder）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#evidence-lower-bound-elbo">1.1. 证据下界(Evidence Lower Bound,ELBO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id2">1.2. 编码-解码</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id3">1.3. 总结</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#em">1.3.1. 和EM算法的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#variational">1.3.2. 为什么叫变分（variational）？</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#vq-vae">1.4. VQ-VAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id4">1.5. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html">2. 扩散概率模型（diffusion probabilistic models）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#diffusion-probabilistic-model">2.1. 扩散概率模型（diffusion probabilistic model）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#markovian-hierarchical-variational-autoencoder-mhvae">2.1.1. 马尔科夫分层自编码器（Markovian Hierarchical Variational Autoencoder,MHVAE)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id6">2.1.2. 扩散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id8">2.1.3. 前向-后向</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#elbo">2.1.4. 目标函数（ELBO）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id14">2.1.5. 图片生成（采样）过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#denoising-diffusion-probabilistic-model-ddpm">2.2. 降噪扩散概率模型（Denoising diffusion probabilistic model,DDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#score-based-ddpm">2.3. 基于分数的解释（Score-based DDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id23">2.4. 扩散模型的三种等价表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#improved-denoising-diffusion-probabilistic-models-iddpm">2.5. 改进降噪扩散概率模型（Improved Denoising Diffusion Probabilistic Models,IDDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id24">2.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/ddim.html">3. 去噪扩散隐式模型（Denoising Diffusion Implicit Models,DDIM）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/ddim.html#id1">3.1. 扩散模型的回顾</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/ddim.html#id2">3.2. 非马尔科夫前向过程</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/ddim.html#id4">3.3. 加速采样</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/ddim.html#id5">3.4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html">4. 基于分数的生成模型（Score-based generative models）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id4">4.1. 基于分数的生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#score-matching">4.1.1. 分数匹配算法（Score Matching）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id8">4.1.2. 基于分数的生成模型面临的困难</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id12">4.1.3. 通过加噪的方法估计分布的近似分数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id13">4.1.4. 基于分数的改进采样算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id15">4.1.5. 改进的分数生成模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id17">4.2. 随机微分方程</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id18">4.2.1. 微分方程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id19">4.2.2. 随机微分方程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id20">4.2.3. 基于随机微分方程的生成模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/Score-Based_Generative_Models.html#id21">4.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/Guidance.html">5. 条件控制扩散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/Guidance.html#classifier-guidance">5.1. classifier guidance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/Guidance.html#classifier-free-guidance">5.2. Classifier-free guidance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/Guidance.html#clip-guidance">5.3. CLIP Guidance</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../aigc/Guidance.html#id12">5.3.1. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html">6. 稳定扩散模型（Stable diffusion model）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#latent-diffusion-model-ldm">6.1. 潜在扩散模型（Latent diffusion model,LDM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#stable-diffusion-probabilistic-model-sdm">6.2. 稳定扩散模型（Stable diffusion probabilistic model,SDM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html#id3">6.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/dalle2.html">7. DALL·E 2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/dalle2.html#glide">7.1. GLIDE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/dalle2.html#unclip">7.2. Unclip</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../aigc/dalle2.html#id1">7.3. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../aigc/imgen.html">8. Imagen</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../audio/index.html">语音技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../audio/feature.html">1. 音频特征</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id2">1.1. 认识声音</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id3">1.2. 认识声波</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id4">1.2.1. 物体的振动以及简谐振动</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id7">1.2.2. 什么是声波</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id8">1.2.3. 纯音和复合音</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#spectrum">1.2.4. 频谱 Spectrum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id10">1.2.5. 名词</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id13">1.3. 语音学</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id14">1.3.1. 发声原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id15">1.3.2. 听觉感应</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id16">1.4. 数字信号处理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id17">1.4.1. 模数转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#wav">1.4.2. 音频文件–WAV</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id18">1.5. 分帧与加窗</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id19">1.5.1. 预加重处理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id20">1.5.2. 分帧与加窗处理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id21">1.6. 声音的感官度量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#sound-pressure-level-spl">1.6.1. 声压与声压级(Sound Pressure Level,SPL)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#intensity-level-il">1.6.2. 声强与声强级(Intensity Level,IL）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id22">1.6.3. 声压与声强的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id23">1.6.4. 响度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id24">1.6.5. 音量计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id25">1.6.6. 频率与音高</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id26">1.7. 时域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id27">1.7.1. 短时能量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id28">1.7.2. 短时平均幅度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id29">1.7.3. 短时过零率</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id31">1.8. 频域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#spectrum-spectrogram">1.8.1. 声谱(spectrum)和时频谱(spectrogram)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#short-time-fourier-transform-stft">1.8.2. 短时傅里叶变换 Short-time Fourier transform (STFT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id33">1.8.3. 倒频谱</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id34">1.8.4. 色谱图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id35">1.9. 小波域特征</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id36">1.9.1. 离散小波域变换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id37">1.9.2. 小波域过零率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id38">1.9.3. 小波域质心</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id39">1.9.4. 小波域子带能量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#mfcc">1.10. 语音识别的音频特征–MFCC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id40">1.11. 参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../edm/index.html">教育领域数据挖掘</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../edm/bkt.html">1. 贝叶斯知识追踪(Bayesian Knowledge Tracing,BKT)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#id1">1.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#hidden-markov-model-hmm">1.2. 隐马尔科夫模型(Hidden Markov Model,HMM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#bayesian-knowledge-tracing">1.3. 贝叶斯知识追踪(Bayesian Knowledge Tracing)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#bkt">1.3.1. BKT的参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#item-response-theory-irt">1.4. 项目反映理论(Item Response Theory,IRT)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#bktirt">1.5. BKT结合IRT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#id5">1.6. 实验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id6">1.6.1. 数据集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id7">1.6.2. 实验方法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id8">1.6.3. 实验结果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id9">1.6.4. 项目代码</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#id10">1.7. 未来工作</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id11">1.7.1. 题目难度的计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#irt">1.7.2. 多参数IRT模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id12">1.7.3. 参数估计算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#id13">1.8. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../nlp/index.html">自然语言处理</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html">1. 文本去重</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id2">1.1. 背景</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id3">1.2. 技术思路</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id4">1.3. 相似（距离）算法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#euclidean-distance">1.3.1. 欧氏距离（Euclidean Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minkowski-distance">1.3.2. 闵科夫斯基距离（Minkowski Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#manhattan-distance">1.3.3. 曼哈顿距离（Manhattan Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#chebyshev-distance">1.3.4. 切比雪夫距离（Chebyshev Distance ）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#mahalanobis-distance">1.3.5. 马氏距离(Mahalanobis Distance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#cosine-similarity">1.3.6. 余弦夹角相似度(Cosine Similarity)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#hamming-distance">1.3.7. 汉明距离（Hamming Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#jaccard">1.3.8. Jaccard 系数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id5">1.3.9. 编辑距离</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id6">1.3.10. 最长公共字串</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id7">1.3.11. 最长公共子序列</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id8">1.4. 文本去重</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#kshingle">1.4.1. KShingle算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minhash">1.4.2. Minhash算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#simhash">1.4.3. simhash</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#ksentence">1.4.4. KSentence算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id9">1.5. 话术去重</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nlp/bert/content.html">2. Attention&amp;Transformer&amp;Bert 简介</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/bert/content.html#transformer">2.1. Transformer 从宏观到微观</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#seq2seq">2.1.1. seq2seq</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#id1">2.1.2. 模型的输入</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/bert/content.html#self-attention">2.2. Self-Attention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#id2">2.2.1. 什么是注意力？</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#id3">2.2.2. 加权求和</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#id4">2.2.3. 位置编码</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#multi-head">2.2.4. 多头注意力（Multi-head）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/bert/content.html#attention">2.3. Attention 机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/bert/content.html#id5">2.4. 其它参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../rst_tutorial/latex.html">latex demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../rst_tutorial/latex.html#latex">latex</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rst_tutorial/latex.html#how-to-write-an-m-x-n-matrix-in-latex">How to write an m x n matrix in LaTeX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-big-parentheses">With big parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-parentheses">With parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-brackets">With brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#latex-matrix-with-no-bracket">LateX matrix with no bracket</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-vertical-bar-brackets">With vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-curly-brackets">with curly brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-double-vertical-bar-brackets">with double vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#small-inline-matrix">small inline matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#examples-matrix-2-x-2-in-latex">Examples matrix 2 x 2 in LaTeX</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../rst_tutorial/graphviz.html">graphviz demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../rst_tutorial/graphviz.html#id1">布局</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rst_tutorial/graphviz.html#id2">其它</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">读书笔记</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">《统计因果推断推理入门》读书笔记</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html">1. 第三章 干预的效果</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id2">1.1. 第3.1节 干预</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id3">1.2. 第3.2节 校正公式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id8">1.3. 第3.3节 后门准则</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html">《深度学习推荐系统》读书笔记</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id2">重点</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id3">冷启动</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id4">探索与利用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id5">召回层的主要策略</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#embedding">协同过滤 &amp; Embedding 向量</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">张振虎的博客</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">广义线性模型</a> &raquo;</li>
      <li><span class="section-number">10. </span>模型检验</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/glm/source/模型评估/influence.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1><span class="section-number">10. </span>模型检验<a class="headerlink" href="#id1" title="永久链接至标题"></a></h1>
<p>我们基于样本训练模型，基于样本计算模型拟合优度指标，并给出模型好坏的结论。
然而，这一切都是建立随机样本的基础上，模型拟合优度指标也是一个随机量，
我们的结论是根据样本推断(influence)得出的，推断得出结论不是百分百准确的，
这就需要同时给出这个结论的可靠程度，而这就是统计推断(statistical inference)所做的事情。</p>
<p>上一章我们介绍了 <code class="docutils literal notranslate"><span class="pre">GLM</span></code> 中评价模型拟合好坏程度的常见指标，以及这些指标的定义和计算方法，
但是没有说明如何根据指标值得出结论，本章我们探讨如何根据拟合优度指标的值得出模型优劣的结论，
以及结论的可靠程度。
假设检验是统计推断中常用的方法之一，
其中似然比检验、wald 检验以及拉格朗日乘子检验是其中最常用的三大模型检验的方法，
在正式讨论三大模型检验之前，我们先回顾一个重要的结论，
这是稍后推导检验方法的理论基础。</p>
<p><strong>渐近正态性</strong></p>
<p>如果响应变量是正态分布，则通常可以准确确定一些统计量的抽样分布。
反之，如果响应变量不是正态分布，就需要依赖中心极限定理，找到其大样本下的近似分布。
注意，这些结论的成立都是有一些前提条件的，
对于来自属于指数族分布的观测数据，特别是对于广义线性模型，确实满足了必要条件。
在本节我们只给出统计量抽样分布的一些关键步骤，
Fahrmeir和Kaufmann（1985）给出了广义线性模型抽样分布理论的详细信息。</p>
<p>如果一个统计量 <span class="math notranslate nohighlight">\(S\)</span>，其渐近服从正态分布 <span class="math notranslate nohighlight">\(S \sim \mathcal{N}(\mathbb{E}[S],V(S))\)</span>
，其中 <span class="math notranslate nohighlight">\(\mathbb{E}[S]\)</span> 和 <span class="math notranslate nohighlight">\(V(S)\)</span> 分别是 <span class="math notranslate nohighlight">\(S\)</span> 的期望和方差
，则近似的有：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-0">
<span class="eqno">(10.1)<a class="headerlink" href="#equation-glm-source-influence-0" title="公式的永久链接"></a></span>\[\frac{S-\mathbb{E}[S]}{\sqrt{V(S)}} \sim \mathcal{N}(0,1)\]</div>
<p>根据卡方分布的定义，等价的有</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-1">
<span class="eqno">(10.2)<a class="headerlink" href="#equation-glm-source-influence-1" title="公式的永久链接"></a></span>\[\frac{(S-\mathbb{E}[S])^2}{V(S)} \sim \chi^2 (1)\]</div>
<p>如果 <span class="math notranslate nohighlight">\(S\)</span> 是一个向量 <span class="math notranslate nohighlight">\(\pmb{S}^{T}=[S_1,\dots,S_k]\)</span> ，上述结论可以写成向量的模式。</p>
<div class="math notranslate nohighlight" id="equation-eq-influence-110">
<span class="eqno">(10.3)<a class="headerlink" href="#equation-eq-influence-110" title="公式的永久链接"></a></span>\[(\pmb{S}-\mathbb{E}[\pmb{S}])^T \pmb{V}^{-1}(\pmb{S}-\mathbb{E}[\pmb{S}])
\sim \chi^2 (k)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\pmb{V}\)</span> 是协方差矩阵，并且必须是非奇异矩阵。</p>
<section id="id2">
<h2><span class="section-number">10.1. </span>拉格朗日乘子检验<a class="headerlink" href="#id2" title="永久链接至标题"></a></h2>
<p>我们已经知道似然函数及其一阶导数都是一个关于样本的函数，
所以似然函数及其一阶导数都是统计量(statistic)。
似然函数的一阶导数又叫做得分函数(score function)，也称为得分统计量(score statics)。
拉格朗日乘子检验（Lagrange multiplier test,LMT）是利用得分统计量对模型进行检验的方法，
因为是通过得分统计量进行检验，所以也被称为是分数检验（score test）。
它可以用于检验在一个模型的基础上增加特征的特征变量后能否显著提升模型的效果。</p>
<section id="id3">
<h3><span class="section-number">10.1.1. </span>得分统计量<a class="headerlink" href="#id3" title="永久链接至标题"></a></h3>
<p>假设 <span class="math notranslate nohighlight">\(Y_1,\dots,Y_N\)</span> 是相互独立的 <code class="docutils literal notranslate"><span class="pre">GLM</span></code> 样本变量，
这里我们强调 <span class="math notranslate nohighlight">\(Y_i\)</span> 是一个随机变量，所以用大写符号表示。
其中有 <span class="math notranslate nohighlight">\(\mathbb{E}[Y_i]=\mu_i\)</span> , <span class="math notranslate nohighlight">\(g(\mu_i)=\beta^T x_i=\eta_i\)</span>
，自然参数 <span class="math notranslate nohighlight">\(\theta_i\)</span> 是一个关于 <span class="math notranslate nohighlight">\(\mu_i\)</span> 函数。
<code class="docutils literal notranslate"><span class="pre">GLM</span></code> 模型的对数似然函数为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-2">
<span class="eqno">(10.1.1)<a class="headerlink" href="#equation-glm-source-influence-2" title="公式的永久链接"></a></span>\[\ell(\beta)= \sum_{i=1}^N \left \{   \frac{Y_i \theta_i - b(\theta_i)}{a(\phi)}   + c(y_i,\phi)   \right \}\]</div>
<p>根据 <a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate"><span class="std std-numref">章节8.1</span></a> 的内容，对数似然函数的一阶导数又叫得分统计量，
记作 <span class="math notranslate nohighlight">\(U\)</span>。
<code class="docutils literal notranslate"><span class="pre">GLM</span></code> 得分统计量的一般形式为</p>
<div class="math notranslate nohighlight" id="equation-eq-glm-influence-033">
<span class="eqno">(10.1.2)<a class="headerlink" href="#equation-eq-glm-influence-033" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}U_j = \frac{ \partial \ell}{\beta_j}
&amp;= \sum_{i=1}^N \left ( \frac{\partial \ell_i}{\partial \theta_i} \right )
\left ( \frac{\partial \theta_i}{\partial \mu_i} \right )
\left ( \frac{\partial \mu_i}{\partial \eta_i} \right )
\left ( \frac{\partial \eta_i}{\partial \beta_j} \right )\\&amp;= \sum_{i=1}^N \left \{ \frac{Y_i-b'(\theta_i)}{a(\phi)}   \right \}
\left \{ \frac{1}{\nu(\mu_i)} \right \} \left ( \frac{\partial \mu}{\partial \eta} \right )_i x_{ij}\\&amp;= \sum_{i=1}^N \frac{Y_i-\mu_i}{a(\phi) \nu(\mu_i) } \left ( \frac{\partial \mu}{\partial \eta} \right )_i x_{ij}\\&amp;= \sum_{i=1}^N \frac{Y_i-\hat{y}_i}{a(\phi) \nu(\hat{y}_i) } \left ( \frac{\partial \mu}{\partial \eta} \right )_i x_{ij}\\
&amp;= \sum_{i=1}^N \frac{Y_i-\hat{y}_i}{V(\hat{y}_i) } \left ( \frac{\partial \mu}{\partial \eta} \right )_i x_{ij}\end{aligned}\end{align} \]</div>
<p>注意下标 <span class="math notranslate nohighlight">\(j\)</span> 表示的协变量参数向量 <span class="math notranslate nohighlight">\(\beta\)</span> 的下标，<span class="math notranslate nohighlight">\(U_j\)</span> 是对数似然函数对 <span class="math notranslate nohighlight">\(\beta_j\)</span> 的一阶偏导数。
对于任意的样本 <span class="math notranslate nohighlight">\(Y_i\)</span> 都有 <span class="math notranslate nohighlight">\(\mathbb{E}[Y_i]=\mu_i\)</span>
，因此有</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-3">
<span class="eqno">(10.1.3)<a class="headerlink" href="#equation-glm-source-influence-3" title="公式的永久链接"></a></span>\[\mathbb{E}_{Y_i}[U_j] = 0\]</div>
<p>统计量 <span class="math notranslate nohighlight">\(U\)</span> 的协方差矩阵又称作信息矩阵，记作 <span class="math notranslate nohighlight">\(\mathcal{J}\)</span>。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-4">
<span class="eqno">(10.1.4)<a class="headerlink" href="#equation-glm-source-influence-4" title="公式的永久链接"></a></span>\[\mathcal{J}_{jk} = \mathbb{E}[U_jU_k]
=\sum_{i=1}^N
\left ( \frac{\partial \mu}{\partial \eta} \right )^2_i  \frac{  x_{ij} x_{ik}}{ a(\phi) \nu(\hat{y}_i) }\]</div>
<p>在  <a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-estimate"><span class="std std-numref">章节2参数估计</span></a> 我们讲过，
信息矩阵 <span class="math notranslate nohighlight">\(\mathcal{J}\)</span> 又等于对数似然函数二阶偏导数（海森矩阵）的期望的负数，</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-5">
<span class="eqno">(10.1.5)<a class="headerlink" href="#equation-glm-source-influence-5" title="公式的永久链接"></a></span>\[\mathcal{J} = - \mathbb{E}[\ell'']
= - \mathbb{E}[U']\]</div>
<p>如果模型的参数向量 <span class="math notranslate nohighlight">\(\beta\)</span> 只有一个截距参数， <span class="math notranslate nohighlight">\(\beta=[\beta_0]\)</span> ，
此时模型只有一个参数，得分统计量 <span class="math notranslate nohighlight">\(U\)</span> 是一个标量，渐近服从正态分布。</p>
<div class="math notranslate nohighlight" id="equation-eq-glm-influence-015">
<span class="eqno">(10.1.6)<a class="headerlink" href="#equation-eq-glm-influence-015" title="公式的永久链接"></a></span>\[U  \sim \mathcal{N}(0,\mathcal{J})
\ \text{或者} \
\frac{U}{\sqrt{\mathcal{J}}} \sim \mathcal{N}(0,1)\]</div>
<p>根据卡方分布的定义，也可以写成</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-6">
<span class="eqno">(10.1.7)<a class="headerlink" href="#equation-glm-source-influence-6" title="公式的永久链接"></a></span>\[\frac{U^2}{\mathcal{J}}  \sim \chi^2 (1)\]</div>
<p>如果 <span class="math notranslate nohighlight">\(\beta\)</span> 是一个参数向量，<span class="math notranslate nohighlight">\(\beta^T=[\beta_0,\beta_1,\dots,\beta_p]\)</span>，
模型一共有 <span class="math notranslate nohighlight">\(p+1\)</span> 个参数，
则 <span class="math notranslate nohighlight">\(\textbf{U}\)</span> 表示一个向量 <span class="math notranslate nohighlight">\(\textbf{U}^T=[U_0,U_1,\dots,U_p]\)</span>
，此时 <span class="math notranslate nohighlight">\(\textbf{U}\)</span> 渐近服从多维正态分布（multivariate Normal distribution,MVN）。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-7">
<span class="eqno">(10.1.8)<a class="headerlink" href="#equation-glm-source-influence-7" title="公式的永久链接"></a></span>\[\textbf{U} \sim MVN(\textbf{0},\mathbf{\mathcal{J}})\]</div>
<p>在大样本下有</p>
<div class="math notranslate nohighlight" id="equation-eq-glm-influence-036">
<span class="eqno">(10.1.9)<a class="headerlink" href="#equation-eq-glm-influence-036" title="公式的永久链接"></a></span>\[\textbf{U}^T \mathbf{\mathcal{J}}^{-1} \textbf{U} \sim  \chi^2 (p+1)\]</div>
<p>在 <code class="docutils literal notranslate"><span class="pre">GLM</span></code> 中通常都是有多个协变量参数的，我们默认符号 <span class="math notranslate nohighlight">\(U\)</span> 是一个向量。</p>
</section>
<section id="id4">
<h3><span class="section-number">10.1.2. </span>检验过程<a class="headerlink" href="#id4" title="永久链接至标题"></a></h3>
<p>假设我们的特征变量 <span class="math notranslate nohighlight">\(X\)</span> 一共有 <span class="math notranslate nohighlight">\(k\)</span> 个，即 <span class="math notranslate nohighlight">\(X_i = [X_1,X_2,\cdots,X_k]\)</span>
，对应的协变量参数向量为 <span class="math notranslate nohighlight">\(\beta=[\beta_0,\beta_1,\cdots,\beta_k]\)</span>
。现在把 <span class="math notranslate nohighlight">\(X_i\)</span> 分成两部分，
一部分有 <span class="math notranslate nohighlight">\(p\)</span> 个，记作 <span class="math notranslate nohighlight">\(X_{i}^p=[X_1,X_2,\cdots,X_p]\)</span>，
相应的协变量参数为 <span class="math notranslate nohighlight">\(\beta^p=[\beta_0,\beta_1,\cdots,\beta_p]\)</span>。
另一部分有 <span class="math notranslate nohighlight">\(q\)</span> 个，记作 <span class="math notranslate nohighlight">\(X_{i}^q=[X_{p+1},X_{p+2},\cdots,X_k]\)</span>，
对应的协变量参数为 <span class="math notranslate nohighlight">\(\beta^q=[\beta_{p+1},\beta_{p+2},\cdots,\beta_k]\)</span>。
其中 <span class="math notranslate nohighlight">\(p+q=k\)</span>。
假设给模型添加一个约束 <span class="math notranslate nohighlight">\(\beta_q=0\)</span>，拉格朗日乘子检验就是检验这个约束是否成立。
换一种说法就是，增加这 <span class="math notranslate nohighlight">\(q\)</span> 个特征后模型的效果是否有显著性提升。</p>
<p>检验的零假设记作</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-8">
<span class="eqno">(10.1.10)<a class="headerlink" href="#equation-glm-source-influence-8" title="公式的永久链接"></a></span>\[H_0 : \beta_q = [\beta_{p+1},\beta_{p+2},\cdots,\beta_k] = [0,0,\dots,0]\]</div>
<p>对立假设为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-9">
<span class="eqno">(10.1.11)<a class="headerlink" href="#equation-glm-source-influence-9" title="公式的永久链接"></a></span>\[H_a : \beta_q = [\beta_{p+1},\beta_{p+2},\cdots,\beta_k] \neq [0,0,\dots,0]\]</div>
<p>首先是，在 <span class="math notranslate nohighlight">\(H_0\)</span> 成立的基础上训练出一个拟合模型，
这等价于训练一个只包含 <span class="math notranslate nohighlight">\(p\)</span> 个参数（特征变量）的模型，记作 <span class="math notranslate nohighlight">\(M_p\)</span>。</p>
<p>然后需要计算检验统计量的值，拉格朗日乘子检验使用的统计量是 <a class="reference internal" href="#equation-eq-glm-influence-036">公式(10.1.9)</a>，
这里暂时把这个统计量记作 <span class="math notranslate nohighlight">\(H\)</span>。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-10">
<span class="eqno">(10.1.12)<a class="headerlink" href="#equation-glm-source-influence-10" title="公式的永久链接"></a></span>\[H(\beta^q) = U(\beta^q)^T \mathcal{J(\beta^q)}^{-1} U(\beta^q)\]</div>
<p>需要注意的是，我们的零假设是关于 <span class="math notranslate nohighlight">\(\beta^q\)</span> 的，
所以这里计算的是 <span class="math notranslate nohighlight">\(H(\beta_q)\)</span> 的值，
而不是 <span class="math notranslate nohighlight">\(H(\beta_p)\)</span> 。
公式中的 <span class="math notranslate nohighlight">\(U(\beta^q)_j\)</span> 按照 <a class="reference internal" href="#equation-eq-glm-influence-033">公式(10.1.2)</a> 进行计算，
其中 <span class="math notranslate nohighlight">\(\hat{y_i}\)</span> 的值是上一步训练出的只包含 <span class="math notranslate nohighlight">\(p\)</span> 个特征的模型 <span class="math notranslate nohighlight">\(M_p\)</span> 的预测值，
<span class="math notranslate nohighlight">\(x_{ij}\)</span> 是参数 <span class="math notranslate nohighlight">\(\beta_{j}^q\)</span> 对应的特征值。</p>
<p>根据上一节的结论，这个统计量的抽样分布是渐近卡方分布。
可以使用卡方检验得出结论，
如果 <span class="math notranslate nohighlight">\(H(\beta^q)\)</span> 的值落在拒绝域，则拒绝零假设，
这意味着拒绝 <span class="math notranslate nohighlight">\(\beta_q\)</span> 全部为 <span class="math notranslate nohighlight">\(0\)</span> 的假设。
也就是说如果为模型增加 <span class="math notranslate nohighlight">\(X_q\)</span> 这部分特征，模型的效果能得到显著的提升。</p>
<p>我们可以为 <span class="math notranslate nohighlight">\(\beta^q\)</span> 的中每一个 <span class="math notranslate nohighlight">\(\beta^q_j\)</span> 单独计算出一个 <span class="math notranslate nohighlight">\(H(\beta^q_j)\)</span>
的值，每个参数独立进行检验，此时统计量的自由度是 <span class="math notranslate nohighlight">\(1\)</span>。
也可以全部 <span class="math notranslate nohighlight">\(q\)</span> 个参数一起计算出整体的 <span class="math notranslate nohighlight">\(H(\beta^q)\)</span>
，以此对全部 <span class="math notranslate nohighlight">\(q\)</span> 参数的整体进行检验，此时统计量的自由度是 <span class="math notranslate nohighlight">\(q\)</span>。</p>
<p>有时还可以对 <span class="math notranslate nohighlight">\(H\)</span> 进行开方，得到一个渐近服从标准正态分布的统计量，
利用标准正态分布进行检验也是可以的。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-11">
<span class="eqno">(10.1.13)<a class="headerlink" href="#equation-glm-source-influence-11" title="公式的永久链接"></a></span>\[\sqrt{H} \sim \mathcal{N}(0,1)\]</div>
<p>拉格朗日乘子检验使用的是得分（score）统计量，因此也被称作分数检验（score test）。</p>
</section>
</section>
<section id="wald">
<span id="ch-glm-influence-wald"></span><h2><span class="section-number">10.2. </span>wald 检验<a class="headerlink" href="#wald" title="永久链接至标题"></a></h2>
<p>拉格朗日乘子检验是对得分统计量的检验，
本节我们讨论的 <code class="docutils literal notranslate"><span class="pre">wald</span></code> 检验是直接对参数估计量的检验。
我们先给出 <code class="docutils literal notranslate"><span class="pre">GLM</span></code> 模型中协变量参数估计量的抽样分布，
然后再给出检验过程，事实上它的检验过程和 Z 检验是没有太大区别的。</p>
<div class="topic">
<p class="topic-title">泰勒级数</p>
<p>定义一个单变量的函数 <span class="math notranslate nohighlight">\(f(x)\)</span>，
对于函数上的某个点 <span class="math notranslate nohighlight">\(x=t\)</span> 的附近有如下近似成立：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-12">
<span class="eqno">(10.2.1)<a class="headerlink" href="#equation-glm-source-influence-12" title="公式的永久链接"></a></span>\[f(x) = f(t) + (x-t)\left[ \frac{df}{dx} \right]_{x=t}
+ \frac{1}{2}(x-t)^2 \left[ \frac{d^2f}{d x^2}  \right ]_{x=t}
+ \dots\]</div>
</div>
<section id="id5">
<h3><span class="section-number">10.2.1. </span>参数估计量<a class="headerlink" href="#id5" title="永久链接至标题"></a></h3>
<p>对数似然函数的一阶偏导数又叫做得分统计量，记作 <span class="math notranslate nohighlight">\(U(\beta)\)</span>
，它是一个关于协变量参数 <span class="math notranslate nohighlight">\(\beta\)</span> 的函数。
现在我们把得分函数在 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 附近的按照泰勒级数近似展开，
忽略二阶以及更高阶的项。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-13">
<span class="eqno">(10.2.2)<a class="headerlink" href="#equation-glm-source-influence-13" title="公式的永久链接"></a></span>\[U(\beta) = U(\hat{\beta}) + (\beta-\hat{\beta}) U'(\hat{\beta})\]</div>
<p><span class="math notranslate nohighlight">\(U(\hat{\beta})\)</span> 是对数似然函数在点 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 处的一阶偏导数，
参数估计值 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 是通过令 <span class="math notranslate nohighlight">\(U(\beta)=0\)</span> 得到的，
所以显然有 <span class="math notranslate nohighlight">\(U(\hat{\beta})=0\)</span> 成立。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-14">
<span class="eqno">(10.2.3)<a class="headerlink" href="#equation-glm-source-influence-14" title="公式的永久链接"></a></span>\[U(\beta) =  (\beta-\hat{\beta}) U'(\hat{\beta})\]</div>
<p><span class="math notranslate nohighlight">\(U'(\hat{\beta})\)</span> 得分函数的偏导数，也就是对数似然函数在点 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 处的二阶偏导数，
一般称为海森矩阵，记作 <span class="math notranslate nohighlight">\(H(\hat{\beta})=U'(\hat{\beta})\)</span>。</p>
<div class="math notranslate nohighlight" id="equation-eq-glm-influence-204">
<span class="eqno">(10.2.4)<a class="headerlink" href="#equation-eq-glm-influence-204" title="公式的永久链接"></a></span>\[U(\beta) =  (\beta-\hat{\beta}) U'(\hat{\beta})
=  H(\hat{\beta})(\beta-\hat{\beta})\]</div>
<p>海森矩阵的期望等于信息矩阵的负数，</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-15">
<span class="eqno">(10.2.5)<a class="headerlink" href="#equation-glm-source-influence-15" title="公式的永久链接"></a></span>\[\mathbb{E}[ H(\hat{\beta}) ] = - \mathcal{J}(\hat{\beta})\]</div>
<p>我们用信息矩阵近似的代替海森矩阵，
<a class="reference internal" href="#equation-eq-glm-influence-204">公式(10.2.4)</a> 可以进一步改写成</p>
<div class="math notranslate nohighlight" id="equation-eq-glm-influence-206">
<span class="eqno">(10.2.6)<a class="headerlink" href="#equation-eq-glm-influence-206" title="公式的永久链接"></a></span>\[U(\beta) = -  \mathcal{J}(\hat{\beta})(\beta-\hat{\beta})
=  \mathcal{J}(\hat{\beta})(\hat{\beta}-\beta)\]</div>
<p>等价的有</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-16">
<span class="eqno">(10.2.7)<a class="headerlink" href="#equation-glm-source-influence-16" title="公式的永久链接"></a></span>\[(\hat{\beta}-\beta) = \mathcal{J}^{-1} U\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mathcal{J}\)</span> 可以看做是常量，根据 <span class="math notranslate nohighlight">\(\mathbb{E}[U]=0\)</span> 可得</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-17">
<span class="eqno">(10.2.8)<a class="headerlink" href="#equation-glm-source-influence-17" title="公式的永久链接"></a></span>\[\mathbb{E}[\hat{\beta}-\beta] =   \mathcal{J}^{-1} \mathbb{E}[U] = 0\]</div>
<p>因此可得 <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\beta}]=\beta\)</span>，
估计量 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 是参数 <span class="math notranslate nohighlight">\(\beta\)</span> 的一致估计。
现在来看下估计量 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 的方差 <span class="math notranslate nohighlight">\(V(\hat{\beta})\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-18">
<span class="eqno">(10.2.9)<a class="headerlink" href="#equation-glm-source-influence-18" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}V(\hat{\beta}) &amp;= \mathbb{E} \left [ (\hat{\beta} -\mathbb{E}[\hat{\beta}]) (\hat{\beta}-\mathbb{E}[\hat{\beta}])^T \right ]\\&amp;= \mathbb{E} \left[ (\hat{\beta} -\beta) (\hat{\beta} -\beta)^T \right ]\\&amp;= \mathbb{E} \left[ \mathcal{J}^{-1}U U^T \mathcal{J}^{-1} \right ]\\&amp;= \mathcal{J}^{-1} \mathbb{E} \left[ U U^T  \right ] \mathcal{J}^{-1}\\&amp;= \mathcal{J}^{-1}\end{aligned}\end{align} \]</div>
<p>根据上一节的结论（<a class="reference internal" href="#equation-eq-glm-influence-015">公式(10.1.6)</a>），统计量 <span class="math notranslate nohighlight">\(U/\sqrt{\mathcal{J}}\)</span>
的抽样分布是标准高斯分布，可得</p>
<div class="math notranslate nohighlight" id="equation-eq-glm-influence-207">
<span class="eqno">(10.2.10)<a class="headerlink" href="#equation-eq-glm-influence-207" title="公式的永久链接"></a></span>\[\frac{U}{\sqrt{\mathcal{J}}}
=  (\hat{\beta}-\beta) \sqrt{\mathcal{J}(\hat{\beta})}  \sim \mathcal{N}(0,1)\]</div>
<p>也可以写成</p>
<div class="math notranslate nohighlight" id="equation-eq-glm-influence-208">
<span class="eqno">(10.2.11)<a class="headerlink" href="#equation-eq-glm-influence-208" title="公式的永久链接"></a></span>\[\hat{\beta} \sim \mathcal{N}(\beta, \mathcal{J}^{-1})\]</div>
<p>如果 <span class="math notranslate nohighlight">\(Y\)</span> 的分布是正态分布，似然估计量 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 就是精确服从正态分布，而不是渐近了。
如果 <span class="math notranslate nohighlight">\(Y\)</span> 的分布是非正态分布，似然估计量 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 就是渐近服从正态分布。</p>
<p>参考本节开始时的理论（<a class="reference internal" href="#equation-eq-influence-110">公式(10.3)</a>）， <a class="reference internal" href="#equation-eq-glm-influence-207">公式(10.2.10)</a>
平方之后得到卡方统计量。</p>
<div class="math notranslate nohighlight" id="equation-eq-glm-influence-210">
<span class="eqno">(10.2.12)<a class="headerlink" href="#equation-eq-glm-influence-210" title="公式的永久链接"></a></span>\[(\hat{\beta}-\beta)^T\mathcal{J}(\hat{\beta})(\hat{\beta}-\beta) \sim \chi^2(p+1)\]</div>
<p><span class="math notranslate nohighlight">\(p\)</span> 是模型的特征数量，也是协变量参数的数量（不含截距参数），<span class="math notranslate nohighlight">\(p+1\)</span> 中的 <span class="math notranslate nohighlight">\(1\)</span> 代表截距参数，
<span class="math notranslate nohighlight">\(p+1\)</span> 就是模型的参数数量。
<a class="reference internal" href="#equation-eq-glm-influence-210">公式(10.2.12)</a> 又叫做 <code class="docutils literal notranslate"><span class="pre">Wald</span></code> 统计量。</p>
</section>
<section id="id6">
<h3><span class="section-number">10.2.2. </span>检验过程<a class="headerlink" href="#id6" title="永久链接至标题"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Wald</span></code> 统计量是有关参数估计量的统计量，因此可以用它对参数估计量进行检验。
检验过程和拉格朗日乘子检验非常类似，
不同的地方在于，拉格朗日乘子检验是训练一个参数较少的模型，然后检验新增特征是否有显著的意义，
而 <code class="docutils literal notranslate"><span class="pre">Wald</span></code> 检验正相反，
<code class="docutils literal notranslate"><span class="pre">Wald</span></code> 检验是训练一个包含全部特征（更多参数）的模型，然后检验模型中部分参数是否有显著意义，
如果没有，意味着这些特征（参数）可以从模型中去掉。</p>
<p><code class="docutils literal notranslate"><span class="pre">Wald</span></code> 检验的零假设就是假设协变量参数 <span class="math notranslate nohighlight">\(\beta\)</span> 的真实值是某个特定的值，
然后基于这个假设做进一步的显著性检验。
通常会假设参数真实值为 <span class="math notranslate nohighlight">\(0\)</span>，比如假设 <span class="math notranslate nohighlight">\(\beta_j=0\)</span>，
如果最后接受这个假设，意味着对应的特征 <span class="math notranslate nohighlight">\(X_j\)</span> 对模型的拟合观测数据是没有贡献的，
理论上就可以去掉这一维特征，进行得到一个更简单（参数更少）的模型。</p>
<p><code class="docutils literal notranslate"><span class="pre">Wald</span></code> 检验可以对每个参数独立检验，
此时可以用 <a class="reference internal" href="#equation-eq-glm-influence-208">公式(10.2.11)</a> 的标准正态分布对单一参数进行检验，
也可以用 <a class="reference internal" href="#equation-eq-glm-influence-210">公式(10.2.12)</a> 同时对全体参数进行检验（全部参数是否同时为 <span class="math notranslate nohighlight">\(0\)</span>）。
实际上对全部参数同时进行检验没有什么意义，
所以通常还是对每个参数单独进行检验。</p>
<p>符号 <span class="math notranslate nohighlight">\(j\)</span> 表示第 <span class="math notranslate nohighlight">\(j\)</span> 个特征，<span class="math notranslate nohighlight">\(\beta_j\)</span> 表示特征 <span class="math notranslate nohighlight">\(X_j\)</span> 对应的协变量参数，
零假设和对立假设，分别是</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-19">
<span class="eqno">(10.2.13)<a class="headerlink" href="#equation-glm-source-influence-19" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}H_0 : \beta_j = 0\\H_a : \beta_j \neq 0\end{aligned}\end{align} \]</div>
<p>单一参数进行检验的过程和 <a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-test"><span class="std std-numref">节 3.6</span></a> 讲的 Z 检验（T检验）没啥区别，
基本是一样的，这里就不再赘述了。
此外，根据参数估计量的抽样分布 <a class="reference internal" href="#equation-eq-glm-influence-208">公式(10.2.11)</a>
，可以同时给出参数估计值的置信区间。
有关置信区间的内容可以复习一下 <a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-interval"><span class="std std-numref">节 3.5</span></a>。</p>
</section>
</section>
<section id="id7">
<h2><span class="section-number">10.3. </span>似然比检验<a class="headerlink" href="#id7" title="永久链接至标题"></a></h2>
<p>在上一章我们已经介绍了对数似然比统计量（log-likelihood ratio,LLR），
<code class="docutils literal notranslate"><span class="pre">LLR</span></code> 用来对比两个嵌套模型的拟合优度，它是复杂模型（协变量参数多一些）和 简单模型（协变量参数少一些）的对数似然差值的2倍。</p>
<div class="math notranslate nohighlight" id="equation-eq-glm-test-021">
<span class="eqno">(10.3.1)<a class="headerlink" href="#equation-eq-glm-test-021" title="公式的永久链接"></a></span>\[LLR= 2(\ln L_g - \ln L_s)\]</div>
<p><code class="docutils literal notranslate"><span class="pre">LLR</span></code> 的值越大意味着被比较的两个模型对数据的拟合优度差异越大。
反之，<code class="docutils literal notranslate"><span class="pre">LLR</span></code> 的值比较小意味着两个模型对数据的拟合优度差异较小。
<code class="docutils literal notranslate"><span class="pre">LLR</span></code> 常用来做嵌套模型的对比选择，
如果两个模型对数据的拟合能力差别较小，我们更倾向于选择简单模型（协变量参数较少的模型）。
<code class="docutils literal notranslate"><span class="pre">LLR</span></code> 有时也会被用来做特征的筛选，对比去掉某些特征后模型的效果是否显著下降，
或者是增加某些特征后模型效果有没有显著的提升。</p>
<section id="id8">
<h3><span class="section-number">10.3.1. </span>抽样分布<a class="headerlink" href="#id8" title="永久链接至标题"></a></h3>
<p>我们继续用符号 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 表示协变量参数 <span class="math notranslate nohighlight">\(\beta\)</span> 的似然估计值，
<span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 是 <span class="math notranslate nohighlight">\(\beta\)</span> 的一致无偏估计，
在样本足够的情况下，理论上二者应该是比较接近的。
对数似然函数 <span class="math notranslate nohighlight">\(\ell(\beta)\)</span> 是关于 <span class="math notranslate nohighlight">\(\beta\)</span> 的一个函数，
在 <span class="math notranslate nohighlight">\(\beta=\hat{\beta}\)</span> 附近利用泰勒级数可以得到</p>
<div class="math notranslate nohighlight" id="equation-eq-glm-test-022">
<span class="eqno">(10.3.2)<a class="headerlink" href="#equation-eq-glm-test-022" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\ell(\beta) &amp;= \ell(\hat{\beta}) + (\beta - \hat{\beta}) \frac{\partial \ell(\beta)}{\partial \hat{\beta}}
+ \frac{1}{2}(\beta-\hat{\beta})^2   \frac{\partial^2 \ell(\beta)}{\partial \hat{\beta}^2}\\&amp;=  \ell(\hat{\beta})  + (\beta - \hat{\beta}) U(\hat{\beta})
-\frac{1}{2}(\beta -\hat{\beta} )^T\mathcal{J}(\hat{\beta})(\beta-\hat{\beta})\end{aligned}\end{align} \]</div>
<p><span class="math notranslate nohighlight">\(U(\hat{\beta})\)</span> 是对数似然函数在点 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 处的一阶偏导数，
<span class="math notranslate nohighlight">\(\mathcal{J}(\hat{\beta})\)</span> 是对数似然函数在点 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 处的二阶偏导数期望的负数，
参数估计值 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 是通过令 <span class="math notranslate nohighlight">\(U(\beta)=0\)</span> 得到的，
所以显然有 <span class="math notranslate nohighlight">\(U(\hat{\beta})=0\)</span> 成立。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-20">
<span class="eqno">(10.3.3)<a class="headerlink" href="#equation-glm-source-influence-20" title="公式的永久链接"></a></span>\[\ell(\beta) - \ell(\hat{\beta}) =   -\frac{1}{2}(\beta -\hat{\beta} )^T\mathcal{J}(\hat{\beta})(\beta-\hat{\beta})\]</div>
<p>继续移项，可得到如下统计量</p>
<div class="math notranslate nohighlight" id="equation-eq-influence-260">
<span class="eqno">(10.3.4)<a class="headerlink" href="#equation-eq-influence-260" title="公式的永久链接"></a></span>\[2[\ell(\hat{\beta}) - \ell(\beta) ] = (\hat{\beta} -\beta )^T\mathcal{J}(\hat{\beta})(\hat{\beta}-\beta)\]</div>
<p>依据 <a class="reference internal" href="#equation-eq-glm-influence-210">公式(10.2.12)</a> 这个统计量是服从自由度为 <span class="math notranslate nohighlight">\(p+1\)</span> 的卡方分布，<span class="math notranslate nohighlight">\(p+1\)</span>
是模型的参数数量。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-21">
<span class="eqno">(10.3.5)<a class="headerlink" href="#equation-glm-source-influence-21" title="公式的永久链接"></a></span>\[2[\ell(\hat{\beta}) - \ell(\beta) ] \sim \chi^2(p+1)\]</div>
<p>我们用下标 <span class="math notranslate nohighlight">\(s\)</span> 表示简单模型，比如 <span class="math notranslate nohighlight">\(\beta_s\)</span> 表示简单的模型的参数向量（真实值），
<span class="math notranslate nohighlight">\(\hat{\beta}_s\)</span> 表示简单模型的参数估计量，其协变量参数数量为 <span class="math notranslate nohighlight">\(p+1\)</span> 个。
用下标 <span class="math notranslate nohighlight">\(g\)</span> 表示负责模型，比如 <span class="math notranslate nohighlight">\(\beta_g\)</span> 表示复杂的模型的参数向量（真实值），
<span class="math notranslate nohighlight">\(\hat{\beta}_g\)</span> 表示复杂模型的参数估计量，其协变量参数数量为 <span class="math notranslate nohighlight">\(q+1\)</span> 个。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-22">
<span class="eqno">(10.3.6)<a class="headerlink" href="#equation-glm-source-influence-22" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}LLR &amp;= 2[\ell(\hat{\beta}_g) - \ell(\hat{\beta}_s)]\\&amp;= 2[\ell(\hat{\beta}_g) - \ell(\hat{\beta}_s)]
+ 2\ell(\beta_{g}) - 2\ell(\beta_{s})
+ 2\ell(\beta_{s})  - 2\ell(\beta_{g})\\&amp;= \underbrace{2[ \ell(\hat{\beta}_g ) -  \ell(\beta_g)  ]}_{\chi^2(q+1)}
- \underbrace{2[ \ell(\hat{\beta}_s) - \ell(\beta_s)  ]}_{\chi^2(p+1)}
+ \underbrace{2[ \ell(\beta_{g}) - \ell(\beta_{s})   ]}_{\text{常数值}v}\end{aligned}\end{align} \]</div>
<p>其中 <span class="math notranslate nohighlight">\(\ell(\beta_{g})\)</span> 与 <span class="math notranslate nohighlight">\(\ell(\beta_{s})\)</span> 表示参数真实值的似然值（模型的理论最大似然值），是一个数值，不是统计量。
最终 <span class="math notranslate nohighlight">\(LLR\)</span> 可以看做是由三部分组成，自由度为 <span class="math notranslate nohighlight">\(q+1\)</span> 的卡方统计量减去自由度为 <span class="math notranslate nohighlight">\(p+1\)</span> 的卡方统计量，
再加上一个常数值 <span class="math notranslate nohighlight">\(v\)</span> 。</p>
<p>根据卡方分布的特性，统计量 <span class="math notranslate nohighlight">\(LLR\)</span> 渐近服从自由度为 <span class="math notranslate nohighlight">\(q-p\)</span> 的 <strong>非中心卡方分布</strong>。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-23">
<span class="eqno">(10.3.7)<a class="headerlink" href="#equation-glm-source-influence-23" title="公式的永久链接"></a></span>\[LLR \sim \chi^2(q-p,v)\]</div>
<p>注意偏差统计量 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 是一个 <strong>非中心卡方分布</strong>，这和之前介绍的统计量不同，
<span class="math notranslate nohighlight">\(v\)</span> 是非中心参数。
<code class="docutils literal notranslate"><span class="pre">LLR</span></code> 的期望值是 <span class="math notranslate nohighlight">\(\mathbb{E}[\text{LLR}] = q-p+v\)</span> 。
现在来重点看一下 <span class="math notranslate nohighlight">\(v\)</span> 的值，</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-24">
<span class="eqno">(10.3.8)<a class="headerlink" href="#equation-glm-source-influence-24" title="公式的永久链接"></a></span>\[v = 2[ \ell(\beta_{s};y) - \ell(\beta_{f};y)   ]\]</div>
<p><span class="math notranslate nohighlight">\(v\)</span> 的值是复杂模型的理论最大似然值和简单模型的理论最大似然值的差，
两个模型对数据的拟合能力越接近，这个差值 <span class="math notranslate nohighlight">\(v\)</span> 就越小。
极限情况下，两个模型拟合能力一样好，差值 <span class="math notranslate nohighlight">\(v=0\)</span>。
此时 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 就是渐进服从 <strong>中心卡方分布</strong> <span class="math notranslate nohighlight">\(\chi^2(q-p)\)</span> 。</p>
<p>如果响应变量 <span class="math notranslate nohighlight">\(Y\)</span> 是高斯分布，则统计量 <code class="docutils literal notranslate"><span class="pre">LLR</span></code>
就是确切服从（非中心）卡方分布的，而不是渐近的。
如果响应变量 <span class="math notranslate nohighlight">\(Y\)</span> 不是高斯分布，则统计量 <code class="docutils literal notranslate"><span class="pre">LLR</span></code>
是 <strong>渐近</strong> 服从（非中心）卡方分布的。</p>
</section>
<section id="id9">
<h3><span class="section-number">10.3.2. </span>模型比较<a class="headerlink" href="#id9" title="永久链接至标题"></a></h3>
<p>似然比统计量可以用来比较两个嵌套模型对同一份数据的拟合效果。
在 <code class="docutils literal notranslate"><span class="pre">GLM</span></code> 中 ，要求两个模型具有相同的指数族分布，以及同样的连接函数，
被比较的两个模型只有线性预测器是不同的，一个参数多，一个参数少，换句话说一个使用的特征多，另一个使用的特征少。
<strong>这种嵌套模型比较通常可以用来判断某些特征是否有价值，对模型是否有足够的贡献</strong>。
然而理论上，两个模型参数不同，对数据的拟合度必然会略有不同，
两个模型的似然值也必然会有一些差异。
但是这个差异能否说明两个模型对数据的拟合能力具有统计显著性，就需要通过检验给出结论，
这个可以通过似然比检验实现。</p>
<p>在 <code class="docutils literal notranslate"><span class="pre">GLM</span></code> 中，检验两个模型拟合能力是否有显著差异的一般性步骤是：</p>
<ol class="arabic simple">
<li><p>定义模型 <span class="math notranslate nohighlight">\(M_0\)</span> 对应着零假设 <span class="math notranslate nohighlight">\(H_0\)</span>，定义另一个更一般(参数更多)的模型 <span class="math notranslate nohighlight">\(M_1\)</span> 对应着备择假设 <span class="math notranslate nohighlight">\(H_a\)</span>。
零假设 <span class="math notranslate nohighlight">\(H_0\)</span> 表示模型 <span class="math notranslate nohighlight">\(M_0\)</span> 和 <span class="math notranslate nohighlight">\(M_1\)</span> 拟合度一样好，反之，
备择假设 <span class="math notranslate nohighlight">\(H_a\)</span> 表示  <span class="math notranslate nohighlight">\(M_0\)</span> 比  <span class="math notranslate nohighlight">\(M_1\)</span> 拟合度差。</p></li>
<li><p>训练模型 <span class="math notranslate nohighlight">\(M_0\)</span> ，然后计算一个拟合优度(goodness of fit,GOF)指标统计量 <span class="math notranslate nohighlight">\(G_0\)</span> 。同样训练模型 <span class="math notranslate nohighlight">\(M_1\)</span> 并计算拟合优度指标 <span class="math notranslate nohighlight">\(G_1\)</span> 。</p></li>
<li><p>计算两个模型拟合度的差异，通常可以是 <span class="math notranslate nohighlight">\(\Delta G=G_1-G_0\)</span> ，或者是 <span class="math notranslate nohighlight">\(\Delta G=G_1/G_0\)</span> 。</p></li>
<li><p>使用差值统计量 <span class="math notranslate nohighlight">\(\Delta G\)</span> 的抽样分布检验接受假设 <span class="math notranslate nohighlight">\(G_1=G_0\)</span> 还是 <span class="math notranslate nohighlight">\(G_1 \ne G_0\)</span></p></li>
<li><p>如果假设 <span class="math notranslate nohighlight">\(G_1=G_0\)</span> 没有被拒绝，则接受 <span class="math notranslate nohighlight">\(H_0\)</span> 。反之，如果假设 <span class="math notranslate nohighlight">\(G_1=G_0\)</span> 被拒绝，则接受备择假设 <span class="math notranslate nohighlight">\(H_a\)</span>，
<span class="math notranslate nohighlight">\(M_1\)</span> 模型在统计学上显著更优。</p></li>
</ol>
<p>我们以对数似然比检验为例，
首先我们设定零假设代表模型 <span class="math notranslate nohighlight">\(M_0\)</span>，模型参数数量为 <span class="math notranslate nohighlight">\(p+1\)</span> 。
对立假设代表模型 <span class="math notranslate nohighlight">\(M_1\)</span> ，参数数量为 <span class="math notranslate nohighlight">\(q+1\)</span> 。
并且有 <span class="math notranslate nohighlight">\(q&gt;p\)</span> 成立。
零假设和对立假设分别为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-25">
<span class="eqno">(10.3.9)<a class="headerlink" href="#equation-glm-source-influence-25" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}&amp;H_0: G_0=G_1 \ \text{两个模型拟合效果一样}\\&amp;H_1: G_0 \neq G_1 \ \text{两个模型拟合效果具有统计学上的显著差异}\end{aligned}\end{align} \]</div>
<p>拟合优度指标选择对数似然值，
我们用 <span class="math notranslate nohighlight">\(\ell_0\)</span> 表示模型 <span class="math notranslate nohighlight">\(M_0\)</span> 的对数似然值，
用符号 <span class="math notranslate nohighlight">\(\ell_1\)</span> 表示模型 <span class="math notranslate nohighlight">\(M_1\)</span> 的对数似然值，
两个模型 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-26">
<span class="eqno">(10.3.10)<a class="headerlink" href="#equation-glm-source-influence-26" title="公式的永久链接"></a></span>\[\Delta G = \text{LLR} =2( \ell_1 - \ell_0)\]</div>
<p>统计量 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 的抽样分布是卡方分布</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-27">
<span class="eqno">(10.3.11)<a class="headerlink" href="#equation-glm-source-influence-27" title="公式的永久链接"></a></span>\[\text{LLR} \sim \chi^2(q-p,v)\]</div>
<p>如果两个模型的拟合能力是接近的，则 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 期望值是 <span class="math notranslate nohighlight">\(q-p\)</span>，
否则就是 <span class="math notranslate nohighlight">\(q-p+v\)</span>。
换句话说，在零假设成立的条件下，<code class="docutils literal notranslate"><span class="pre">LLR</span></code> 的抽样分布是自由度为 <span class="math notranslate nohighlight">\(q-p\)</span>
的中心卡方分布。</p>
<p>根据假设检验的过程，我们计算出 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 的值，然后看这个值是否落在
分布 <span class="math notranslate nohighlight">\(\chi^2(p-q)\)</span> 的拒绝域(比如是否落在图形右侧 <span class="math notranslate nohighlight">\(100*\alpha \%\)</span> 的区域内)
。如果落在拒绝域内，则拒绝 <span class="math notranslate nohighlight">\(H_0\)</span> 假设，接受 <span class="math notranslate nohighlight">\(H_1\)</span> 假设。
通常如果两个模型拟合能力相差巨大，<code class="docutils literal notranslate"><span class="pre">LLR</span></code> 直观上远远大于 <span class="math notranslate nohighlight">\(q-p\)</span> 了，此时也没有进行假设检验的必要了。
当两个模型的拟合能力比较接近，从经验上(直观上)无法判断是否显著时，才有假设检验的必要。</p>
</section>
<section id="id10">
<h3><span class="section-number">10.3.3. </span>偏差统计量<a class="headerlink" href="#id10" title="永久链接至标题"></a></h3>
<p>我们知道偏差统计量就是饱和模型（saturated model）和拟合模型的对数似然比，
记作</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-28">
<span class="eqno">(10.3.12)<a class="headerlink" href="#equation-glm-source-influence-28" title="公式的永久链接"></a></span>\[D = 2[\ell_s - \ell_t]\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\ell_s\)</span> 表示饱和模型对数似然值，
<span class="math notranslate nohighlight">\(\ell_t\)</span> 表示拟合模型的对数似然值。
饱和模型的参数数量和观测样本的数量 <span class="math notranslate nohighlight">\(N\)</span> 是相同的，
假设拟合模型的参数数量是 <span class="math notranslate nohighlight">\(p+1\)</span>，
显然偏差统计量的抽样分布就是</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-29">
<span class="eqno">(10.3.13)<a class="headerlink" href="#equation-glm-source-influence-29" title="公式的永久链接"></a></span>\[D \sim \chi^2(N-p-1,v)\]</div>
<p>根据卡方分布的特性，统计量 <span class="math notranslate nohighlight">\(D\)</span> 渐近服从 <strong>非中心卡方分布</strong> ，
其自由度是 <span class="math notranslate nohighlight">\(N-p-1\)</span> 。</p>
<p>模型对数据拟合的越好(越接近饱和模型)，其偏差 <span class="math notranslate nohighlight">\(D\)</span> 就越接近中心卡方分布 <span class="math notranslate nohighlight">\(\chi^2(N-p-1)\)</span> ，
此时偏差统计量 <span class="math notranslate nohighlight">\(D\)</span> 的期望就越接近 <span class="math notranslate nohighlight">\(N-p-1\)</span> 。反之如果模型拟合的不好，偏差统计量 <span class="math notranslate nohighlight">\(D\)</span>
就是非中心卡方分布 <span class="math notranslate nohighlight">\(\chi^2(N-p-1,v)\)</span> ，其期望值就是 <span class="math notranslate nohighlight">\(v+N-p-1\)</span> 。</p>
<p>既然偏差统计量就是对数似然比统计量，原则上可以用偏差统计检验拟合模型和饱和模型的拟合能力是否具有显著性差异，
然而实际上这没有意义。
实际应用中，拟合模型的参数数量普遍是远远小于样本数量的，二者对数据的拟合能力肯定是相差很大的，
也就是说偏差值几乎必然是显著的，没有必要进行检验了。</p>
</section>
<section id="f">
<h3><span class="section-number">10.3.4. </span>F 检验<a class="headerlink" href="#f" title="永久链接至标题"></a></h3>
<p>似然比检验可以用来比较两个嵌套模型是否有显著差异，
进而判断两个模型相差的那些特征对模型是否有显著意义。
然而对于 <code class="docutils literal notranslate"><span class="pre">GLM</span></code> 的某些模型计算出准确的对数似然值并不容易。
回顾一下 <code class="docutils literal notranslate"><span class="pre">GLM</span></code> 模型对数似然函数的一般形式</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-30">
<span class="eqno">(10.3.14)<a class="headerlink" href="#equation-glm-source-influence-30" title="公式的永久链接"></a></span>\[\ell(\beta)= \sum_{i=1}^N \left \{   \frac{Y_i \theta_i - b(\theta_i)}{a(\phi)}   + c(y_i,\phi)   \right \}\]</div>
<p>可以看到对数似然函数依赖分散参数 <span class="math notranslate nohighlight">\(\phi\)</span>，对于嵌套模型 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 来说，
式中的项 <span class="math notranslate nohighlight">\(c(y_i,\phi)\)</span> 是可以抵消掉的，
但是 <span class="math notranslate nohighlight">\(a(\phi)\)</span> 仍然是存在的，
如果这个 <span class="math notranslate nohighlight">\(\phi\)</span> 未知显然是无法计算出来的。
当然部分 <code class="docutils literal notranslate"><span class="pre">GLM</span></code> 的模型是没有分散参数的，也就不存在这个问，比如大部分的离散模型。
然后很多连续值模型是存在分散参数的。
在 <code class="docutils literal notranslate"><span class="pre">GLM</span></code> 中，通常会建立如下两个假设来简化这个问题。</p>
<ul class="simple">
<li><p>对比的两个模型是嵌套模型，并且共享分散参数 <span class="math notranslate nohighlight">\(\phi\)</span>，即两个模型使用同样的参数值。</p></li>
<li><p>分散参数 <span class="math notranslate nohighlight">\(\phi\)</span> 与样本观测样本无关，即所有观测样本有一样的参数值。</p></li>
</ul>
<p>在这两个假设成立的前提下，可以估计值 <span class="math notranslate nohighlight">\(\phi\)</span> 的值，然后代入进去求得 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 的值，
有关 <span class="math notranslate nohighlight">\(\phi\)</span> 的估计方法在前面的章节中已经讨论过，这里就不再细说了。
传统线性回归模型的做法是假设分散参数是常量 <span class="math notranslate nohighlight">\(1\)</span>，
即假设 <span class="math notranslate nohighlight">\(\phi=\sigma^2=1\)</span>
。当然这样的强假设未必对所有数据都成立，有关这个问题不再本节的讨论范围内，就不细说了。
这里我们讨论另外一种方法解决 <span class="math notranslate nohighlight">\(\phi\)</span> 未知的问题。</p>
<p>回顾下 <code class="docutils literal notranslate"><span class="pre">GLM</span></code> 模型一般形式的定义，在定义中，分散参数 <span class="math notranslate nohighlight">\(\phi\)</span> 与线性预测器 <span class="math notranslate nohighlight">\(\eta_i=\beta^T x_i\)</span> 是独立无关的，
换句话说，两个嵌套模型，拥有同样的 <span class="math notranslate nohighlight">\(\phi\)</span>。
在这个假设的前提下 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 可以写为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-31">
<span class="eqno">(10.3.15)<a class="headerlink" href="#equation-glm-source-influence-31" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}LLR &amp;= 2[\ell_{M_1} - \ell_{M_0}]\\&amp;= 2 \left \{ \sum_{i=1}^N \left [   \frac{Y_i \theta_i - b(\theta_i)}{a(\phi)}   + c(y_i,\phi)  \right ]_{M_1}
-
 \sum_{i=1}^N \left [   \frac{Y_i \theta_i - b(\theta_i)}{a(\phi)}   + c(y_i,\phi)  \right ]_{M_0}
    \right \}\\
&amp;=  \frac{2[ \ell'_{M_1} - \ell'_{M_0}] }{a(\phi)}\\&amp;=  \frac{LLR'}{a(\phi)} \sim \chi^2(q-p)\end{aligned}\end{align} \]</div>
<p>其中 <span class="math notranslate nohighlight">\(\ell'\)</span> 为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-32">
<span class="eqno">(10.3.16)<a class="headerlink" href="#equation-glm-source-influence-32" title="公式的永久链接"></a></span>\[\ell' = \sum_{i=1}^N \left [   \frac{Y_i \theta_i - b(\theta_i)}{a(\phi)} \right ]\]</div>
<p>偏差统计量是似然比的一个特例，用符号 <span class="math notranslate nohighlight">\(M_s\)</span> 表示饱和模型，模型 <span class="math notranslate nohighlight">\(M_0\)</span> 和模型 <span class="math notranslate nohighlight">\(M_1\)</span>
的偏差统计量分别为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-33">
<span class="eqno">(10.3.17)<a class="headerlink" href="#equation-glm-source-influence-33" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}D_{M_0} &amp;= \frac{2[ \ell'_{M_s} - \ell'_{M_0}] }{a(\phi)} = \frac{D'_{M_0}}{a(\phi)} \sim \chi^2(p+1)\\D_{M_1} &amp;= \frac{2[ \ell'_{M_s} - \ell'_{M_1}] }{a(\phi)} = \frac{D'_{M_1}}{a(\phi)} \sim \chi^2(q+1)\end{aligned}\end{align} \]</div>
<p>现在回顾下三大抽样分布中的 <span class="math notranslate nohighlight">\(F\)</span> 分布，根据 <span class="math notranslate nohighlight">\(F\)</span> 分布的定义，
两个卡方统计量各自除以自由度之后的比值服从 <span class="math notranslate nohighlight">\(F\)</span> 分布，
因此以下统计量的抽样分布是 <span class="math notranslate nohighlight">\(F\)</span> 分布。</p>
<div class="math notranslate nohighlight" id="equation-eq-glm-influence-345">
<span class="eqno">(10.3.18)<a class="headerlink" href="#equation-eq-glm-influence-345" title="公式的永久链接"></a></span>\[F = \left. \frac{LLR}{q-p} \middle/ \frac{D_{M_1}}{N-q-1} \right.
= \left. \frac{LLR'}{q-p} \middle/ \frac{D'_{M_1}}{N-q-1} \right.
\sim F(q-p,N-q-1)\]</div>
<p>实际上对数似然比 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 可以通过两个模型的偏差得到</p>
<div class="math notranslate nohighlight" id="equation-eq-glm-influence-346">
<span class="eqno">(10.3.19)<a class="headerlink" href="#equation-eq-glm-influence-346" title="公式的永久链接"></a></span>\[LLR = \Delta D =  D_{M_0} -  D_{M_1} = \frac{D'_{M_0} - D'_{M_1}}{a(\phi)} = \frac{\Delta D'}{a(\phi)}\]</div>
<p>因此 <span class="math notranslate nohighlight">\(F\)</span> 统计量也可以完全通过模型的偏差计算</p>
<div class="math notranslate nohighlight" id="equation-glm-source-influence-34">
<span class="eqno">(10.3.20)<a class="headerlink" href="#equation-glm-source-influence-34" title="公式的永久链接"></a></span>\[F = \left. \frac{\Delta D'}{q-p} \middle/ \frac{D'_{M_1}}{N-q-1} \right.
 \sim F(q-p,N-q-1)\]</div>
<p><span class="math notranslate nohighlight">\(F\)</span> 检验统计量可以消除分散参数 <span class="math notranslate nohighlight">\(\phi\)</span> 的影响，
并且 <span class="math notranslate nohighlight">\(F\)</span> 统计量的值可以利用模型的偏差计算得到，
而 <code class="docutils literal notranslate"><span class="pre">IRLS</span></code> 算法是可以同时产出模型的偏差值的，
因此  <span class="math notranslate nohighlight">\(F\)</span> 的值是比较容易得到的。</p>
<p>按照 <span class="math notranslate nohighlight">\(F\)</span> 分布的定义，两个独立的 <strong>中心卡方</strong> 随机变量各自除以自由度后，再相除得到 <strong>中心</strong> <span class="math notranslate nohighlight">\(F\)</span> 分布。
一个 <strong>非中心卡方</strong> 随机变量除以一个 <strong>中心卡方</strong> 随机变量得到 <strong>非中心</strong> <span class="math notranslate nohighlight">\(F\)</span> 分布。
<strong>这里都要求第二个卡方变量必须是中心卡方变量</strong>，所以要应用 <span class="math notranslate nohighlight">\(F\)</span> 检验统计量前提是模型 <span class="math notranslate nohighlight">\(M_1\)</span> 是一个”好的”模型，
其偏差统计量 <span class="math notranslate nohighlight">\(D_1\)</span> 是一个中心卡方分布。
然而这通常并不容易实现，
因此在实际应用中 <span class="math notranslate nohighlight">\(F\)</span> 检验也不是经常使用。</p>
</section>
</section>
<section id="id11">
<h2><span class="section-number">10.4. </span>总结<a class="headerlink" href="#id11" title="永久链接至标题"></a></h2>
<p>如上所述，这三种检验方法都是在解决相同的问题，即忽略（参数约束为 <span class="math notranslate nohighlight">\(0\)</span>）部分特征（预测变量）后模型的拟合度是否显著下降，
但它们的解决方法不相同。
似然比检验，必须同时训练出两个模型，然后比较这两个模型；
<code class="docutils literal notranslate"><span class="pre">score</span></code> 检验和 <code class="docutils literal notranslate"><span class="pre">wald</span></code> 检验近似于似然比检验，但只需要训练一个模型。
<code class="docutils literal notranslate"><span class="pre">score</span></code> 检验训练的是简单模型，模型不包括需要检验的那部分特征，
<code class="docutils literal notranslate"><span class="pre">wald</span></code> 检验训练的是复杂模型，模型包括了需要检验的特征集合。
<strong>随着样本变得无限大，三种检验方法趋近于等同的</strong>。</p>
<p>这三个测试之间的一个有趣的关系是，当模型为线性时，
三个测试统计量具有以下关系 <code class="docutils literal notranslate"><span class="pre">Wald</span></code> <span class="math notranslate nohighlight">\(\geq\)</span> <code class="docutils literal notranslate"><span class="pre">LR</span></code> <span class="math notranslate nohighlight">\(\geq\)</span>
<code class="docutils literal notranslate"><span class="pre">Score</span></code> （Johnston and DiNardo 1997 p.150）。
也就是说，<code class="docutils literal notranslate"><span class="pre">Wald</span></code> 检验统计量将始终大于 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 检验统计量，
而 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 检验统计量将始终大于 <code class="docutils literal notranslate"><span class="pre">Score</span></code> 检验统计量。</p>
<p>在有限的样本中，这三个方法往往会产生不同的检验统计量，但通常得出的结论是相同的。
当计算能力受到更大限制，训练模型需要很长时间时，能够使用单个模型来近似得到与 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 相同检验结果是一个相当大的优势。
如今，对于大多数研究人员可能想要比较的模型而言，计算时间已不再是问题，
我们通常建议在大多数情况下使用 <code class="docutils literal notranslate"><span class="pre">LLR</span></code> 检验。</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="content.html" class="btn btn-neutral float-left" title="9. 模型评估" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html" class="btn btn-neutral float-right" title="11. 高斯模型" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2018, zhangzhenhu(acmtiger@outlook.com) 禁止一切形式的转载！.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
  


    <script src="https://utteranc.es/client.js"
            repo="zhangzhenhu/blog"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>



</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>