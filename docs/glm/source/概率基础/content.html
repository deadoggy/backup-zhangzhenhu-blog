<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>1. 概率基础 &mdash; 张振虎的博客 张振虎 文档</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
    <link rel="canonical" href="https://zhangzhenhu.github.io/blog/glm/source/概率基础/content.html" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/translations.js"></script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"macros": {"RR": "{\\bf R}", "bold": ["{\\bf #1}", 1], "KL": ["{D_\\textrm{KL}\\left ( #1 \\| #2 \\right )}", 2], "EE": ["{\\mathbb{E}_{#1} \\left [ #2 \\right ]}", 2, ""], "scalemath": ["{\\scalebox{#1}{\\mbox{\\ensuremath{\\displaystyle #2}}}}", 2], "ind": ["{\\perp\\!\\!\\!\\!\\perp}"]}}}</script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" />
    <link rel="next" title="2. 最大似然估计" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html" />
    <link rel="prev" title="广义线性模型" href="../index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> 张振虎的博客
          </a>
              <div class="version">
                acmtiger@outlook.com
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">目录:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">广义线性模型</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">1.1. 概率模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">1.1.1. 概率律</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">1.1.2. 离散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">1.1.3. 连续模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id6">1.2. 条件概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">1.3. 联合概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id8">1.4. 全概率与贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">1.5. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id10">1.6. 随机变量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id11">1.6.1. 离散随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">1.6.2. 连续随机变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id13">1.6.3. 累积分布函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id14">1.6.4. 随机变量的函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id15">1.6.5. 期望与方差</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id16">1.7. 边缘化</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id17">1.8. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ch-basic-bernoulli">1.8.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id19">1.8.2. 二项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id20">1.8.3. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id21">1.8.4. 多项式分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id22">1.8.5. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id23">1.8.6. 卡方分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#t">1.8.7. t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="#f">1.8.8. F分布</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html">2. 最大似然估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-liklihood">2.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id3">2.2. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id4">2.3. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-2-gaussian-ml">2.4. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id6">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html">3. 推断与检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id2">3.1. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution">3.2. 抽样分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution-normal">3.2.1. 正态分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#t">3.2.2. 学生t分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id5">3.2.3. 卡方分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id6">3.3. 极限理论</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id7">3.3.1. 马尔可夫和切比雪夫不等式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id8">3.3.2. 弱大数定律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id9">3.3.3. 依概率收敛</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-clt">3.3.4. 中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id11">3.3.5. 强大数定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id12">3.4. 似然估计量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id13">3.4.1. 估计量的偏差与方差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-fisher-information">3.4.2. 信息量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-mle-estimator">3.4.3. 最大似然估计的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-interval">3.5. 置信区间</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#z">3.5.1. 均值参数的 Z 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id18">3.5.2. 均值参数的 T 区间估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id19">3.5.3. 方差参数的区间估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-test">3.6. 简单假设检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id22">3.6.1. Z检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id23">3.6.2. T检验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id24">3.6.3. 卡方检验</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html">4. 贝叶斯估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id2">4.1. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id3">4.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id4">4.1.2. 类别分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id5">4.2. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id6">4.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id7">4.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/content.html#id8">4.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html">5. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-1">5.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id3">5.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id4">5.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id5">5.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id6">5.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id7">5.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-moments">5.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id9">5.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#kl">5.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html">6. 线性回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id2">6.1. 最小二乘</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id3">6.1.1. 最小误差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id4">6.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id5">6.2. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id6">6.2.1. 高斯假设</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/content.html#id7">6.2.2. 参数估计</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html">7. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id2">7.1. 指数族分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id3">7.1.1. 自然指数族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id4">7.1.2. 示例：高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id5">7.1.3. 示例：伯努利分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id6">7.2. 广义线性模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id7">7.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html">8. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate">8.1. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id3">8.2. 泰勒级数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id4">8.3. 梯度下降法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id6">8.4. 牛顿法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id7">8.4.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id8">8.4.2. 标准连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id9">8.4.3. 迭代初始值的设定</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#irls">8.5. 迭代重加权最小二乘(IRLS)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id10">8.5.1. 算法推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id11">8.5.2. 算法过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id12">8.6. 估计量的标准误差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate-phi">8.7. 分散参数的估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html">9. 模型评估</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id2">9.1. 拟合优度</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id3">9.1.1. 嵌套模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#likelihood-ratio">9.1.2. 对数似然比(Likelihood ratio)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance">9.1.3. 偏差(deviance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#r-2">9.1.4. 决定系数 <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#ch-glm-gof-chi">9.1.5. 广义皮尔逊卡方统计量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#residual-analysis">9.2. 残差分析(Residual analysis)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#response-residuals">9.2.1. Response residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#working-residuals">9.2.2. Working residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#partial-residuals">9.2.3. Partial residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#pearson-residuals">9.2.4. Pearson residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance-residuals">9.2.5. Deviance residuals</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#score-residuals">9.2.6. Score residuals</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#model-selection">9.3. 模型选择(model selection)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#aic">9.3.1. AIC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#bic">9.3.2. BIC</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html">10. 模型检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id2">10.1. 拉格朗日乘子检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id3">10.1.1. 得分统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id4">10.1.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#wald">10.2. wald 检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id5">10.2.1. 参数估计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id6">10.2.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id7">10.3. 似然比检验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id8">10.3.1. 抽样分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id9">10.3.2. 模型比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id10">10.3.3. 偏差统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#f">10.3.4. F 检验</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id11">10.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">11. 高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">11.1. 传统线性回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">11.2. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">11.3. 高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">11.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">11.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">11.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">11.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id10">11.5. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">12. 逆高斯模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">12.1. 逆高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">12.2. 逆高斯回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">12.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">12.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">12.3.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">12.3.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">12.4. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">13. 二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">13.1. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">13.2. 逻辑回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">13.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">13.2.2. 参数估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#odds-logit">13.2.3. odds 与 logit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">13.3. 二项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">13.4. 二项式回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">13.4.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">13.4.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">13.5. 其它连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">13.5.1. 恒等连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#probit">13.5.2. probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#log-log-clog-log">13.5.3. log-log 和 clog-log</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">13.6. 分组数据与比例数据</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html">14. 泊松模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#poisson">14.1. 泊松(Poisson)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id2">14.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id3">14.1.2. 泊松分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id5">14.2. 泊松回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id6">14.3. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id7">14.4. 拟合统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id8">14.5. 频率模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id9">14.6. 泊松模型的局限性</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html">15. 指数模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#exponential">15.1. 指数(exponential)分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id2">15.1.1. 推导过程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id3">15.1.2. 分布的特性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id6">15.2. 指数回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id7">15.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id8">15.3.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id9">15.3.2. 拟合优度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#irls">15.3.3. IRLS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html">16. Gamma 模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id1">16.1. Gamma 函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id2">16.2. Gamma 分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id3">16.3. Gamma 回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id4">16.4. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id5">16.4.1. 似然函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#irls">16.4.2. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id6">16.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id7">16.5. 其他连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id8">16.5.1. 对数 Gamma 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#identity-gamma">16.5.2. 恒等(identity) Gamma 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html">17. 过度分散</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id2">17.1. 什么是过度分散</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id3">17.2. 过度分散的检测</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id4">17.3. 过度分散的影响</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id5">17.4. 标准误差的修正</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">18. 负二项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">18.1. 负二项式分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">18.1.1. 从二项式分布推导</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">18.1.2. 泊松-伽马混合分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#alpha">18.1.3. 辅助参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的影响</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">18.2. 负二项回归模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">18.3. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#irls">18.3.1. IRLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">18.3.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">18.4. 负二项式模型扩展</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">18.4.1. 对数连接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">18.4.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">18.4.3. 几何模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">18.4.4. 广义负二项式模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html">19. 零计数问题</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id2">19.1. 零截断模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id3">19.1.1. 零截断泊松模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id4">19.1.2. 零截断负二项式模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id5">19.2. 零膨胀模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#hurdle">19.2.1. Hurdle 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#zero-inflate">19.2.2. Zero-inflate 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">20. 多项式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">20.1. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#softmax">20.2. softmax 回归模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">20.2.1. 模型定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">20.3. 多项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id6">20.4. 多项式回归模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">21. 有序离散模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">21.1. 有序逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">21.2. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">21.3. 连接函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#logit">21.3.1. logit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#probit">21.3.2. probit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#clog-log">21.3.3. clog-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#log-log">21.3.4. log-log</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#cauchit">21.3.5. cauchit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">21.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html">附录</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id2">标准正态累积分布表</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id3">卡方分布临界值表</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/content.html">参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../probability_model/index_html.html">概率图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html">1. 概率基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id2">1.1. 概率分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id3">1.2. 独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#marginalization">1.3. 边缘化(marginalization)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id6">1.4. 贝叶斯定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id7">1.5. 期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id8">1.6. 常见概率分布</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id9">1.6.1. 离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id10">1.6.2. 连续变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id11">1.6.3. 计数变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id12">1.7. 大数定律</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id13">1.7.1. 独立同分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id14">1.7.2. 中心极限定理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id15">1.8. 信息论基础</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id16">1.8.1. 信息熵</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#kl">1.8.2. KL散度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/1.%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80.html#id18">1.8.3. 互信息</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html">2. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-liklihood">2.1. 极大似然估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id3">2.1.1. 二值离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id4">2.1.2. 一般离散变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-gaussian-ml">2.1.3. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id6">2.1.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-bayesian-estimation">2.2. 贝叶斯估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id8">2.2.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id9">2.2.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id10">2.3. 最大后验估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id11">2.3.1. 伯努利变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id12">2.3.2. 类别变量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id13">2.4. 最大似然估计与贝叶斯估计的对比</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id14">2.5. 统计量和充分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#fisher-information">2.6. Fisher Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id15">2.7. 估计量的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id16">2.7.1. 估计量的方差与偏差</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#id17">2.7.2. 大数定律和中心极限定理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/2.%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.html#ch-2-mle-estimator">2.7.3. 最大似然估计的特性</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html">3. 指数族</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-1">3.1. 指数族的定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id3">3.1.1. 伯努利分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id4">3.1.2. 类别分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id5">3.1.3. 泊松分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id6">3.1.4. 高斯分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id7">3.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#ch-24-moments">3.2. 指数族的期望与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#id9">3.3. 最大似然估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/18.%E6%8C%87%E6%95%B0%E6%97%8F_24.html#kl">3.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/19.%E5%A4%9A%E7%BB%B4%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83_26.html">4. 多维高斯分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html">5. 有向图(Directed Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id1">5.1. 有向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id2">5.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/4.%E6%9C%89%E5%90%91%E5%9B%BE_lecture_2.html#id3">5.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html">6. 无向图(Undirected Graphical Models)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id1">6.1. 无向图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id2">6.2. 条件独立性</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id3">6.3. 图的分解</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#vs">6.4. 有向图 vs 无向图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id4">6.5. 树</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/5.%E6%97%A0%E5%90%91%E5%9B%BE_lecture_3.html#id5">6.6. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html">7. 因子图</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id2">7.1. 因子图的定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id3">7.2. 图模型之间的转换</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id4">7.2.1. 转换为因子图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id5">7.2.2. 因子图转换为有向图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#id6">7.3. 图模型的评价</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#i-map">7.3.1. I-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#d-map">7.3.2. D-map</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/6.%E5%9B%A0%E5%AD%90%E5%9B%BE_lecture_4.html#p-map">7.3.3. P-map</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html">8. 模型推断：消元法</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id2">8.1. 什么是模型的推断</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id3">8.2. 消元法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id4">8.2.1. 有向图消元算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#ch-condition-margin">8.2.2. 条件概率和边缘概率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id6">8.2.3. 无向图的消元法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id7">8.3. 图消除</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/8.%E6%B6%88%E5%85%83%E6%B3%95.html#id9">8.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html">9. 加和乘积算法(sum-product algorithm)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id1">9.1. 树结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id2">9.2. 从消元法到信息传播</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id3">9.3. 树模型的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id4">9.4. 因子图的和积算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id5">9.5. 类树结构图模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#polytrees">9.6. 多重树(polytrees)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/9.%E5%92%8C%E7%A7%AF%E7%AE%97%E6%B3%95_lecture_8.html#id6">9.7. 总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html">10. 最大后验估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id2">10.1. 最大后验概率</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id3">10.2. 最大化后验的状态</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/11.%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1_lecture_11.html#id4">10.3. 本章总结</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html">11. 完整观测的参数学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id2">11.1. 有向图的参数学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id3">11.2. 无向图的参数学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id4">11.2.1. 成对二值变量模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/12.%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_20.html#id5">11.2.2. 一般二值变量模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html">12. 不完整观测的学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#id2">12.1. 隐变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/13.%E4%B8%8D%E5%AE%8C%E6%95%B4%E8%A7%82%E6%B5%8B%E7%9A%84%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0_lecture_22.html#em">12.2. 期望最大化算法(EM)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/14.%E5%9B%BE%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E5%AD%A6%E4%B9%A0_lecture_23.html">13. 有向图结构学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/16.%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD_lecture_17.html">14. 变分推断</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html">15. 马尔科夫蒙特卡洛</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#why-sampling">15.1. Why sampling？</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#monte-carlo">15.2. 蒙特卡罗(Monte Carlo)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain">15.3. 马尔科夫链(Markov Chain)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id2">15.3.1. 一个例子</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#time-reversibility">15.3.2. 时间可逆性(Time Reversibility)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id3">15.3.3. 总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#markov-chain-monte-carlo">15.4. Markov Chain Monte Carlo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#metropolis-hastings">15.4.1. Metropolis-Hastings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id4">15.4.2. 例子：正态分布的采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#id5">15.4.3. 多变量采样</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#gibbs">15.4.4. Gibbs 采样</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#mixing-time">15.5. Mixing Time</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/17.%E9%87%87%E6%A0%B7%E6%B3%95_lecture_18.html#approximate-map-and-partitioning">15.6. Approximate MAP and Partitioning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html">16. 贝叶斯分类器</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id2">16.1. 朴素贝叶斯模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id3">16.1.1. 模型表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id4">16.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id5">16.2. 高斯判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id6">16.2.1. 一元高斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id7">16.2.2. 多元高斯模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id8">16.3. 逻辑回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id9">16.4. 生成模型和判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id10">16.5. 多分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html#id11">16.6. 其它扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html">17. 回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id2">17.1. 机器学习的概率解释</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id3">17.2. 经典线性回归</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id4">17.2.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id5">17.3. 线性回归的概率解释</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id6">17.3.1. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id7">17.4. 凸函数最优化问题</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/21.%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_29.html#id8">17.5. 岭回归</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html">18. 分类模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id2">18.1. 生成模型与判别模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id3">18.2. 线性回归与线性分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id4">18.3. 生成模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id5">18.3.1. 高斯判别模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id6">18.3.2. 朴素贝叶斯模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id7">18.3.3. 指数族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id8">18.4. 判别模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id9">18.4.1. 逻辑回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id10">18.4.2. 多分类</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id11">18.4.3. 最大熵模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#probit">18.4.4. Probit 回归</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#noisy-or">18.4.5. Noisy-OR 模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/22.%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB_32.html#id12">18.4.6. 其它指数模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html">19. 广义线性模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id2">19.1. 定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id3">19.1.1. 指数族分布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id4">19.1.2. 链接函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id5">19.1.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id6">19.2. 参数估计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id7">19.2.1. 梯度下降法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id8">19.2.2. 牛顿法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#irls">19.2.3. 迭代重加权最小二乘(IRLS)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#goodness-of-fit">19.3. goodness of fit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id9">19.4. 连续值响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id10">19.4.1. 高斯族</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#gamma">19.4.2. Gamma族</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id12">19.5. 二项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id13">19.6. 多项响应模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id14">19.7. 计数响应模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id15">19.7.1. 泊松分布</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/25.%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_34.html#id16">19.8. GLM扩展</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html">20. 混合模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id2">20.1. 一般混合模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id3">20.1.1. 模型的有向图表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id5">20.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id8">20.2. 高斯混合模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id9">20.2.1. 模型的表示</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#id10">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/31.%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B_41.html#k-means">20.3. K-means</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/32.%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90_42.html">21. 因子分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E4%BA%8C%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B.html">22. 二变量模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/33.LDA_43.html">23. 主题模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/33.LDA_43.html#plsa">23.1. PLSA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/33.LDA_43.html#lda">23.2. LDA</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html">24. 隐马尔可夫模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id2">24.1. 隐马尔可夫模型</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id3">24.1.1. 马尔可夫模型和朴素贝叶斯模型的关系</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/26.%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB_36.html#id4">24.2. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/27.%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA_37.html">25. 条件随机场</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/28.%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8_38.html">26. 卡尔曼滤波器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/40.%E9%A1%B9%E7%9B%AE%E5%8F%8D%E5%BA%94%E7%90%86%E8%AE%BA_50.html">27. 项目反应理论</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/41.%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA_51.html">28. 贝叶斯知识追踪</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html">29. 变分自编码器（Variational Autoencoder）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#evidence-lower-bound-elbo">29.1. 证据下界(Evidence Lower Bound,ELBO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id2">29.2. 编码-解码</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id3">29.3. 总结</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#em">29.3.1. 和EM算法的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#variational">29.3.2. 为什么叫变分（variational）？</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id4">29.4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html">30. 扩散概率模型（diffusion probabilistic models）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#diffusion-probabilistic-model">30.1. 扩散概率模型（diffusion probabilistic model）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#markovian-hierarchical-variational-autoencoder-mhvae">30.1.1. 马尔科夫分层自编码器（Markovian Hierarchical Variational Autoencoder,MHVAE)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id6">30.1.2. 扩散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id8">30.1.3. 前向-后向</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#elbo">30.1.4. ELBO</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#denoising-diffusion-probabilistic-model-ddpm">30.2. 降噪扩散概率模型（Denoising diffusion probabilistic model,DDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#guidance">30.3. Guidance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#latent-diffusion-model-ldm">30.4. 潜在扩散模型（Latent diffusion model,LDM）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#stable-diffusion-probabilistic-model-sdm">30.4.1. 稳定扩散模型（Stable diffusion probabilistic model,SDM）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id23">30.5. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE.html">31. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../probability_model/aigc_index.html">AI内容生成（ai-gc）</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html">29. 变分自编码器（Variational Autoencoder）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#evidence-lower-bound-elbo">29.1. 证据下界(Evidence Lower Bound,ELBO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id2">29.2. 编码-解码</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id3">29.3. 总结</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#em">29.3.1. 和EM算法的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#variational">29.3.2. 为什么叫变分（variational）？</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8.html#id4">29.4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html">30. 扩散概率模型（diffusion probabilistic models）</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#diffusion-probabilistic-model">30.1. 扩散概率模型（diffusion probabilistic model）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#markovian-hierarchical-variational-autoencoder-mhvae">30.1.1. 马尔科夫分层自编码器（Markovian Hierarchical Variational Autoencoder,MHVAE)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id6">30.1.2. 扩散模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id8">30.1.3. 前向-后向</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#elbo">30.1.4. ELBO</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#denoising-diffusion-probabilistic-model-ddpm">30.2. 降噪扩散概率模型（Denoising diffusion probabilistic model,DDPM）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#guidance">30.3. Guidance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#latent-diffusion-model-ldm">30.4. 潜在扩散模型（Latent diffusion model,LDM）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#stable-diffusion-probabilistic-model-sdm">30.4.1. 稳定扩散模型（Stable diffusion probabilistic model,SDM）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../probability_model/%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B.html#id23">30.5. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../audio/index.html">语音技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../audio/feature.html">1. 音频特征</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id2">1.1. 认识声音</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id3">1.2. 认识声波</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id4">1.2.1. 物体的振动以及简谐振动</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id7">1.2.2. 什么是声波</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id8">1.2.3. 纯音和复合音</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#spectrum">1.2.4. 频谱 Spectrum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id10">1.2.5. 名词</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id13">1.3. 语音学</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id14">1.3.1. 发声原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id15">1.3.2. 听觉感应</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id16">1.4. 数字信号处理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id17">1.4.1. 模数转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#wav">1.4.2. 音频文件–WAV</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id18">1.5. 分帧与加窗</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id19">1.5.1. 预加重处理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id20">1.5.2. 分帧与加窗处理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id21">1.6. 声音的感官度量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#sound-pressure-level-spl">1.6.1. 声压与声压级(Sound Pressure Level,SPL)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#intensity-level-il">1.6.2. 声强与声强级(Intensity Level,IL）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id22">1.6.3. 声压与声强的关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id23">1.6.4. 响度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id24">1.6.5. 音量计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id25">1.6.6. 频率与音高</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id26">1.7. 时域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id27">1.7.1. 短时能量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id28">1.7.2. 短时平均幅度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id29">1.7.3. 短时过零率</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id31">1.8. 频域分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#spectrum-spectrogram">1.8.1. 声谱(spectrum)和时频谱(spectrogram)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#short-time-fourier-transform-stft">1.8.2. 短时傅里叶变换 Short-time Fourier transform (STFT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id33">1.8.3. 倒频谱</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id34">1.8.4. 色谱图</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id35">1.9. 小波域特征</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id36">1.9.1. 离散小波域变换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id37">1.9.2. 小波域过零率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id38">1.9.3. 小波域质心</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../audio/feature.html#id39">1.9.4. 小波域子带能量</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#mfcc">1.10. 语音识别的音频特征–MFCC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../audio/feature.html#id40">1.11. 参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../edm/index.html">教育领域数据挖掘</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../edm/bkt.html">1. 贝叶斯知识追踪(Bayesian Knowledge Tracing,BKT)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#id1">1.1. 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#hidden-markov-model-hmm">1.2. 隐马尔科夫模型(Hidden Markov Model,HMM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#bayesian-knowledge-tracing">1.3. 贝叶斯知识追踪(Bayesian Knowledge Tracing)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#bkt">1.3.1. BKT的参数估计</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#item-response-theory-irt">1.4. 项目反映理论(Item Response Theory,IRT)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#bktirt">1.5. BKT结合IRT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#id5">1.6. 实验</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id6">1.6.1. 数据集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id7">1.6.2. 实验方法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id8">1.6.3. 实验结果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id9">1.6.4. 项目代码</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#id10">1.7. 未来工作</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id11">1.7.1. 题目难度的计算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#irt">1.7.2. 多参数IRT模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../edm/bkt.html#id12">1.7.3. 参数估计算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../edm/bkt.html#id13">1.8. 参考文献</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../nlp/index.html">自然语言处理</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html">1. 文本去重</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id2">1.1. 背景</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id3">1.2. 技术思路</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id4">1.3. 相似（距离）算法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#euclidean-distance">1.3.1. 欧氏距离（Euclidean Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minkowski-distance">1.3.2. 闵科夫斯基距离（Minkowski Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#manhattan-distance">1.3.3. 曼哈顿距离（Manhattan Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#chebyshev-distance">1.3.4. 切比雪夫距离（Chebyshev Distance ）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#mahalanobis-distance">1.3.5. 马氏距离(Mahalanobis Distance)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#cosine-similarity">1.3.6. 余弦夹角相似度(Cosine Similarity)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#hamming-distance">1.3.7. 汉明距离（Hamming Distance）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#jaccard">1.3.8. Jaccard 系数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id5">1.3.9. 编辑距离</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id6">1.3.10. 最长公共字串</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id7">1.3.11. 最长公共子序列</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id8">1.4. 文本去重</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#kshingle">1.4.1. KShingle算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#minhash">1.4.2. Minhash算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#simhash">1.4.3. simhash</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#ksentence">1.4.4. KSentence算法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/%E6%96%87%E6%9C%AC%E5%8E%BB%E9%87%8D/content.html#id9">1.5. 话术去重</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nlp/bert/content.html">2. Attention&amp;Transformer&amp;Bert 简介</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/bert/content.html#transformer">2.1. Transformer 从宏观到微观</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#seq2seq">2.1.1. seq2seq</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#id1">2.1.2. 模型的输入</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/bert/content.html#self-attention">2.2. Self-Attention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#id2">2.2.1. 什么是注意力？</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#id3">2.2.2. 加权求和</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#id4">2.2.3. 位置编码</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../nlp/bert/content.html#multi-head">2.2.4. 多头注意力（Multi-head）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/bert/content.html#attention">2.3. Attention 机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nlp/bert/content.html#id5">2.4. 其它参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../rst_tutorial/latex.html">latex demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../rst_tutorial/latex.html#latex">latex</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rst_tutorial/latex.html#how-to-write-an-m-x-n-matrix-in-latex">How to write an m x n matrix in LaTeX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-big-parentheses">With big parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-parentheses">With parentheses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-brackets">With brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#latex-matrix-with-no-bracket">LateX matrix with no bracket</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-vertical-bar-brackets">With vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-curly-brackets">with curly brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#with-double-vertical-bar-brackets">with double vertical bar brackets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#small-inline-matrix">small inline matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rst_tutorial/latex.html#examples-matrix-2-x-2-in-latex">Examples matrix 2 x 2 in LaTeX</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../rst_tutorial/graphviz.html">graphviz demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../rst_tutorial/graphviz.html#id1">布局</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rst_tutorial/graphviz.html#id2">其它</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">读书笔记</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">《统计因果推断推理入门》读书笔记</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html">1. 第三章 干预的效果</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id2">1.1. 第3.1节 干预</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id3">1.2. 第3.2节 校正公式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD%E6%8E%A8%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E7%AC%AC%E4%B8%89%E7%AB%A0.html#id8">1.3. 第3.3节 后门准则</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html">《深度学习推荐系统》读书笔记</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id2">重点</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id3">冷启动</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id4">探索与利用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#id5">召回层的主要策略</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/content.html#embedding">协同过滤 &amp; Embedding 向量</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">张振虎的博客</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">广义线性模型</a> &raquo;</li>
      <li><span class="section-number">1. </span>概率基础</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/glm/source/概率基础/content.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1><span class="section-number">1. </span>概率基础<a class="headerlink" href="#id1" title="永久链接至标题"></a></h1>
<p>广义线性模型的理论大量依赖概率论的知识，因此本章先回顾一下概率论的一些基础知识。
为了帮助非数学专业的读者更容易理解和入门，
本章乃至本书都是采用大白话的方式进行讲解，
并不追求严谨的学术定义，所以一些描述可能并不严谨。</p>
<section id="id2">
<h2><span class="section-number">1.1. </span>概率模型<a class="headerlink" href="#id2" title="永久链接至标题"></a></h2>
<p>在日常生活中，经常会遇到某些”事情”的结果是不确定的，
比如投掷一枚硬币，其结果可能正面朝上，也可能反面朝上，更有可能是立着。
一般来说，如果一件”事情”的结果是不确定的，那么就意味着这件”事情”多个可能的结果。
反过来，如果一件”事情”只有一种结果，那么这个结果的发生就是必然的，这样的”事情”的结果就是确定性的。
通常可以把”事情”的结果具有不确定性的现象，称为 <strong>随机现象</strong>。
比如投硬币、掷骰子等。</p>
<p>一个具有不确定性的”事情”，其结果的发生具有随机性。
那么每种结果发生的”可能性”是多少呢？能否具体的量化出来呢？
如果可以把每种结果的可能性量化出来，就可以帮助我们对结果进行预判。
最典型的例子就是赌博，投掷一枚骰子的结果是随机的，如果能清楚的知道每个点数的可能性的大小，
就可以一直押注最大可能性的点数，这样就稳赚不赔了。</p>
<section id="id3">
<h3><span class="section-number">1.1.1. </span>概率律<a class="headerlink" href="#id3" title="永久链接至标题"></a></h3>
<p><strong>概率模型</strong> 就是对不确定现象的数学描述，每种可能结果发生的可能性的量化结果就是 <strong>概率律</strong>。
比如正常的投掷一枚正常的硬币，其结果是正面向上的概率是 <span class="math notranslate nohighlight">\(0.5\)</span> ，反面向上的概率也是 <span class="math notranslate nohighlight">\(0.5\)</span>
，至于立起来的结果，我们认为其几乎不可能发生，因此立起来的概率是 <span class="math notranslate nohighlight">\(0\)</span> 。
概率值越大，意味其发生的可能性越大。</p>
<p>我们可以把每一个概率模型都关联着一个 <em>试验</em>，
试验的所有可能结果和这个概率模型的所有结果一一对应，该试验的所有可能结果就构成 <strong>样本空间</strong>，
用 <span class="math notranslate nohighlight">\(\Omega\)</span> 表示样本空间，样本空间的子集称之为 <strong>随机事件</strong>，通常用大写的字母表示随机事件。</p>
<p>我们以掷骰子为例，一个六面体的骰子，把投掷骰子的行为定义为试验，投掷的结果有六种可能，
这六种结果就构成了样本空间 <span class="math notranslate nohighlight">\(\Omega\)</span> ，空间 <span class="math notranslate nohighlight">\(\Omega\)</span> 中有六个样本点，
点数为 <span class="math notranslate nohighlight">\(1\)</span> 的样本点(结果)就是一个 <em>随机事件</em>，
同样点数为 <span class="math notranslate nohighlight">\(2\)</span> 的样本点(结果)也是一个 <em>随机事件</em>。
以此类推，样本空间 <span class="math notranslate nohighlight">\(\Omega\)</span> 中的每一个样本点都可以看做是一个随机事件，
随机事件的结果可以是发生，也可以是不发生。</p>
<p><strong>随机事件</strong> 是一个随机试验的样本空间的 <strong>子集</strong> ，
注意，这里是子集，而不是单个样本点。
子集是样本点的集合，可以包含多个样本点。
比如掷骰子的试验，其样本空间为 <span class="math notranslate nohighlight">\(\Omega=\{1,2,3,4,5,6\}\)</span>
，样本空集的一个子集 <span class="math notranslate nohighlight">\(A=\{1,3,5\}\)</span> ，
可以描述成”结果为单数的随机事件”，
与之对应的另一个随机事件就是”结果为偶数”
。再比如，可以定义一个 “结果为 <span class="math notranslate nohighlight">\(1\)</span> 或 <span class="math notranslate nohighlight">\(2\)</span> ”
的随机事件。
原则上对于随机事件的定义（子集的划分）没有限制，
但是，<strong>一个样本空间的多个随机事件必须是互斥的</strong>
。像”结果是 <span class="math notranslate nohighlight">\(1\)</span> 或者 <span class="math notranslate nohighlight">\(3\)</span> “与”结果是 <span class="math notranslate nohighlight">\(1\)</span> 或者 <span class="math notranslate nohighlight">\(4\)</span> “，
这样两个事件是不允许的，
因为它们两个存在交集 <span class="math notranslate nohighlight">\(1\)</span> 。
并且，<strong>一个样本空间中所有事件的并集，是这个样本空间全部的样本点</strong>。</p>
<p><strong>概率</strong> 是对一个随机事件发生的可能性的量化结果，
对概率最直观的理解是 <strong>频率</strong> 。
<strong>在重复进行多次互不影响试验的结果中，事件发生的频率就可以看做是这个事件发生的概率。</strong>
随机事件 <span class="math notranslate nohighlight">\(A\)</span> 的概率记作 <span class="math notranslate nohighlight">\(P(A)\)</span> 。</p>
<p>比如投掷骰子的试验，假设重复进行 <span class="math notranslate nohighlight">\(N\)</span> 次，点数为 <span class="math notranslate nohighlight">\(1\)</span> 的事件发生了 <span class="math notranslate nohighlight">\(n_1\)</span> 次
，点数为 <span class="math notranslate nohighlight">\(2\)</span> 的事件发生了 <span class="math notranslate nohighlight">\(n_2\)</span> 次，
以此类推，点数为 <span class="math notranslate nohighlight">\(i\)</span> 的事件发生了 <span class="math notranslate nohighlight">\(n_i\)</span> 次。
则有，<span class="math notranslate nohighlight">\(N=n_1+n_2+n_3+n_4+n_5+n_6\)</span> 。
其中每个点数发生的频次(数)是 <span class="math notranslate nohighlight">\(n_i\)</span>
，发生的 <strong>频率</strong> 是 <span class="math notranslate nohighlight">\(\frac{n_i}{N}\)</span> 。
则点数为 <span class="math notranslate nohighlight">\(1\)</span> 的事件发生概率为 <span class="math notranslate nohighlight">\(P(1)=\frac{n_1}{N}\)</span>
，同理，点数为 <span class="math notranslate nohighlight">\(i\)</span> 的事件发生概率为 <span class="math notranslate nohighlight">\(P(i)=\frac{n_i}{N}\)</span> 。</p>
<p>按照 <strong>概率=频率</strong> 的定义，
概率自然也符合频率的一些特性。
比如频率一定是正数，并且频率是大于等于 <span class="math notranslate nohighlight">\(0\)</span> 小于等于 <span class="math notranslate nohighlight">\(1\)</span> 的，
并且，同一个样本空间中，所有随机事件发生的频率之和为 <span class="math notranslate nohighlight">\(1\)</span>
，这是因为所有事件发生的频次相加就等于试验总次数。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-0">
<span class="eqno">(1.1.1)<a class="headerlink" href="#equation-glm-source-content-0" title="公式的永久链接"></a></span>\[\frac{n_1}{N} + \dots + \frac{n_i}{N} = \frac{N}{N} = 1\]</div>
<p>假定我们已经确定了样本空间 <span class="math notranslate nohighlight">\(\Omega\)</span> 以及与之关联的试验，
<strong>概率律</strong> 确定了任何结果或者任何结果的集合（称为随机事件）的似然（可能性）程度。
更精确一点的说，它给每一个事件 <span class="math notranslate nohighlight">\(A\)</span> ，确定一个数 <span class="math notranslate nohighlight">\(P(A)\)</span>，
称为事件 <span class="math notranslate nohighlight">\(A\)</span> 的概率，
概率律需要满足下面几条公理。</p>
<dl class="glossary">
<dt id="term-0">概率公理<a class="headerlink" href="#term-0" title="Permalink to this term"></a></dt><dd><ul>
<li><p><strong>非负性</strong>。 对一切事件 <span class="math notranslate nohighlight">\(A\)</span> ，满足 <span class="math notranslate nohighlight">\(P(A) \ge 0\)</span></p></li>
<li><p><strong>可加性</strong>。 设 <span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span> 是两个互不相容的事件，则它们的并满足：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-1">
<span class="eqno">(1.1.2)<a class="headerlink" href="#equation-glm-source-content-1" title="公式的永久链接"></a></span>\[P(A \cup B) = P(A) + P(B)\]</div>
<p>更一般地，若 <span class="math notranslate nohighlight">\(A_1,A_2,\cdots\)</span> 是互不相容的事件序列，则它们的并满足</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-2">
<span class="eqno">(1.1.3)<a class="headerlink" href="#equation-glm-source-content-2" title="公式的永久链接"></a></span>\[P(A_1 \cup A_2 \cup \cdots) = P(A_1) + P(A_2) + \cdots\]</div>
</li>
<li><p><strong>归一化</strong>。 整个样本空间 <span class="math notranslate nohighlight">\(\Omega\)</span> (称为必然事件)的概率为 <span class="math notranslate nohighlight">\(1\)</span>，
即 <span class="math notranslate nohighlight">\(P(\Omega) = 1\)</span> 。</p></li>
</ul>
</dd>
</dl>
</section>
<section id="id4">
<h3><span class="section-number">1.1.2. </span>离散模型<a class="headerlink" href="#id4" title="永久链接至标题"></a></h3>
<p><strong>当样本空间由有限个样本点组成时，称之为离散模型</strong>。
假设一个离散样本空间 <span class="math notranslate nohighlight">\(\Omega=\{\omega_1,\omega_2,\cdots,\omega_N \}\)</span>
，其含有 <span class="math notranslate nohighlight">\(N\)</span> 个样本点，<span class="math notranslate nohighlight">\(N\)</span> 是有限的，
定义在这个样本空间的随机事件集合为 <span class="math notranslate nohighlight">\(S=\{s_1,s_2,\cdots,s_N \}\)</span>。
假设随机事件集合 <span class="math notranslate nohighlight">\(S\)</span> 和样本空间 <span class="math notranslate nohighlight">\(\Omega\)</span> 是一一对应的，
即事件 <span class="math notranslate nohighlight">\(s_i\)</span> 表示实验结果是样本点 <span class="math notranslate nohighlight">\(\omega_i\)</span> 。
则事件 <span class="math notranslate nohighlight">\(s_i\)</span> 发生的概率为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-3">
<span class="eqno">(1.1.4)<a class="headerlink" href="#equation-glm-source-content-3" title="公式的永久链接"></a></span>\[P(s_i) = \frac{\text{试验结果中}\omega_i \text{的次数} }{\text{试验的总次数}}\]</div>
<p>并且，事件 <span class="math notranslate nohighlight">\(\{s_1,s_2,\cdots,s_N\}\)</span> 的概率是 <span class="math notranslate nohighlight">\(P(s_i)\)</span> 之和。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-4">
<span class="eqno">(1.1.5)<a class="headerlink" href="#equation-glm-source-content-4" title="公式的永久链接"></a></span>\[P(s_1,s_2,\cdots,s_N) = P(s_1) + P(s_2)+\cdots + P(s_N)\]</div>
</section>
<section id="id5">
<h3><span class="section-number">1.1.3. </span>连续模型<a class="headerlink" href="#id5" title="永久链接至标题"></a></h3>
<p><strong>若样本空间是一个连续值集合，称之为连续模型，此时样本点的数量是无限的</strong>。
连续值模型和离散模型有很大的不同，
在连续值模型中，由于样本点的数量是无限的，
如果单个样本点的概率为正数，则所有样本点的概率之和将无穷大，这显然是不行的。
因此我们将连续值模型中，单个样本点的概率定义为 <span class="math notranslate nohighlight">\(0\)</span>
。那要如何表示连续值模型的概率呢？</p>
<p>连续值模型的样本空间是一段连续值的区间，我们可以把这个区间分给成一份一份的，
然后定义每一份的概率就是这一份的长度和整个区间长度的比值。
比如，在赌场中有一种幸运大转盘的赌具，假设这个圆盘被分割成 <span class="math notranslate nohighlight">\(12\)</span> 个扇形，
转动圆盘，当圆盘停止时，指针指向哪个区域，就表示这个区域所代表的事件发生了。
如果圆盘是被等分成 <span class="math notranslate nohighlight">\(12\)</span> 份，则指针落在每个区域的概率都是 <span class="math notranslate nohighlight">\(1/12\)</span>
。</p>
<p>显然，如果样本空间是一个一维空间，则可以划分成一个个的线段，
每个线段可以代表一个事件，事件的发生概率就是线段长度和样本空间总长度的比值。
如果样本空间是一个二维平面空间，则可以划分成一个个子平面，
每个子平面代表一个事件，事件的发生概率就是子平面的面积和样本空间总面积的比值。
以此可以类推更高维的空间。</p>
<p><strong>连续概率模型的计算，就是把整个样本空间分割成子区间，每个子区间的概率值就是这个子区间和整个样本空间的比值</strong>。
显然通过这样的定义得到的概率律，也是符合概率的三个公理的。
本质上就是把连续值区间离散化了。</p>
</section>
</section>
<section id="id6">
<h2><span class="section-number">1.2. </span>条件概率<a class="headerlink" href="#id6" title="永久链接至标题"></a></h2>
<p>条件概率是给定 <em>部分信息</em> 的基础上对实验结果的一种推断。例如
在连续两次抛掷骰子的试验中，已知两次抛掷的点数的总和为 <span class="math notranslate nohighlight">\(9\)</span>,第一次抛掷的点数为 <span class="math notranslate nohighlight">\(6\)</span> 的可能性有多大。
换句话说，假设我们已经知道给定的事件 <span class="math notranslate nohighlight">\(B\)</span> 发生了，而希望知道令一个给定事件 <span class="math notranslate nohighlight">\(A\)</span> 发生的可能性。
此时，我们需要构建一个新的概率律，它顾及了事件 <span class="math notranslate nohighlight">\(B\)</span> 已经发生的信息，求出任何事件 <span class="math notranslate nohighlight">\(A\)</span> 发生的概率。
这个概率就是给定 <span class="math notranslate nohighlight">\(B\)</span> 发生之后事件 <span class="math notranslate nohighlight">\(A\)</span> 的 <strong>条件概率</strong> ，
记作 <span class="math notranslate nohighlight">\(P(A|B)\)</span> ，读作 <span class="math notranslate nohighlight">\(B\)</span> 的条件下 <span class="math notranslate nohighlight">\(A\)</span> 的概率。
当然，条件概率也必须符合三条概率公理。
我们用实际的例子来说明条件概率。</p>
<div class="topic">
<p class="topic-title">例1：箱子里取球</p>
<p>假设我们有两个箱子分别为 <span class="math notranslate nohighlight">\(a_1,a_2\)</span> ，箱子中分别装有红色球和白色球。
假设 <span class="math notranslate nohighlight">\(a_1\)</span> 箱子中有 <span class="math notranslate nohighlight">\(4\)</span> 个红色球和 <span class="math notranslate nohighlight">\(6\)</span> 个白色球，
<span class="math notranslate nohighlight">\(a_2\)</span> 箱子中有 <span class="math notranslate nohighlight">\(8\)</span> 个红色球和 <span class="math notranslate nohighlight">\(2\)</span> 个白色球。 另外我们有一个特殊的硬币，
投放后正面向上的概率是 <span class="math notranslate nohighlight">\(0.6\)</span> ，反面向上的概率是 <span class="math notranslate nohighlight">\(0.4\)</span> 。
现在我们进行如下实验：</p>
<blockquote>
<div><ul class="simple">
<li><p>步骤1. 投掷硬币，然后观察硬币的朝向，根据硬币的朝向选择一个箱子。如果正面向上就选择 <span class="math notranslate nohighlight">\(a_1\)</span> 箱子；如果反面朝上，就选择 <span class="math notranslate nohighlight">\(a_2\)</span> 箱子。</p></li>
<li><p>步骤2. 从选出的箱子中随机（不允许刻意挑选）取出一个球，并记录球的颜色。</p></li>
</ul>
</div></blockquote>
</div>
<p>假设投掷硬币的结果组成样本空间 <span class="math notranslate nohighlight">\(\Omega_{\text{币}} = \{\text{正},\text{反}\}\)</span>，
正面向上的结果定义为事件 <span class="math notranslate nohighlight">\(B_{\text{正}}\)</span> ，反面向上的结果定义为事件 <span class="math notranslate nohighlight">\(B_{\text{反}}\)</span> 。
取出球的颜色组成的样本空间为 <span class="math notranslate nohighlight">\(\Omega_{\text{球}} = \{\text{红},\text{白}\}\)</span>，
定义红色的结果为事件 <span class="math notranslate nohighlight">\(A_{\text{红}}\)</span>，
白色的结果为事件 <span class="math notranslate nohighlight">\(A_{\text{白}}\)</span> 。</p>
<ul class="simple">
<li><p>已知事件 <span class="math notranslate nohighlight">\(B_{\text{正}}\)</span> 发生的情况下，事件 <span class="math notranslate nohighlight">\(A_{\text{红}}\)</span> 发生的概率，就是条件概率
<span class="math notranslate nohighlight">\(P(A_{\text{红}}|B_{\text{正}})=4/10=0.4\)</span>。</p></li>
<li><p>已知事件 <span class="math notranslate nohighlight">\(B_{\text{正}}\)</span> 发生的情况下，事件 <span class="math notranslate nohighlight">\(A_{\text{白}}\)</span> 发生的概率，就是条件概率
<span class="math notranslate nohighlight">\(P(A_{\text{白}}|B_{\text{正}})=6/10=0.6\)</span>。</p></li>
<li><p>已知事件 <span class="math notranslate nohighlight">\(B_{\text{反}}\)</span> 发生的情况下，事件 <span class="math notranslate nohighlight">\(A_{\text{红}}\)</span> 发生的概率，就是条件概率
<span class="math notranslate nohighlight">\(P(A_{\text{红}}|B_{\text{反}})=8/10=0.8\)</span>。</p></li>
<li><p>已知事件 <span class="math notranslate nohighlight">\(B_{\text{反}}\)</span> 发生的情况下，事件 <span class="math notranslate nohighlight">\(A_{\text{白}}\)</span> 发生的概率，就是条件概率
<span class="math notranslate nohighlight">\(P(A_{\text{白}}|B_{\text{反}})=2/10=0.2\)</span>。</p></li>
</ul>
<div class="topic">
<p class="topic-title">例2：掷骰子</p>
<p>假设有一个六面的骰子，投掷结果中每个面的概率相同。如果我们已知试验的结果是偶数，即 <span class="math notranslate nohighlight">\(2,4,6\)</span> 这三种结果之一发生，
由于这三个结果发生的可能性是相等的，这样可以得到</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-5">
<span class="eqno">(1.2.1)<a class="headerlink" href="#equation-glm-source-content-5" title="公式的永久链接"></a></span>\[P(\text{试验结果是6}|\text{试验结果是偶数}) = \frac{1}{3}\]</div>
</div>
<p>从这个结果的推导可以看出，对于等概率模型的情况，下面关于条件概率的定义是合适的，即</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-6">
<span class="eqno">(1.2.2)<a class="headerlink" href="#equation-glm-source-content-6" title="公式的永久链接"></a></span>\[P(A|B) = \frac{\text{事件}A \cap B\text{的试验结果数}}{\text{事件B的试验结果数}}\]</div>
<p>将这个结果推广，我们得到下面的条件概率的定义：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-7">
<span class="eqno">(1.2.3)<a class="headerlink" href="#equation-glm-source-content-7" title="公式的永久链接"></a></span>\[P(A|B) = \frac{P(A\cap B)}{P(B)}\]</div>
<p>其中假定 <span class="math notranslate nohighlight">\(P(B)&gt;0\)</span> 。如果 <span class="math notranslate nohighlight">\(P(B)=0\)</span> ，相应的条件概率是没有意义的。
总而言之，<span class="math notranslate nohighlight">\(P(A|B)\)</span> 是事件 <span class="math notranslate nohighlight">\(A \cap B\)</span> 的概率与事件 <span class="math notranslate nohighlight">\(B\)</span> 的概率的比值。</p>
<p>这个式子可以理解成，在事件 <span class="math notranslate nohighlight">\(B\)</span> 发生的结果中，事件 <span class="math notranslate nohighlight">\(A\)</span> 发生的结果数和事件 <span class="math notranslate nohighlight">\(B\)</span> 发生次数的比值。
如下图所示，整个矩形空间 <span class="math notranslate nohighlight">\(S\)</span> 是全部结果集，
两个圆圈分别是事件 <span class="math notranslate nohighlight">\(A\)</span> 和是事件 <span class="math notranslate nohighlight">\(B\)</span> 发生的结果集，
<span class="math notranslate nohighlight">\(A\)</span> 与 <span class="math notranslate nohighlight">\(B\)</span> 的交集部分，就是 <span class="math notranslate nohighlight">\(A\)</span> 与 <span class="math notranslate nohighlight">\(B\)</span> 同时发生的结果集。</p>
<figure class="align-center" id="id24">
<span id="fg-probability-03"></span><a class="reference internal image-reference" href="../../../_images/1.jpg"><img alt="../../../_images/1.jpg" src="../../../_images/1.jpg" style="width: 434.0px; height: 325.5px;" /></a>
<figcaption>
<p><span class="caption-number">图 1.2.1 </span><span class="caption-text"><span class="math notranslate nohighlight">\(P(A) = \frac{A}{S},P(B) = \frac{B}{S},P(A|B) = \frac{A \cap B}{B}= \frac{P(A \cap B)}{P(B)}\)</span></span><a class="headerlink" href="#id24" title="永久链接至图片"></a></p>
</figcaption>
</figure>
<p>条件概率 <span class="math notranslate nohighlight">\(P(A|B)\)</span> 表示在 <span class="math notranslate nohighlight">\(B\)</span> 发生的条件下 <span class="math notranslate nohighlight">\(A\)</span> 发生的概率，
就是 <strong>限定在</strong> <span class="math notranslate nohighlight">\(B\)</span> <strong>的范围内</strong> <span class="math notranslate nohighlight">\(A\)</span> <strong>发生的概率</strong>。
<span class="math notranslate nohighlight">\(B\)</span> 的范围内就是  <span class="math notranslate nohighlight">\(B\)</span> 发生结果内，
<span class="math notranslate nohighlight">\(B\)</span> 的范围内 <span class="math notranslate nohighlight">\(A\)</span> 的结果数是 <span class="math notranslate nohighlight">\(A \cap B\)</span> 结果数，
因此条件概率 <span class="math notranslate nohighlight">\(P(A|B)\)</span> 就等于</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-8">
<span class="eqno">(1.2.4)<a class="headerlink" href="#equation-glm-source-content-8" title="公式的永久链接"></a></span>\[P(A|B) = \frac{A \cap B}{B}\]</div>
<p>注意，这里分母是 <span class="math notranslate nohighlight">\(B\)</span> 而不是 <span class="math notranslate nohighlight">\(S\)</span> ，因为是 <span class="math notranslate nohighlight">\(B\)</span> <strong>的前提下</strong>。
分子分母同时除以 <span class="math notranslate nohighlight">\(S\)</span> 后等价于</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-9">
<span class="eqno">(1.2.5)<a class="headerlink" href="#equation-glm-source-content-9" title="公式的永久链接"></a></span>\[P(A|B) = \frac{(A \cap B)/S}{B/S} = \frac{P(A \cap B)}{P(B)}\]</div>
<div class="topic">
<p class="topic-title">条件概率的性质</p>
<ul>
<li><p>设事件 <span class="math notranslate nohighlight">\(B\)</span> 满足 <span class="math notranslate nohighlight">\(P(B) &gt; 0\)</span> ，则给定 <span class="math notranslate nohighlight">\(B\)</span> 的条件下，事件 <span class="math notranslate nohighlight">\(A\)</span> 的条件概率由下式给出</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-10">
<span class="eqno">(1.2.6)<a class="headerlink" href="#equation-glm-source-content-10" title="公式的永久链接"></a></span>\[P(A|B) = \frac{P(A \cap B)}{P(B)}\]</div>
</li>
<li><p>由于条件概率所关心的事件都是事件 <span class="math notranslate nohighlight">\(B\)</span> 的子事件，可以把条件概率看作是 <span class="math notranslate nohighlight">\(B\)</span> 上的概率律，
即把事件 <span class="math notranslate nohighlight">\(B\)</span> 看作是全空间或者必然事件。</p></li>
<li><p>当试验的 <span class="math notranslate nohighlight">\(\Omega\)</span> 为有限集，并且所有试验结果为等可能的情况下，条件概率可以由下式给出。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-11">
<span class="eqno">(1.2.7)<a class="headerlink" href="#equation-glm-source-content-11" title="公式的永久链接"></a></span>\[P(A|B) = \frac{ \text{事件} A \cap B \text{的试验结果数} }{\text{事件B的试验结果数}}\]</div>
</li>
</ul>
</div>
<p>总结起来就一句话，条件概率就是把试验结果空间缩小到一个更小的空间，其它照旧。</p>
</section>
<section id="id7">
<h2><span class="section-number">1.3. </span>联合概率<a class="headerlink" href="#id7" title="永久链接至标题"></a></h2>
<p>假设两个随机事件 <span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span>，
在已知 <span class="math notranslate nohighlight">\(B\)</span> 发生的条件下 <span class="math notranslate nohighlight">\(A\)</span> 发生的概率是条件概率 <span class="math notranslate nohighlight">\(P(A|B)\)</span>
。那如果不知道 <span class="math notranslate nohighlight">\(B\)</span> 是否发生了呢？
在没有任何已知条件下， <span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span> 同时发生的概率是什么呢？</p>
<p>我们定义，<strong>属于不同样本空间的多个随机事件同时发生的概率为联合概率</strong> ，记作 <span class="math notranslate nohighlight">\(P(A,B)\)</span> 。</p>
<p>观察 <a class="reference internal" href="#fg-probability-03"><span class="std std-numref">图 1.2.1</span></a> ，<span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span> 同时发生的集合就是
<span class="math notranslate nohighlight">\(A \cap B\)</span> ，因此 <span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span> 的联合概率为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-12">
<span class="eqno">(1.3.1)<a class="headerlink" href="#equation-glm-source-content-12" title="公式的永久链接"></a></span>\[P(A,B) = \frac{A\cap B}{S}\]</div>
<p>实际上，联合概率和条件概率之间是存在关系的，二者可以互相转换。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-13">
<span class="eqno">(1.3.2)<a class="headerlink" href="#equation-glm-source-content-13" title="公式的永久链接"></a></span>\[P(A,B) = \frac{A\cap B}{S} =\frac{B}{S} \times \frac{A\cap B}{B} = P(B)P(A|B)\]</div>
<p>我们继续以箱子里取求为例，在上面的例子中，</p>
<ul class="simple">
<li><p>事件 <span class="math notranslate nohighlight">\(B_{\text{正}}\)</span> 与 <span class="math notranslate nohighlight">\(A_{\text{红}}\)</span> 同时发生概率就是 <span class="math notranslate nohighlight">\(P(B_{\text{正}},A_{\text{红}})=0.6\times 0.4=0.24\)</span> 。</p></li>
<li><p>事件 <span class="math notranslate nohighlight">\(B_{\text{正}}\)</span> 与 <span class="math notranslate nohighlight">\(A_{\text{白}}\)</span> 同时发生概率就是 <span class="math notranslate nohighlight">\(P(B_{\text{正}},A_{\text{红}})=0.6\times 0.6=0.36\)</span> 。</p></li>
<li><p>事件 <span class="math notranslate nohighlight">\(B_{\text{反}}\)</span> 与 <span class="math notranslate nohighlight">\(A_{\text{红}}\)</span> 同时发生概率就是 <span class="math notranslate nohighlight">\(P(B_{\text{正}},A_{\text{红}})=0.4\times 0.8=0.32\)</span> 。</p></li>
<li><p>事件 <span class="math notranslate nohighlight">\(B_{\text{反}}\)</span> 与 <span class="math notranslate nohighlight">\(A_{\text{白}}\)</span> 同时发生概率就是 <span class="math notranslate nohighlight">\(P(B_{\text{正}},A_{\text{红}})=0.4\times 0.2=0.08\)</span> 。</p></li>
</ul>
<p>也可以用一个 <span class="math notranslate nohighlight">\(2\times 2\)</span> 的表格来表示。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-14">
<span class="eqno">(1.3.3)<a class="headerlink" href="#equation-glm-source-content-14" title="公式的永久链接"></a></span>\[\begin{split}\begin{array}{|c|c|c|} \hline
 &amp; B_{\text{正}} &amp; B_{\text{反}} \\\hline
 A_{\text{红}} &amp; 0.6 \times 0.4=0.24 &amp; 0.4 \times 0.8=0.32 \\\hline
 A_\text{白} &amp; 0.6 \times 0.6=0.36&amp; 0.4 \times 0.2=0.08 \\\hline
\end{array}\end{split}\]</div>
<p>当随机事件更多的时候，上面的表格就无法表示了，此时可以用如下的表格</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-008">
<span class="eqno">(1.3.4)<a class="headerlink" href="#equation-eq-probability-008" title="公式的永久链接"></a></span>\[\begin{split}\begin{array}{|c|c|c|} \hline
B &amp; A &amp; P(A,B) \\\hline
{\text{正}} &amp; \text{红} &amp; 0.6 \times 0.4=0.24 \\\hline
{\text{正}} &amp; \text{白} &amp; 0.6 \times 0.6=0.36 \\\hline
{\text{反}} &amp; \text{红} &amp; 0.4 \times 0.8=0.32 \\\hline
{\text{反}}&amp; \text{白} &amp; 0.4 \times 0.2=0.08 \\\hline
\end{array}\end{split}\]</div>
<p>可以看到联合概率可以分解成条件概率的乘积，我们可以扩展到更多事件的联合概率。
假设有 <span class="math notranslate nohighlight">\(A,B,C,D\)</span> 四个随机事件，它们组成的联合概率可以分解为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-15">
<span class="eqno">(1.3.5)<a class="headerlink" href="#equation-glm-source-content-15" title="公式的永久链接"></a></span>\[P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)\]</div>
<p>更一般地，假设有 <span class="math notranslate nohighlight">\(A_1,A_2,\cdots,A_N\)</span> 共 <span class="math notranslate nohighlight">\(N\)</span> 个随机事件，
它们的联合概率可以写为</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-022">
<span class="eqno">(1.3.6)<a class="headerlink" href="#equation-eq-probability-022" title="公式的永久链接"></a></span>\[P(A_1,A_2,\cdots,A_N) = P(A_1)P(A_2|A_1)P(A_3|A_1,A_2) \cdots P(A_N|A_1,A_2,\cdots,A_{N-1})\]</div>
<p><a class="reference internal" href="#equation-eq-probability-022">公式(1.3.6)</a> 被称为联合概率的 <strong>链式法则</strong> 。</p>
<p>联合概率就是多个随机事件同时发生的概率，它的计算方法就是按照事件发生的先后顺序拆解成一系列条件概率的乘积。
在 <a class="reference internal" href="#equation-eq-probability-022">公式(1.3.6)</a> 中，事件 <span class="math notranslate nohighlight">\(A_1\)</span> 是第一个发生的事件，它的前面没有其它事件了，
因此 <span class="math notranslate nohighlight">\(A_1\)</span> 的发生概率不是条件概率，而是 <span class="math notranslate nohighlight">\(P(A_1)\)</span> 。
在 <span class="math notranslate nohighlight">\(A_1\)</span> 之后发生的事件就都是以 <span class="math notranslate nohighlight">\(A_1\)</span> 为前置条件了，依次类推，最后一个事件 <span class="math notranslate nohighlight">\(A_N\)</span>
是在其它 <span class="math notranslate nohighlight">\(N-1\)</span> 个事件之后发生的。</p>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p>先发生的事件是后发生的事件的前置条件，你可以把先发生的事件理解成”因”，后发生的事件看作是”果”，
那么条件概率就是一种因果关系。</p>
</div>
<p>最后，联合概率也是一个合格的概率律，也符合概率三公理。</p>
</section>
<section id="id8">
<h2><span class="section-number">1.4. </span>全概率与贝叶斯定理<a class="headerlink" href="#id8" title="永久链接至标题"></a></h2>
<p>上一节我们讲到，联合概率可以按照事件发生的顺序拆解成条件概率的乘积。
如果不按照事件发生的顺序呢，可不可以把顺序反过来呢？
答案是可以的，但是反过来后将会面临一个问题，
最后一个事件 <span class="math notranslate nohighlight">\(A_N\)</span> 的概率 <span class="math notranslate nohighlight">\(P(A_N)\)</span> 是什么?</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-16">
<span class="eqno">(1.4.1)<a class="headerlink" href="#equation-glm-source-content-16" title="公式的永久链接"></a></span>\[P(A_1,A_2,\cdots,A_N) = P(A_N)P(A_{N-1}|A_N)P(A_{N-2}|A_N,A_{N-1}) \cdots P(A_1|A_N,A_{N-1},\cdots,A_2)\]</div>
<p>我们继续以箱子取球为例，
为简化说明，我们重新定义事件 <span class="math notranslate nohighlight">\(B\)</span> 是事件 <span class="math notranslate nohighlight">\(\{ B_{\text{正}},B_{\text{反}} \}\)</span> 的集合，
记作 <span class="math notranslate nohighlight">\(B=\{B_{\text{正}},B_{\text{反}}\}\)</span> 。
同理，事件 <span class="math notranslate nohighlight">\(A\)</span> 是事件 <span class="math notranslate nohighlight">\(\{A_{\text{红}},A_{\text{白}}\}\)</span> 的集合，
记作 <span class="math notranslate nohighlight">\(A=\{A_{\text{红}},A_{\text{白}}\}\)</span> 。</p>
<p>在这个例子中，先投掷硬币，然后根据硬币的朝向决定从哪个箱子里取球。
因此事件 <span class="math notranslate nohighlight">\(B\)</span> 先发生，事件 <span class="math notranslate nohighlight">\(A\)</span> 后发生，
可以把事件 <span class="math notranslate nohighlight">\(B\)</span> 看做”因”，把事件 <span class="math notranslate nohighlight">\(A\)</span> 看做”果”。
根据链式法则，二者的联合概率 <span class="math notranslate nohighlight">\(P(A,B)\)</span> 可以写成</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-025">
<span class="eqno">(1.4.2)<a class="headerlink" href="#equation-eq-probability-025" title="公式的永久链接"></a></span>\[P(A,B) = P(B)P(A|B)\]</div>
<p>如果我们把 <a class="reference internal" href="#equation-eq-probability-025">公式(1.4.2)</a> 中的事件顺序反过来，就是</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-026">
<span class="eqno">(1.4.3)<a class="headerlink" href="#equation-eq-probability-026" title="公式的永久链接"></a></span>\[P(A,B) = P(A)P(B|A)\]</div>
<p>这时就产生了一个问题，<span class="math notranslate nohighlight">\(P(A)\)</span> 是多少？ <span class="math notranslate nohighlight">\(P(B|A)\)</span> 又是多少？
形象一点就是， <span class="math notranslate nohighlight">\(P(A)\)</span> 代表 <span class="math notranslate nohighlight">\(P(\text{果})\)</span> ，
<span class="math notranslate nohighlight">\(P(B|A)\)</span> 代表 <span class="math notranslate nohighlight">\(P(因|果)\)</span> ，从”果”到”因”就是 <strong>推断(inference)问题</strong> 。
本节我们讨论的 <strong>全概率公式</strong> 和 <strong>贝叶斯定理</strong> 就是分别来求得
<span class="math notranslate nohighlight">\(P(A)\)</span> 和  <span class="math notranslate nohighlight">\(P(B|A)\)</span> 的方法，
我们先从 <span class="math notranslate nohighlight">\(P(A)\)</span> 说起。</p>
<p>事件 <span class="math notranslate nohighlight">\(A=\{A_{\text{红}},A_{\text{白}}\}\)</span> 表示球的颜色事件(集合)，球的颜色有红白两种，
取到红球白球的概率会受到硬币事件 <span class="math notranslate nohighlight">\(B=\{B_{\text{正}},B_{\text{反}}\}\)</span> 的影响，
因此可以把事件 <span class="math notranslate nohighlight">\(B\)</span> 看做是”因” ，事件 <span class="math notranslate nohighlight">\(A\)</span> 看作是”果”。
现在我们回顾一下 <span class="math notranslate nohighlight">\(B\)</span> 与 <span class="math notranslate nohighlight">\(A\)</span> 的联合概率表，如下表所示。</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-028">
<span class="eqno">(1.4.4)<a class="headerlink" href="#equation-eq-probability-028" title="公式的永久链接"></a></span>\[\begin{split}\begin{array}{|c|c|c|} \hline
B &amp; A &amp; P(A,B) \\\hline
{\text{正}} &amp; \text{红} &amp; P(\text{正})P(\text{红}|\text{正}) = 0.6 \times 0.4=0.24 \quad \\\hline
{\text{正}} &amp; \text{白} &amp; P(\text{正})P(\text{白}|\text{正}) = 0.6 \times 0.6=0.36  \quad \\\hline
{\text{反}} &amp; \text{红} &amp; P(\text{反})P((\text{红}|\text{反}) =  0.4 \times 0.8=0.32  \quad \\\hline
{\text{反}} &amp; \text{白} &amp; P(\text{反})P((\text{白}|\text{反}) = 0.4 \times 0.2=0.08  \quad \\\hline
\end{array}\end{split}\]</div>
<p>事件 <span class="math notranslate nohighlight">\(A=A_{红}\)</span> 的状态被分割成了两部分，一部分是 <span class="math notranslate nohighlight">\(P(A=\text{红},B=\text{正})\)</span>
，另一部分是 <span class="math notranslate nohighlight">\(P(A=\text{红},B=\text{反})\)</span>
。显然 <span class="math notranslate nohighlight">\(P(A=\text{红})\)</span> 的概率需要把两部分加起来。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-17">
<span class="eqno">(1.4.5)<a class="headerlink" href="#equation-glm-source-content-17" title="公式的永久链接"></a></span>\[P(A=\text{红}) = P(A=\text{红},B=\text{正}) + P(A=\text{红},B=\text{反}) = 0.24+0.32 = 0.56\]</div>
<p>同理，<span class="math notranslate nohighlight">\(P(A=\text{白})\)</span> 为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-18">
<span class="eqno">(1.4.6)<a class="headerlink" href="#equation-glm-source-content-18" title="公式的永久链接"></a></span>\[P(A=\text{白}) = P(A=\text{白},B=\text{正}) + P(A=\text{白},B=\text{反}) = 0.36 + 0.08 = 0.44\]</div>
<p>可以看出，事件 <span class="math notranslate nohighlight">\(A\)</span> 的每个状态都被事件 <span class="math notranslate nohighlight">\(B\)</span> 分割了，分割的份数就是事件  <span class="math notranslate nohighlight">\(B\)</span> 的状态数量，
要想求得事件 <span class="math notranslate nohighlight">\(A\)</span> 每个状态的概率，就需要把被 <span class="math notranslate nohighlight">\(B\)</span> 分割的各个部分加起来才行。
总结起来就是这样</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-19">
<span class="eqno">(1.4.7)<a class="headerlink" href="#equation-glm-source-content-19" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}P(A=a) &amp;= P(B=b_1)P(a|B=b_1) +  P(B=b_2)P(a|B=b_2) + \cdots + P(B=b_n)P(a|B=b_n)\\&amp;= \sum_{i=1}^n P(B=b_i)P(a|B=b_i)\\&amp;= \sum_{B} P(B)P(a|B)\\&amp;= \sum_{B} P(A=a,B)\end{aligned}\end{align} \]</div>
<p>以上就是全概率公式，简单来说 <strong>全概率公式就是联合概率中消除掉一个事件得到剩余事件的概率</strong>。
<strong>消除掉的方法就是对这个事件的各个状态进行求和，如果被消除事件是一个连续值概率模型，就把求和符号换成积分</strong>。</p>
<div class="topic">
<p class="topic-title">全概率定理</p>
<p>设 <span class="math notranslate nohighlight">\(B_1,B_2,\cdots,B_n\)</span> 是一组互不相容的事件，形成样本空间的一个分割（每一个试验结果必定使得其中一个事件发生），
又假定对每一个 <span class="math notranslate nohighlight">\(i\)</span>， <span class="math notranslate nohighlight">\(P(B_i)&gt;0\)</span>，则对于任何事件 <span class="math notranslate nohighlight">\(A\)</span>，下列公式成立。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-20">
<span class="eqno">(1.4.8)<a class="headerlink" href="#equation-glm-source-content-20" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}P(A) &amp;= P(B_1)P(A|B_1)+ P(B_2)P(A|B_2) + \cdots +P(B_n)P(A|B_n)\\&amp;=  P(A,B_1) + P(A,B_2) + \cdots+P(A,B_n)\\&amp;= \sum_{i=1}^N P(A,B_i)\end{aligned}\end{align} \]</div>
</div>
<p>回到最初的问题，我们已经可以通过全概率公式得到”果”的概率 <span class="math notranslate nohighlight">\(P(A)\)</span> ,
现在看下如何从”果”推断出”因”，即 <span class="math notranslate nohighlight">\(P(B|A)\)</span>
。我们可以把 <a class="reference internal" href="#equation-eq-probability-025">公式(1.4.2)</a> 和 <a class="reference internal" href="#equation-eq-probability-026">公式(1.4.3)</a>
放在一起。</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-030">
<span class="eqno">(1.4.9)<a class="headerlink" href="#equation-eq-probability-030" title="公式的永久链接"></a></span>\[P(A,B) = P(A)P(B|A) = P(B)P(A|B)\]</div>
<p>通过移项可得</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-031">
<span class="eqno">(1.4.10)<a class="headerlink" href="#equation-eq-probability-031" title="公式的永久链接"></a></span>\[P(B|A) = \frac{P(B)P(A|B)}{P(A)}\]</div>
<p>其中分子部分 <span class="math notranslate nohighlight">\(P(B)P(A|B)\)</span> 是正向的”因果”关系，我们是已知的，
分母 <span class="math notranslate nohighlight">\(P(A)\)</span> 可以通过全概率公式得到。
二者的比值就得到了 <span class="math notranslate nohighlight">\(P(B|A)\)</span> 。</p>
<p><a class="reference internal" href="#equation-eq-probability-031">公式(1.4.10)</a> 就是 <strong>贝叶斯定理</strong>，
又叫做贝叶斯公式。
贝叶斯定理就是贝叶斯推断的核心，经常被用来做 <strong>因果推断</strong>。
有许多”原因”可以造成某一种”结果”，当已知结果要推断成因时，
就是”因果推断”。所谓推断成因，就是推断出造成这一结果的每种原因的概率是多少。</p>
<p>现在设事件 <span class="math notranslate nohighlight">\(B_1,B_2,\cdots,B_n\)</span> 是原因，而 <span class="math notranslate nohighlight">\(A\)</span> 代表由原因引起的结果。
<span class="math notranslate nohighlight">\(P(A|B_i)\)</span> 表示在因果模型中由”原因” <span class="math notranslate nohighlight">\(B_i\)</span> 造成结果 <span class="math notranslate nohighlight">\(A\)</span> 出现的概率。
当观察到结果 <span class="math notranslate nohighlight">\(A\)</span> 的时候，我们希望反推结果 <span class="math notranslate nohighlight">\(A\)</span> 是由原因 <span class="math notranslate nohighlight">\(B_i\)</span> 造成的概率 <span class="math notranslate nohighlight">\(P(B_i|A)\)</span> 。
<span class="math notranslate nohighlight">\(P(B_i|A)\)</span> 代表新进得到的信息 <span class="math notranslate nohighlight">\(A\)</span> 之后 <span class="math notranslate nohighlight">\(B_i\)</span> 出现的概率，
因此称之为 <strong>后验概率</strong>。
<span class="math notranslate nohighlight">\(P(B_i)\)</span> 是在没有信息之前就知道的 <span class="math notranslate nohighlight">\(B_i\)</span> 出现的概率，
因此称之为 <strong>先验概率</strong>。
<span class="math notranslate nohighlight">\(P(A|B_i)\)</span> 是有了原因 <span class="math notranslate nohighlight">\(B_i\)</span> 之后 <span class="math notranslate nohighlight">\(A\)</span> 出现的可能性
，因此称之为 <strong>似然</strong>。
最后，贝叶斯公式可以用如下方式描述。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-21">
<span class="eqno">(1.4.11)<a class="headerlink" href="#equation-glm-source-content-21" title="公式的永久链接"></a></span>\[\text{后验概率} = \frac{\text{先验概率} \times \text{似然} }{\text{全概率}}\]</div>
<p>此外，可以注意到，分母全概率公式 <span class="math notranslate nohighlight">\(P(A)=\sum_B  P(B)P(A|B)\)</span> 其实就是分子 <span class="math notranslate nohighlight">\(P(B)P(A|B)\)</span>
的累加。</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-033">
<span class="eqno">(1.4.12)<a class="headerlink" href="#equation-eq-probability-033" title="公式的永久链接"></a></span>\[P(B|A) = \frac{P(B)P(A|B)}{P(A)} =  \frac{P(B)P(A|B)}{\sum_B  P(B)P(A|B) }\]</div>
<p>因此作为分母的全概率可以看做是分子的 <strong>归一化项</strong>，
归一化的结果是把等式右侧的数值转换到区间 <span class="math notranslate nohighlight">\([0,1]\)</span>，
使其符合概率的定义。相对于分子来说，分母的值是固定不变的，
贝叶斯公式 <a class="reference internal" href="#equation-eq-probability-033">公式(1.4.12)</a> 可以简写成一个正比关系。</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-035">
<span class="eqno">(1.4.13)<a class="headerlink" href="#equation-eq-probability-035" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}P(B|A) &amp;\propto P(B) P(A|B)\\\text{后延}  &amp;\propto \text{先验} \times \text{似然}\end{aligned}\end{align} \]</div>
</section>
<section id="id9">
<h2><span class="section-number">1.5. </span>独立性<a class="headerlink" href="#id9" title="永久链接至标题"></a></h2>
<p>前面的内容中，我们探讨了多个随机事件的关系，条件概率、联合概率以及因果推断
。那如果两个随机事件没有任何关系呢？
如果随机事件之间没有任何关系，我们称它们是 <strong>相互独立事件</strong>。</p>
<p>如果两个事件 <span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span> 是相互独立的，则事件 <span class="math notranslate nohighlight">\(B\)</span> 的发生不会改变事件
<span class="math notranslate nohighlight">\(A\)</span> 的概率，反之亦然。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-22">
<span class="eqno">(1.5.1)<a class="headerlink" href="#equation-glm-source-content-22" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}P(A) &amp;= P(A|B)\\P(B) &amp;= P(B|A)\end{aligned}\end{align} \]</div>
<p>在上述等式成立的情况下，我们称事件 <span class="math notranslate nohighlight">\(A\)</span> 独立于事件 <span class="math notranslate nohighlight">\(B\)</span>，
记作 <span class="math notranslate nohighlight">\(A \perp \!\!\! \perp  B\)</span> 。
如果两个事件是独立的，则它们的联合概率将变得简单。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-23">
<span class="eqno">(1.5.2)<a class="headerlink" href="#equation-glm-source-content-23" title="公式的永久链接"></a></span>\[P(A,B) = P(A)P(B)\]</div>
<p>有些时候，单独看两个事件可能不是独立的，但是在给定另外一个条件下是独立的。
例如在给定事件 <span class="math notranslate nohighlight">\(C\)</span> 发生的情况下，事件 <span class="math notranslate nohighlight">\(A\)</span> 与 <span class="math notranslate nohighlight">\(B\)</span>
是相互独立的，记作 <span class="math notranslate nohighlight">\(A \perp \!\!\! \perp  B | C\)</span>
，这种情况称之为 <strong>条件独立</strong> 。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-24">
<span class="eqno">(1.5.3)<a class="headerlink" href="#equation-glm-source-content-24" title="公式的永久链接"></a></span>\[P(A,B|C) = P(A|C)P(B|C)\]</div>
<div class="math notranslate nohighlight" id="equation-glm-source-content-25">
<span class="eqno">(1.5.4)<a class="headerlink" href="#equation-glm-source-content-25" title="公式的永久链接"></a></span>\[P(A,B,C) = P(C)P(A,B|C) = P(C) P(A|C)P(B|C)\]</div>
</section>
<section id="id10">
<h2><span class="section-number">1.6. </span>随机变量<a class="headerlink" href="#id10" title="永久链接至标题"></a></h2>
<p>试验的所有可能结果形成了样本空间 <span class="math notranslate nohighlight">\(\Omega=\{\omega_1,\omega_2,\cdots,\omega_n\}\)</span> ，
样本空间中的每个样本点 <span class="math notranslate nohighlight">\(\omega_i\)</span> 就表示试验的一种可能结果，
一个试验的结果必定是样本空间 <span class="math notranslate nohighlight">\(\Omega\)</span> 中的一个。
在许多概率模型中试验结果是数值化的，例如股价或者骰子的点数等等，
也有一些试验结果不是数值化的，例如投硬币的结果。
显然这样既有非数字又有数值的情况，不利于我们的研究和处理，
因此我们希望能全部转换成数值进行处理，
而这可以通过 <strong>随机变量</strong> 来实现。</p>
<p>我们把样本空间中的每一个可能的试验结果，关联一个特定的数，
这种试验结果与数的对应关系形成 <strong>随机变量</strong> 。
更直白的说就是，我们用一个变量符号来表示实验结果，
变量的取值就是试验结果所对应的数。
从数学上将，随机变量是试验结果的实值函数，
随机变量通常用大写的字母表示。
我们用两个例子来说明。</p>
<p>首先以投硬币的试验为例，投硬币的结果形成样本空间 <span class="math notranslate nohighlight">\(\Omega=\{\text{正},\text{反}\}\)</span>
，首先我们需要把非数值的样本映射成数值，比如我们把正面向上的结果映射成数字 <span class="math notranslate nohighlight">\(1\)</span>，
把反面向上的结果映射成数字 <span class="math notranslate nohighlight">\(0\)</span> ，样本空间就变成了一个数值空间 <span class="math notranslate nohighlight">\(\Omega=\{1,0\}\)</span>
。然后定义一个随机变量 <span class="math notranslate nohighlight">\(X\)</span> 来表示试验的结果，那么变量 <span class="math notranslate nohighlight">\(X\)</span> 的取值空间就是数值空间 <span class="math notranslate nohighlight">\(\Omega=\{1,0\}\)</span>
。 <span class="math notranslate nohighlight">\(X=1\)</span> 表示试验结果是正面向上， <span class="math notranslate nohighlight">\(X=0\)</span> 表示试验结果是反面向上。</p>
<p>再比如掷骰子的试验中，试验结果的样本空间就是六种点数，
记作 <span class="math notranslate nohighlight">\(\Omega=\{1,2,3,4,5,6\}\)</span>
。在这个试验中，样本空间已经是数值化了，因此就不需要进行数值转化了，
我用变量 <span class="math notranslate nohighlight">\(X\)</span> 表示试验的结果，则变量 <span class="math notranslate nohighlight">\(X\)</span> 的的取值空间就是 <span class="math notranslate nohighlight">\(\{1,2,3,4,5,6\}\)</span>
，通常用小写的字母表示变量的取值，此时变量 <span class="math notranslate nohighlight">\(X=x\)</span> 就表示试验结果是 <span class="math notranslate nohighlight">\(x\)</span>
。</p>
<p>现在再举几个随机变量的例子。</p>
<ol class="arabic simple">
<li><p>连续抛掷一枚硬币共 <span class="math notranslate nohighlight">\(5\)</span> 次，在这个试验中正面出现的次数是一个随机变量。</p></li>
<li><dl class="simple">
<dt>在两次抛掷一个骰子的试验中，下面的例子是随机变量。</dt><dd><ol class="loweralpha simple">
<li><p>两次抛掷骰子所得到的的点数之和。</p></li>
<li><p>两次抛掷得到 <span class="math notranslate nohighlight">\(6\)</span> 点的次数。</p></li>
<li><p>第二次抛掷所得到的点数的 <span class="math notranslate nohighlight">\(5\)</span> 次方。</p></li>
</ol>
</dd>
</dl>
</li>
<li><p>在传输信号的试验中，传输信号所需的时间、接收到的信号中发生错误的次数、传输信号过程中的时间延迟等都是随机变量。</p></li>
</ol>
<div class="topic">
<p class="topic-title">与随机变量相关的主要概念</p>
<p>在一个试验的概率模型之下：</p>
<ul class="simple">
<li><p><strong>随机变量</strong> 是试验结果的实值函数；</p></li>
<li><p><strong>随机变量的函数</strong> 定义了另一个随机变量；</p></li>
<li><p>对于一个随机变量，我们可以定义一些平均量，例如 <strong>均值</strong> 和 <strong>方差</strong> ；</p></li>
<li><p>可以在某事件或某随机变量的 <strong>条件</strong> 之下定义一个随机变量；</p></li>
<li><p>存在一个随机变量与某事件或者某随机变量相互 <strong>独立</strong> 的概率。</p></li>
</ul>
</div>
<p>随机变量的这些特性当中，比较重要的一点是，<strong>随机变量的函数仍然是一个随机变量</strong> 。
这一点在本书之后的内容中会使用，比如统计量、参数估计量就是建立在这一点之上。</p>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p>如果你难以理解随机变量的概念，没关系，可以暂时先把随机变量就理解成随机事件。虽然这样不是很准确，
但不妨碍对本书之后内容的理解。随机事件拥有的特性随机变量也有，比如条件概率、联合概率、独立性等等，
二者的差别就是，随机事件只有发生、不发生两种结果，而随机变量可以多种结果值，并且随机变量的值是数值（数字）。</p>
</div>
<p>样本空间的大小可以是有限的，也可以是无限的，有限的样本空间是离散概率模型，
无限的样本空间是连续值概率模型。
随机变量是样本空间的实值函数，
因此随机变量也分为 <strong>离散随机变量</strong> 和 <strong>连续随机变量</strong> 。</p>
<section id="id11">
<h3><span class="section-number">1.6.1. </span>离散随机变量<a class="headerlink" href="#id11" title="永久链接至标题"></a></h3>
<p>若一个随机变量的值域（随机变量的取值范围）为一个有限集合或最多为可数无限集合，
则称这个随机变量为 <strong>离散的</strong> 。
由于它只能取有限多个值，所以是离散的随机变量。</p>
<p>离散随机变量，既然称为 <strong>随机</strong> 变量，意味着它的取值并不是确定性，
而是具有 <strong>随机</strong> 性，有可能是值域中的任何一个值，
值域中每个值都有一定的概率。
假设随机变量 <span class="math notranslate nohighlight">\(X\)</span> 表示一次投掷硬币的试验结果，
<span class="math notranslate nohighlight">\(X=1\)</span> 表示正面向上结果，<span class="math notranslate nohighlight">\(X=0\)</span> 表示反面向上的结果，
对于一枚正常的硬币两种结果应该是等概率的，
随机变量 <span class="math notranslate nohighlight">\(X\)</span> 每种取值的概率情况为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-26">
<span class="eqno">(1.6.1)<a class="headerlink" href="#equation-glm-source-content-26" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}P(X=1) &amp;= 0.5\\P(X=0) &amp;= 0.5\end{aligned}\end{align} \]</div>
<p>对于离散随机变量，其值域是有限个，因此有时也可以用一个表格的形式表达其各个值的概率情况</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-27">
<span class="eqno">(1.6.2)<a class="headerlink" href="#equation-glm-source-content-27" title="公式的永久链接"></a></span>\[\begin{split}\begin{array}{|c|c|} \hline
X &amp; P(X) \\\hline
1 &amp; 0.5  \\\hline
0 &amp; 0.5  \\\hline
\end{array}\end{split}\]</div>
<p>我们把一个随机变量各个可能取值的概率分布情况，称为随机变量的 <strong>概率分布</strong>。
虽然我们可以用表格的形式表达离散随机变量的概率分布，但是如果随机变量的值域规模较大，
表格将变得异常庞大，使用起来也是不方便的。
此时，可以用一个 <strong>数学函数</strong> 来表达随机变量的概率分布，
用来表达随机变量的概率分布的函数就称为 <strong>概率分布函数</strong> 。
比如，对于仅有 <span class="math notranslate nohighlight">\(\{0,1\}\)</span> 两个值的随机变量 <span class="math notranslate nohighlight">\(X\)</span>
，假设其为 <span class="math notranslate nohighlight">\(1\)</span> 的概率是 <span class="math notranslate nohighlight">\(\pi\)</span>
，那么它的概率分布函数可以为</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-050">
<span class="eqno">(1.6.3)<a class="headerlink" href="#equation-eq-probability-050" title="公式的永久链接"></a></span>\[P(X) = \pi^x(1-\pi)^{(1-x)}\]</div>
<p>把随机变量的某个可能取值代入到概率分布函数，得到的就是变量为这个值的概率。
我们把 <span class="math notranslate nohighlight">\(1\)</span> 代入到 <a class="reference internal" href="#equation-eq-probability-050">公式(1.6.3)</a> 得到就是随机变量 <span class="math notranslate nohighlight">\(X\)</span>
为 <span class="math notranslate nohighlight">\(1\)</span> 的概率 <span class="math notranslate nohighlight">\(P(X=1)\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-28">
<span class="eqno">(1.6.4)<a class="headerlink" href="#equation-glm-source-content-28" title="公式的永久链接"></a></span>\[P(X=1) = \pi^1(1-\pi)^{(1-1)}= \pi\]</div>
<p>同理，把 <span class="math notranslate nohighlight">\(0\)</span> 带入到 <a class="reference internal" href="#equation-eq-probability-050">公式(1.6.3)</a> 得到就是
<span class="math notranslate nohighlight">\(P(X=0)\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-29">
<span class="eqno">(1.6.5)<a class="headerlink" href="#equation-glm-source-content-29" title="公式的永久链接"></a></span>\[P(X=0) = \pi^0(1-\pi)^{(1-0)}= 1 - \pi\]</div>
<p>前文已经讲过随机变量是随机事件的一个扩展，随机变量的概率分布也是满足概率三公理的，
包括非负性、可加性和归一化。
假设有一个离散随机变量 <span class="math notranslate nohighlight">\(Y\)</span>，
其值域空间为 <span class="math notranslate nohighlight">\(\{y_1,y_2,\cdots,y_n\}\)</span>
，任意一个值 <span class="math notranslate nohighlight">\(y_i\)</span> 的概率记为 <span class="math notranslate nohighlight">\(P(y_i)\)</span>
，则有如下归一化等式成立。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-30">
<span class="eqno">(1.6.6)<a class="headerlink" href="#equation-glm-source-content-30" title="公式的永久链接"></a></span>\[\sum_{i=1}^n P(y_i) = 1\]</div>
</section>
<section id="id12">
<h3><span class="section-number">1.6.2. </span>连续随机变量<a class="headerlink" href="#id12" title="永久链接至标题"></a></h3>
<p>在前文讲过，当样本空间有无限个样本点时就是连续概率模型，
同理，若一个随机变量可以取到无限多个数，
那么这个随机变量就是 <strong>连续值随机变量</strong> 。
在连续概率模型中，单个样本点的概率是 <span class="math notranslate nohighlight">\(0\)</span> ，
我们需要把整个连续的样本区间划分成子区间，让后定义样本点落在每个子区间的概率。
同理，连续值随机变量也是一样的。</p>
<p>假设一个连续值随机变量 <span class="math notranslate nohighlight">\(X\)</span> ，其值域空间是整个实数域 <span class="math notranslate nohighlight">\(X \in \mathcal{R}\)</span>
，它的概率分布函数是 <span class="math notranslate nohighlight">\(f(x)\)</span> 。
随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的取值落在区间 <span class="math notranslate nohighlight">\([a,b]\)</span> 的概率为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-31">
<span class="eqno">(1.6.7)<a class="headerlink" href="#equation-glm-source-content-31" title="公式的永久链接"></a></span>\[P(a \le X \le b) = \int_a^b f(x) dx\]</div>
<p>一定要注意，对于连续值随机变量的概率分布函数 <span class="math notranslate nohighlight">\(f(x)\)</span>
，<span class="math notranslate nohighlight">\(f(x)\)</span> 的值并不是概率值，而是这一点的 <strong>密度值</strong> ，
在一个区间上的积分结果表示随机变量落在这个区间的概率值。</p>
<p>如果我们把这个区间 <span class="math notranslate nohighlight">\([a,b]\)</span> 扩大到 <span class="math notranslate nohighlight">\(X\)</span> 的整个值域空间 <span class="math notranslate nohighlight">\(\mathcal{R}\)</span>
，此时相当于 <span class="math notranslate nohighlight">\(a=-\infty,b=\infty\)</span>，
不管试验结果是什么，<span class="math notranslate nohighlight">\(X\)</span> 的值肯定会落在自己的值域空间，
落在整个值域空间的概率必然是 <span class="math notranslate nohighlight">\(1\)</span> 。
因此有如下约束必须满足</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-32">
<span class="eqno">(1.6.8)<a class="headerlink" href="#equation-glm-source-content-32" title="公式的永久链接"></a></span>\[P(- \infty &lt; X &lt; \infty) = \int_{- \infty}^{\infty} f(x) dx = 1\]</div>
<p>也就是说，我们约束连续值随机变量概率分布函数在整个值域的积分必须是 <span class="math notranslate nohighlight">\(1\)</span>
，这就和离散随机变量值域内所有值的概率之和是 <span class="math notranslate nohighlight">\(1\)</span> 一样。
虽然在这个例子中，连续值随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的值域是整个实数域，但这不是必须的，
连续值随机变量的值域空间可以是任意的区间，一般是根据试验的样本空间确定的，
对值域空间的范围和大小并没有额外的限制。
但不管值域空间是如何的，都是可以通过调整 <span class="math notranslate nohighlight">\(f(x)\)</span>
以使得它在整个空间的积分是 <span class="math notranslate nohighlight">\(1\)</span> 。</p>
<p>离散随机变量的概率分布函数可以直接为每个点计算出概率值（类比于每个点的质量），
因此通常称为 <strong>概率质量函数(probability mass function, pmf)</strong>，
而连续值随机变量的概率分布函数，需要积分才能得到概率值（质量），
函数本身相当于每个点的 <em>密度值</em> ，
因此连续值随机变量的概率分布函数一般称为 <strong>概率密度函数(probability density function, pdf)</strong> 。</p>
</section>
<section id="id13">
<h3><span class="section-number">1.6.3. </span>累积分布函数<a class="headerlink" href="#id13" title="永久链接至标题"></a></h3>
<p>对于离散随机变量和连续随机变量分别用概率质量函数和概率密度函数刻画他们的概率分布情况，
本节我们介绍另一种刻画概率分布的方法，累积分布函数（Cumulative Distribution Function,CDF）
。</p>
<p>累积分布函数是概率质量（密度）函数的积分函数，
通常使用小写字母 <span class="math notranslate nohighlight">\(f\)</span> 代表概率质量（密度）函数和函数，而用大写字母 <span class="math notranslate nohighlight">\(F\)</span> 表示累积分布函数。</p>
<div class="topic">
<p class="topic-title">累计分布函数</p>
<p>假设随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的 <code class="docutils literal notranslate"><span class="pre">CDF</span></code> 是 <span class="math notranslate nohighlight">\(x\)</span> 的函数 <span class="math notranslate nohighlight">\(F_X\)</span>，
对于每一个 <span class="math notranslate nohighlight">\(x\)</span>， <span class="math notranslate nohighlight">\(F_X(x)\)</span> 定义为 <span class="math notranslate nohighlight">\(P(X \le x)\)</span>。
特别地，当 <span class="math notranslate nohighlight">\(X\)</span> 为离散或连续的情况下，</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-33">
<span class="eqno">(1.6.9)<a class="headerlink" href="#equation-glm-source-content-33" title="公式的永久链接"></a></span>\[\begin{split}F_X(x) = P(X \le x) = \left \{
\begin{aligned}
\sum_{x \le x} P_X(k) &amp;, \quad \text{若} X \text{是离散的} \\
\int_{-\infty}^{x} f_X(t) dt &amp;, \quad \text{若} X \text{是连续的}
\end{aligned}
\right.\end{split}\]</div>
<p>累积分布函数 <span class="math notranslate nohighlight">\(F_X(x)\)</span> 表示将 <span class="math notranslate nohighlight">\(X\)</span> 取值的概率由 <span class="math notranslate nohighlight">\(-\infty\)</span> 累计到 <span class="math notranslate nohighlight">\(x\)</span> 。</p>
</div>
<p>在一个概率模型中，随机变量可以有不同的类型，可以是离散的，也可以是连续的，甚至可以是既非离散也非连续的。
但不管什么类型的随机变量，它们都会有一个相对应的累积分布函数。
这是因为 <span class="math notranslate nohighlight">\(\{X \le x\}\)</span> 是一个随机事件，这些事件的概率形成概率分布。</p>
<div class="topic">
<p class="topic-title">累积分布函数的性质</p>
<p>随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的累积分布函数 <span class="math notranslate nohighlight">\(F_X\)</span> 由下式定义，</p>
<blockquote>
<div><p>对每一个 <span class="math notranslate nohighlight">\(x\)</span>， <span class="math notranslate nohighlight">\(F_X(x) = P(X \le x)\)</span>，</p>
</div></blockquote>
<p>并且 <span class="math notranslate nohighlight">\(F_X\)</span> 具有下列形式</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(F_X\)</span> 是单调非减函数。</p>
<blockquote>
<div><p>若 <span class="math notranslate nohighlight">\(x_1\le x_2\)</span>，则 <span class="math notranslate nohighlight">\(F_X(x_1) \le F_X(x_2)\)</span></p>
</div></blockquote>
</li>
<li><p>当 <span class="math notranslate nohighlight">\(x \rightarrow -\infty\)</span> 时的时候， <span class="math notranslate nohighlight">\(F_X(x)\)</span> 趋近于 <span class="math notranslate nohighlight">\(0\)</span>，
当 <span class="math notranslate nohighlight">\(x \rightarrow \infty\)</span> 时的时候， <span class="math notranslate nohighlight">\(F_X(x)\)</span> 趋近于 <span class="math notranslate nohighlight">\(1\)</span>。</p></li>
<li><p>当 <span class="math notranslate nohighlight">\(X\)</span> 是离散随机变量的时候， <span class="math notranslate nohighlight">\(F_X(x)\)</span> 为 <span class="math notranslate nohighlight">\(x\)</span> 的阶梯函数。</p></li>
<li><p>当 <span class="math notranslate nohighlight">\(X\)</span> 是连续随机变量的时候， <span class="math notranslate nohighlight">\(F_X(x)\)</span> 为 <span class="math notranslate nohighlight">\(x\)</span> 的连续函数。</p></li>
<li><p>当 <span class="math notranslate nohighlight">\(X\)</span> 是离散随机变量并取整数值的时候， 累积分布函数 <span class="math notranslate nohighlight">\(F_X(x)\)</span> 和概率质量函数 <span class="math notranslate nohighlight">\(f_X(x)\)</span>
可以利用求和或差分互求：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-34">
<span class="eqno">(1.6.10)<a class="headerlink" href="#equation-glm-source-content-34" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}F_X(k) &amp;= \sum_{i=-\infty}^k f_X(i)\\f_X(k) &amp;= P(X \le k) - P(X \le k-1) =  F_X(k) -  F_X(k-1)\end{aligned}\end{align} \]</div>
<p>其中 <span class="math notranslate nohighlight">\(k\)</span> 可以是任意整数。</p>
</li>
<li><p>当 <span class="math notranslate nohighlight">\(X\)</span> 是连续随机变量的时候，累积分布函数 <span class="math notranslate nohighlight">\(F_X(x)\)</span> 和概率密度函数 <span class="math notranslate nohighlight">\(f_X(x)\)</span>
可以利用积分或微分函数互求：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-35">
<span class="eqno">(1.6.11)<a class="headerlink" href="#equation-glm-source-content-35" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}F_X(x) &amp;= \int_{-\infty}^x f_X(t) dt\\f_X(x) &amp;= \frac{d F_X }{dx} (x)\end{aligned}\end{align} \]</div>
</li>
</ul>
</div>
</section>
<section id="id14">
<h3><span class="section-number">1.6.4. </span>随机变量的函数<a class="headerlink" href="#id14" title="永久链接至标题"></a></h3>
<p>之前已经提到过，你可以把随机变量看做随机事件的扩展，随机事件只有发生、不发生两个状态，
而随机变量是试验结果样本空间到数值的映射，它可以有更多的状态，样本空间中每个样本点对应着随机变量的一个取值。
随机事件拥有的特性随机变量也有，
比如条件概率、联合概率、贝叶斯定理等等，
对随机变量都是成立的，只需要把那些大写字母的符号看做是随机变量即可，
这里就不再赘述了。
本节我们讨论之前没有讨论过的内容，
随机变量的函数。</p>
<p>假设 <span class="math notranslate nohighlight">\(X\)</span> 是一个随机变量，
<span class="math notranslate nohighlight">\(g(X)\)</span> 是随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的一个函数，
<span class="math notranslate nohighlight">\(g(X)\)</span> 就相当于对 <span class="math notranslate nohighlight">\(X\)</span> 施行了一个变换，
这种变换可以是线性的也可以是非线性的。
假设 <span class="math notranslate nohighlight">\(g(X)\)</span> 是一个线性变换</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-36">
<span class="eqno">(1.6.12)<a class="headerlink" href="#equation-glm-source-content-36" title="公式的永久链接"></a></span>\[Y = g(X) = aX + b\]</div>
<p>其中 <span class="math notranslate nohighlight">\(a,b\)</span> 是数值。
我们也可以考虑非线性的函数，比如</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-37">
<span class="eqno">(1.6.13)<a class="headerlink" href="#equation-glm-source-content-37" title="公式的永久链接"></a></span>\[Y = g(X) = X^2\]</div>
<p>设 <span class="math notranslate nohighlight">\(Y=g(X)\)</span> 是随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的函数，
由于对于 <span class="math notranslate nohighlight">\(X\)</span> 的每一个可能取值，也对应的 <span class="math notranslate nohighlight">\(Y\)</span>
的一个数值，故 <span class="math notranslate nohighlight">\(Y\)</span> 也是一个随机变量，
<span class="math notranslate nohighlight">\(Y\)</span> 的概率分布可以通过 <span class="math notranslate nohighlight">\(X\)</span> 的概率分布计算得到。</p>
<p>设离散随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的值域为 <span class="math notranslate nohighlight">\(\{-1,-2,0,1,2\}\)</span>
，并且每个值的概率都是 <span class="math notranslate nohighlight">\(1/5\)</span>
。 <span class="math notranslate nohighlight">\(Y\)</span> 是 <span class="math notranslate nohighlight">\(X\)</span> 的平方，即 <span class="math notranslate nohighlight">\(Y=g(X)=X^2\)</span>
，则 <span class="math notranslate nohighlight">\(Y\)</span> 的取值空间为 <span class="math notranslate nohighlight">\(\{0,1,4\}\)</span>
。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-38">
<span class="eqno">(1.6.14)<a class="headerlink" href="#equation-glm-source-content-38" title="公式的永久链接"></a></span>\[\begin{split}Y =\left\{
    \begin{aligned}
    0 &amp;, \quad x =0 \\
    1 &amp;, \quad x \in \{-1,1\} \\
    4 &amp;, \quad x \in \{-1,2\}
    \end{aligned}
    \right.\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(Y\)</span> 的概率分布为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-39">
<span class="eqno">(1.6.15)<a class="headerlink" href="#equation-glm-source-content-39" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}P(Y=0) &amp;= P(X=0) = \frac{1}{5}\\P(Y=1) &amp;= P(X=-1) + P(X=1) = \frac{2}{5}\\P(Y=4) &amp;= P(X=-2) + P(X=2) = \frac{2}{5}\end{aligned}\end{align} \]</div>
<p>多个随机变量的函数也是一样的。假设 <span class="math notranslate nohighlight">\(X\)</span> 和 <span class="math notranslate nohighlight">\(Y\)</span> 是两个随机变量，
那么 <span class="math notranslate nohighlight">\(Z=g(X,Y)\)</span> 也是一个随机变量，即使更多的随机变量的函数也是如此。</p>
</section>
<section id="id15">
<h3><span class="section-number">1.6.5. </span>期望与方差<a class="headerlink" href="#id15" title="永久链接至标题"></a></h3>
<p>随机变量的取值不是确定性的，有一定的随机性，它的概率分布给出了其所有可能取值的概率。
随机变量的概率分布不是很方便进行比较和评价，
通常，我们希望将这些信息综合成一个能代表这个随机变量的 <strong>数值</strong> 。
在数学和统计学中，<strong>矩（moment）</strong> 是对变量分布和形态特点的一组度量。
<strong>n阶矩</strong> 被定义为变量的 <span class="math notranslate nohighlight">\(n\)</span> 次方与其概率分布函数之积的积分，
变量的 <strong>一阶矩</strong> 就是变量的期望值，也叫平均值。</p>
<div class="topic">
<p class="topic-title">随机变量的期望</p>
<p>设离散随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的概率质量函数为 <span class="math notranslate nohighlight">\(f(x)\)</span> ，<span class="math notranslate nohighlight">\(X\)</span> 的 <strong>期望值</strong> （也称为 <strong>期望</strong> 或者 <strong>均值</strong> ）
由下式给出</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-glm-source-content-40">
<span class="eqno">(1.6.16)<a class="headerlink" href="#equation-glm-source-content-40" title="公式的永久链接"></a></span>\[\mathbb{E}[X] = \sum_x x f(x)\]</div>
</div></blockquote>
<p>设连续值随机变量 <span class="math notranslate nohighlight">\(Y\)</span> 的概率密度函数为 <span class="math notranslate nohighlight">\(g(x)\)</span> ，<span class="math notranslate nohighlight">\(Y\)</span> 的 <strong>期望值</strong> （也称为 <strong>期望</strong> 或者 <strong>均值</strong> ）
由下式给出</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-glm-source-content-41">
<span class="eqno">(1.6.17)<a class="headerlink" href="#equation-glm-source-content-41" title="公式的永久链接"></a></span>\[\mathbb{E}[Y] = \int y g(y) d y\]</div>
</div></blockquote>
</div>
<p><strong>随机变量的期望值是一个数值，而不再是随机变量</strong>。
随机变量的期望值可以看做是这个变量的 <strong>中心</strong> ，
大量重复试验结果的数学平均值就渐近等于变量的期望值，
在之后讲最大似然估计时会详细介绍。</p>
<p>我们已经知道随机变量的函数也是一个随机变量，随机变量的函数的期望可以用如下方式得到。</p>
<div class="topic">
<p class="topic-title">随机变量函数的期望</p>
<p>设随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的概率分布函数为 <span class="math notranslate nohighlight">\(f(x)\)</span>，又设 <span class="math notranslate nohighlight">\(h(x)\)</span> 是变量 <span class="math notranslate nohighlight">\(X\)</span> 的一个函数，
则 <span class="math notranslate nohighlight">\(h(x)\)</span> 的期望由下式得到</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-42">
<span class="eqno">(1.6.18)<a class="headerlink" href="#equation-glm-source-content-42" title="公式的永久链接"></a></span>\[\mathbb{E}[h(x)] = \int h(x) f(x) dx\]</div>
<p>如果 <span class="math notranslate nohighlight">\(X\)</span> 是离散随机变量，只需要把积分换成求和。</p>
</div>
<p>直接使用变量计算的矩被称为原始矩（raw moment），比如期望就是原始矩。
移除均值后计算的矩被称为中心矩（central moment），
变量的一阶原始矩等价于数学期望（expectation）、二至四阶中心矩被定义为方差（variance）、偏度（skewness）和峰度（kurtosis）
。</p>
<p>随机变量另一个常见的独立方法就是 <strong>方差（variance）</strong>
，<strong>方差是二阶中心矩</strong> 。所谓中心矩就是去除中心(期望值)，
所谓的二阶就是二次方，因此方差的计算方法为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-43">
<span class="eqno">(1.6.19)<a class="headerlink" href="#equation-glm-source-content-43" title="公式的永久链接"></a></span>\[V(X) = \mathbb{E}[(X-\mathbb{E}[X])^2]\]</div>
<p>注意上式中最外层又求了一次期望，这是因为 <span class="math notranslate nohighlight">\((X-\mathbb{E}[X])^2\)</span> 本身是随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的一个函数，
它也是一个随机变量，因此再对它求一次期望以便得到一个 <em>数值</em> 。
根据随机变量函数期望的求法，方差的计算方法为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-44">
<span class="eqno">(1.6.20)<a class="headerlink" href="#equation-glm-source-content-44" title="公式的永久链接"></a></span>\[V(X) = \int (X-\mathbb{E}[X])^2 f(x) dx\]</div>
<p>其中 <span class="math notranslate nohighlight">\(f(x)\)</span> 是变量 <span class="math notranslate nohighlight">\(X\)</span> 的概率分布函数。</p>
<p>方差的值是原始变量的平方，量纲发生了变化，其量纲是原始值的平方，不利于和原始值进行比较，
因此定义方差的非负平方根为 <strong>标准差</strong> ，
标准差和原始变量的量纲是一致的，可以直接进行比较。</p>
<p>现在我们来看一下均值和方差的一些性质，
首先考虑随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的线性函数</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-45">
<span class="eqno">(1.6.21)<a class="headerlink" href="#equation-glm-source-content-45" title="公式的永久链接"></a></span>\[Y = aX +b\]</div>
<p>其中 <span class="math notranslate nohighlight">\(a\)</span> 和 <span class="math notranslate nohighlight">\(b\)</span> 是已知常数，
<span class="math notranslate nohighlight">\(f(x)\)</span> 为随机变量  <span class="math notranslate nohighlight">\(X\)</span> 的概率分布函数，
关于线性函数 <span class="math notranslate nohighlight">\(Y\)</span> 的均值和方差，我们有</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-46">
<span class="eqno">(1.6.22)<a class="headerlink" href="#equation-glm-source-content-46" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mathbb{E}[Y] &amp;= \sum_x (ax+b) f(x)\\&amp;= a \sum_x x f(x) +  b \sum_x  f(x)\\&amp; = a \mathbb{E}[X] + b\end{aligned}\end{align} \]</div>
<p>进一步地</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-47">
<span class="eqno">(1.6.23)<a class="headerlink" href="#equation-glm-source-content-47" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}V(Y) &amp;= \sum_x (Y - \mathbb{E}[Y])^2 f(x)\\&amp;= \sum_x [ax+b -  (a \mathbb{E}[X] + b)  ]^2 f(x)\\&amp;= \sum_x (ax - a \mathbb{E}[X] )^2 f(x)\\&amp;= a^2 \sum_x (x - \mathbb{E}[X]  )^2 f(x)\\&amp;= a^2 V(X)\end{aligned}\end{align} \]</div>
<div class="topic">
<p class="topic-title">随机变量的线性函数的均值和方差</p>
<p>设 <span class="math notranslate nohighlight">\(X\)</span> 为随机变量，令</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-48">
<span class="eqno">(1.6.24)<a class="headerlink" href="#equation-glm-source-content-48" title="公式的永久链接"></a></span>\[Y = aX +b\]</div>
<p>其中 <span class="math notranslate nohighlight">\(a\)</span> 和 <span class="math notranslate nohighlight">\(b\)</span> 为给定的常数，则</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-49">
<span class="eqno">(1.6.25)<a class="headerlink" href="#equation-glm-source-content-49" title="公式的永久链接"></a></span>\[\mathbb{E}[Y]=a \mathbb{E}[X] + b \quad V(Y)= a^2 V(X)\]</div>
<p><strong>特别注意，这只对线性函数成立，非线性函数不成立</strong>。</p>
</div>
<p>此外，还有一个用矩表达方差的重要公式。</p>
<div class="topic">
<p class="topic-title">用矩表达的方差公式</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-50">
<span class="eqno">(1.6.26)<a class="headerlink" href="#equation-glm-source-content-50" title="公式的永久链接"></a></span>\[V(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2\]</div>
<p>证明如下：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-51">
<span class="eqno">(1.6.27)<a class="headerlink" href="#equation-glm-source-content-51" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}V(X) &amp;= \sum_x (x - \mathbb{E}[X])^2 f(x)\\&amp;= \sum_x (x^2 - 2x \mathbb{E}[X]  + \mathbb{E}[X]^2 ) f(x)\\&amp;= \sum_x x^2 f(x) + 2 \mathbb{E}[X] \sum_x xf(x) + \mathbb{E}[X]^2 \sum_x f(x)\\&amp;=  \mathbb{E}[X^2] - 2 \mathbb{E}[X] \times  \mathbb{E}[X] + (\mathbb{E}[X])^2\\&amp;=  \mathbb{E}[X^2] - 2 (\mathbb{E}[X])^2 + (\mathbb{E}[X])^2\\&amp;=  \mathbb{E}[X^2] -  (\mathbb{E}[X])^2\end{aligned}\end{align} \]</div>
</div>
</section>
</section>
<section id="id16">
<h2><span class="section-number">1.7. </span>边缘化<a class="headerlink" href="#id16" title="永久链接至标题"></a></h2>
<div class="topic">
<p class="topic-title">边缘化（marginalization）</p>
<p>边缘化（marginalization），又叫边际化，它是指从多个随机变量的联合概率分布中求解出部分随机变量联合概率分布的过程。
字面意思就是：在一个随机变量集合中，把其中部分随机变量”边缘化（marginalization）” ，
“边缘化” 这个词的目标变量是剩下的变量子集，而不是被”去掉”的那些。</p>
</div>
<p>假设已知随机变量 <span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span> 的联合概率分布为 <span class="math notranslate nohighlight">\(P(A,B)\)</span>，
以及变量 <span class="math notranslate nohighlight">\(A\)</span> 的概率分布 <span class="math notranslate nohighlight">\(P(A)\)</span>，现在想求出变量 <span class="math notranslate nohighlight">\(B\)</span> 的概率分布 <span class="math notranslate nohighlight">\(P(B)\)</span>。
这个过程就是把 <span class="math notranslate nohighlight">\(B\)</span> 进行边缘化，求出边缘概率分布 <span class="math notranslate nohighlight">\(P(B)\)</span> 。</p>
<div class="topic">
<p class="topic-title">边缘概率分布</p>
<p>边缘分布（Marginal Distribution）是指，在多个随机变量的联合概率分布中，只包含其中 <strong>部分变量（可以是多个联合）</strong> 的概率分布。
边缘概率分布可以通过对联合概率分布在除目标变量以外的其他变量（边缘化）求和(积分)得到。</p>
</div>
<p>它的计算过程其实就是利用全概率公式把变量 <span class="math notranslate nohighlight">\(A\)</span> 从联合概率分布中 <strong>消除掉</strong>，
因此可以称为 <strong>消元法</strong>。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-52">
<span class="eqno">(1.7.1)<a class="headerlink" href="#equation-glm-source-content-52" title="公式的永久链接"></a></span>\[P(B) = \sum_A P(A,B) = \sum_A P(A)(B|A)\]</div>
<p>当已知 <span class="math notranslate nohighlight">\(P(A)\)</span> 和 <span class="math notranslate nohighlight">\(P(A,B)\)</span>，可以利用边缘化的方法求得 <span class="math notranslate nohighlight">\(P(B)\)</span> 的概率分布。
即使更多的随机变量也是如此，
比如，已知 <span class="math notranslate nohighlight">\(4\)</span> 个变量的联合概率分布 <span class="math notranslate nohighlight">\(P(A,B,C,D)\)</span> 以及 <span class="math notranslate nohighlight">\(P(C),P(D|C)\)</span>，
想要求 <span class="math notranslate nohighlight">\(P(A,B)\)</span> 的概率分布，这时就需要”边缘化”随机变量 <span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span>，
也就是从 <span class="math notranslate nohighlight">\(P(A,B,C,D)\)</span> “消除掉”随机变量 <span class="math notranslate nohighlight">\(C\)</span> 和 <span class="math notranslate nohighlight">\(D\)</span>，
从而得到 <span class="math notranslate nohighlight">\(P(A,B)\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-53">
<span class="eqno">(1.7.2)<a class="headerlink" href="#equation-glm-source-content-53" title="公式的永久链接"></a></span>\[P(A,B) = \sum_C \sum_D P(A,B,C,D) = \sum_C \sum_D P(C) P(D|C) P(A,B|C,D)\]</div>
<p>如果是连续随机变量，就把求和换成积分。</p>
</section>
<section id="id17">
<h2><span class="section-number">1.8. </span>常见概率分布<a class="headerlink" href="#id17" title="永久链接至标题"></a></h2>
<p>本节我们介绍一些已知的并且常用的概率分布，
这些概率分布会在本书之后的章节中频繁使用，
需要读者对这些分布的特性十分熟悉。</p>
<section id="ch-basic-bernoulli">
<span id="id18"></span><h3><span class="section-number">1.8.1. </span>伯努利分布<a class="headerlink" href="#ch-basic-bernoulli" title="永久链接至标题"></a></h3>
<p>伯努利分布是最简单的离散概率分布，伯努利分布是单次伯努利试验结果的分布。
<strong>伯努利试验（Bernoulli experiment）</strong> 是在同样的条件下重复地、相互独立地进行的一种随机试验，
其特点是该随机试验只有两种可能结果：发生或者不发生。
最简单的例子就是投掷硬币的试验，投掷硬币的结果只有正面（正面发生）和反面（正面不发生）两种结果，
投硬币试验就是一种伯努利实验。</p>
<p>单次伯努利试验结果的概率分布就称为伯努利概率分布，服从伯努利概率分布的随机变量可以称为 <strong>伯努利变量</strong>
。假设随机变量 <span class="math notranslate nohighlight">\(X\)</span> 是伯努利变量，设正面向上的概率为 <span class="math notranslate nohighlight">\(\pi\)</span>，
则反面向上的概率为 <span class="math notranslate nohighlight">\(1-\pi\)</span> ，正面向上的结果用数字 <span class="math notranslate nohighlight">\(1\)</span> 表示，
反面向上的结果用数字 <span class="math notranslate nohighlight">\(0\)</span> 表示，即</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-54">
<span class="eqno">(1.8.1)<a class="headerlink" href="#equation-glm-source-content-54" title="公式的永久链接"></a></span>\[\begin{split}X = \left \{
    \begin{aligned}
    1 &amp;, \quad \text{若正面向上} \\
    0 &amp;, \quad \text{若反面向上}
    \end{aligned}
    \right.\end{split}\]</div>
<p>它的概率分布为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-55">
<span class="eqno">(1.8.2)<a class="headerlink" href="#equation-glm-source-content-55" title="公式的永久链接"></a></span>\[\begin{split}P(X) = \left \{
    \begin{aligned}
    \pi &amp;, \quad \text{若}X=1 \\
    1-\pi &amp;, \quad \text{若}X=0
    \end{aligned}
    \right.\end{split}\]</div>
<p>分段函数不利于参与计算，通常伯努利变量的概率质量函数可以写成如下简单的形式</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-56">
<span class="eqno">(1.8.3)<a class="headerlink" href="#equation-glm-source-content-56" title="公式的永久链接"></a></span>\[P(X) = \pi^x (1-\pi)^{1-x}\]</div>
<p>根据随机变量期望的计算公式，伯努利变量的期望为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-57">
<span class="eqno">(1.8.4)<a class="headerlink" href="#equation-glm-source-content-57" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mathbb{E}[X] &amp;= \sum_{x\in \{0,1\}} x P(X)\\&amp;= \sum_{x\in \{0,1\}} x  \pi^x (1-\pi)^{1-x}\\&amp;= \pi\end{aligned}\end{align} \]</div>
<p>可以看到，对于伯努利变量来说， <span class="math notranslate nohighlight">\(\pi\)</span> 既是 <span class="math notranslate nohighlight">\(X=1\)</span> 的概率，也是它的期望值。
我们再来看下伯努利变量的方差。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-58">
<span class="eqno">(1.8.5)<a class="headerlink" href="#equation-glm-source-content-58" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}V(X) &amp;= \mathbb{E}[(X-\mathbb{E}[X])^2]\\&amp;= \sum_{x\in \{0,1\}} (x-\mathbb{E}[X])^2 P(X)\\&amp;= \sum_{x\in \{0,1\}} (x-\pi)^2 \pi^x (1-\pi)^{1-x}\\&amp;= (-\pi)^2 \pi^0 (1-\pi)^{1-0} + (1-\pi)^2 \pi^1 (1-\pi)^{1-1}\\&amp;= \pi^2 (1-\pi) + (1-\pi)^2 \pi\\&amp;= \pi (1-\pi)\end{aligned}\end{align} \]</div>
</section>
<section id="id19">
<h3><span class="section-number">1.8.2. </span>二项式分布<a class="headerlink" href="#id19" title="永久链接至标题"></a></h3>
<p>单次伯努利试验的结果分布是伯努利分布，如果进行多次伯努利试验，试验结果中证明向上的次数定义为随机变量
则这个随机变量的概率分布是二项式分布，
<strong>注意这多次伯努利试验要求是同样的条件下重复地、相互独立地进行的</strong>。</p>
<p>假设我们在同样的条件下重复地、相互独立进行了 <span class="math notranslate nohighlight">\(n\)</span> 次伯努利实验，
单次试验成功的概率为 <span class="math notranslate nohighlight">\(\pi\)</span>，
<span class="math notranslate nohighlight">\(n\)</span> 试验后一共成功的次数为 <span class="math notranslate nohighlight">\(X\)</span>
，则 <span class="math notranslate nohighlight">\(X\)</span> 的概率分布称为二项式分布，一般记作 <span class="math notranslate nohighlight">\(X \sim B(n,\pi)\)</span>，
其概率质量函数为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-59">
<span class="eqno">(1.8.6)<a class="headerlink" href="#equation-glm-source-content-59" title="公式的永久链接"></a></span>\[P(X) = \binom{n}{x} \pi^x (1-\pi)^{n-x}\]</div>
<p>变量 <span class="math notranslate nohighlight">\(X\)</span> 表示在 <span class="math notranslate nohighlight">\(n\)</span> 次试验中成功的次数，只是一个次数累计，
对成功的位置并没有限制，因此在 <span class="math notranslate nohighlight">\(n\)</span> 次试验中任意位置成功 <span class="math notranslate nohighlight">\(X\)</span> 次都可以，
所以上式中需要一个组合数项 <span class="math notranslate nohighlight">\(\binom{n}{x}\)</span> 。
由于是在同样的条件下重复地、相互独立的进行试验，
因此每一次试验成功的概率都是 <span class="math notranslate nohighlight">\(\pi\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-60">
<span class="eqno">(1.8.7)<a class="headerlink" href="#equation-glm-source-content-60" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mathbb{E}[X] &amp;= \sum_{x=0}^{n}  x P(X)\\&amp;= \sum_{x=0}^{n}  x \binom{n}{x} \pi^x (1-\pi)^{n-x}\\&amp;= \sum_{x=1}^{n}   \frac{xn!}{x!(n-x)!} \pi^x (1-\pi)^{n-x}\\&amp;= \sum_{x=1}^{n}   \frac{n \pi (n-1)!}{(x-1)![(n-1)-(x-1)]!} \pi^{x-1} (1-\pi)^{(n-1)-(x-1)}\\&amp;= n \pi   \underbrace{\sum_{x=1}^{n} \frac{ (n-1)!}{(x-1)![(n-1)-(x-1)]!} \pi^{x-1} (1-\pi)^{(n-1)-(x-1)}}_{
\text{相当于}n-1 \text{次二项分布的累加，等于}1}\\&amp;= n \pi\end{aligned}\end{align} \]</div>
<p>二项式变量 <span class="math notranslate nohighlight">\(X\)</span> 表示 <span class="math notranslate nohighlight">\(n\)</span> 次伯努利试验中成功的次数
，它的期望值 <span class="math notranslate nohighlight">\(n \pi\)</span>。
现在我们把它的概率分布用柱状图的形式呈现出来，以便直观的感受其中的变化。</p>
<figure class="align-center" id="id25">
<span id="fg-probability-002"></span><a class="reference internal image-reference" href="../../../_images/二项式分布_1.jpg"><img alt="../../../_images/二项式分布_1.jpg" src="../../../_images/二项式分布_1.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 1.8.1 </span><span class="caption-text">当 <span class="math notranslate nohighlight">\(\pi=0\)</span> 时，<span class="math notranslate nohighlight">\(P(X=0)=1,P(X&gt;0)=0\)</span> ；
当 <span class="math notranslate nohighlight">\(\pi=1\)</span> 时，<span class="math notranslate nohighlight">\(P(X=n)=1,P(X&lt;n)=0\)</span> 。</span><a class="headerlink" href="#id25" title="永久链接至图片"></a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#fg-probability-002"><span class="std std-numref">图 1.8.1</span></a> 所示是进行 <span class="math notranslate nohighlight">\(n=50\)</span> 次伯努利试验时，
不同 <span class="math notranslate nohighlight">\(\pi\)</span> 值的情况下，二项式变量 <span class="math notranslate nohighlight">\(X\)</span> 的概率分布图。
可以看到概率最大（最高的柱子）的点就是期望值 <span class="math notranslate nohighlight">\(n \pi\)</span> 的点
，随着从期望值的点向着两侧延伸概率逐渐变小。
有两个特殊的情况，当 <span class="math notranslate nohighlight">\(\pi=0\)</span> 时意味着成功的概率是 <span class="math notranslate nohighlight">\(0\)</span>
，因此图形上 <span class="math notranslate nohighlight">\(x=0\)</span> 的点概率是 <span class="math notranslate nohighlight">\(1\)</span>，其它点 <span class="math notranslate nohighlight">\(x&gt;0\)</span> 的概率是 <span class="math notranslate nohighlight">\(0\)</span>
。反过来，当 <span class="math notranslate nohighlight">\(\pi=1\)</span> 时意味着成功的概率是 <span class="math notranslate nohighlight">\(1\)</span>，
因此图形上 <span class="math notranslate nohighlight">\(x=n\)</span> 的点的概率是 <span class="math notranslate nohighlight">\(1\)</span>，其它点 <span class="math notranslate nohighlight">\(x&lt;n\)</span> 的概率是 <span class="math notranslate nohighlight">\(0\)</span>。</p>
<p>最后我们看下二项式分布的方差，其计算过程如 <a class="reference internal" href="#equation-eq-probability-080">公式(1.8.8)</a> 所示，
可以看到二项式分布的方差就是伯努利分布方差的 <span class="math notranslate nohighlight">\(n\)</span> 倍。</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-080">
<span class="eqno">(1.8.8)<a class="headerlink" href="#equation-eq-probability-080" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}V(X) &amp;= \mathbb{E}[X^2] - (\mathbb{E}[X])^2\\&amp;=\mathbb{E}[X(X-1) +X ] - (\mathbb{E}[X])^2\\&amp;=\mathbb{E}[ X(X-1)] +\mathbb{E}[X] - (\mathbb{E}[X])^2\\&amp;= \sum_{x=0}^{n} \left [ x(x-1)  \binom{n}{x} \pi^x (1-\pi)^{n-x} \right ] + n \pi + (n \pi)^2\\&amp;= \sum_{x=0}^{n} \underbrace{\left [ x(x-1)   \frac{n!}{x!(n-x)!} \pi^x (1-\pi)^{n-x} \right ]}_{
  x=0,x=1\text{时此项为0，求和可以省去}  }+ n \pi + (n \pi)^2\\&amp;= \sum_{x=2}^{n} \left [ n(n-1)\pi^2 \frac{(n-2)!}{(x-2)![(n-2)-(x-2)]!} \pi^{x-2} (1-\pi)^{(n-2)-(x-2)} \right ] + n \pi + (n \pi)^2\\&amp;=  n(n-1)\pi^2  \underbrace{\sum_{x=2}^{n} \left [\frac{(n-2)!}{(x-2)![(n-2)-(x-2)]!} \pi^{x-2} (1-\pi)^{(n-2)-(x-2)} \right ]}_{
    \text{相当于概率分布} B(n-2,\pi) \text{的累积，其结果为}1  } + n \pi + (n \pi)^2\\&amp;= n(n-1)\pi^2  +  n \pi + (n \pi)^2\\&amp;= n^2\pi^2 - n \pi^2  +  n \pi + (n \pi)^2\\&amp;= n \pi(1-\pi)\end{aligned}\end{align} \]</div>
</section>
<section id="id20">
<h3><span class="section-number">1.8.3. </span>类别分布<a class="headerlink" href="#id20" title="永久链接至标题"></a></h3>
<p>伯努利随机变量只有两个离散状态，
如果一个离散随机变量拥有更多的离散状态，就称这个变量为类别变量（categorical variable）
，它的概率分布称为类别分布(categorical distribution)。
显然类别随机变量也是一个离散随机变量，它比伯努利变量拥有更多的可能取值。</p>
<p>假设随机变量 <span class="math notranslate nohighlight">\(X\)</span> 是一个拥有 <span class="math notranslate nohighlight">\(K\)</span> 个可能取值的离散随机变量
其取值空间记作 <span class="math notranslate nohighlight">\(\mathcal{X}=\{x_1,\dots,x_K\}\)</span>。
变量 <span class="math notranslate nohighlight">\(X\)</span> 取值为 <span class="math notranslate nohighlight">\(x_i\)</span> 的概率记作 <span class="math notranslate nohighlight">\(\pi_i\)</span>
，类似于伯努利变量，类别变量的概率质量函数可以用一个分段函数表示。</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-081">
<span class="eqno">(1.8.9)<a class="headerlink" href="#equation-eq-probability-081" title="公式的永久链接"></a></span>\[\begin{split}P(X) = \left \{
    \begin{aligned}
    \pi_1 &amp;, \quad \text{若}X=x_1 \\
    \pi_2 &amp;, \quad \text{若}X=x_2 \\
    \cdots &amp;,\quad \cdots \\
    \pi_K &amp;, \quad \text{若}X=x_K \\
    \end{aligned}
    \right.\end{split}\]</div>
<p>同样也需要满足概率和为 <span class="math notranslate nohighlight">\(1\)</span> 的约束， <span class="math notranslate nohighlight">\(\sum_{k=1}^K \pi_k = 1\)</span>。</p>
<p>分段函数的形式不够简洁，通常会借用一个指示函数改写一下类别分布的概率质量函数，
使它的形式更利于参与到各类复杂计算中。</p>
<div class="topic">
<p class="topic-title">指示函数</p>
<p>定义如下函数为指示函数。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-61">
<span class="eqno">(1.8.10)<a class="headerlink" href="#equation-glm-source-content-61" title="公式的永久链接"></a></span>\[\begin{split}\mathbb{I}(x,a)=\left\{
\begin{aligned}
1 &amp; \quad &amp;if \quad x = a \\
0 &amp; \quad &amp;otherwise
\end{aligned}
\right.\end{split}\]</div>
<p>当满足 <span class="math notranslate nohighlight">\(x=a\)</span> 是函数输出值为 <span class="math notranslate nohighlight">\(1\)</span>，反之函数输出值为 <span class="math notranslate nohighlight">\(0\)</span>。</p>
</div>
<p>利用指示函数可以把 <a class="reference internal" href="#equation-eq-probability-081">公式(1.8.9)</a> 改写成如下更简洁的形式。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-62">
<span class="eqno">(1.8.11)<a class="headerlink" href="#equation-glm-source-content-62" title="公式的永久链接"></a></span>\[P(X) = \prod_{k=1}^{K} \pi_i^{\mathbb{I} (x,x_k)}, \quad \sum_{k=1}^K \pi_k = 1\]</div>
<p>虽然通常会用连续的整数 <span class="math notranslate nohighlight">\({1,2,3,\cdots,K}\)</span> 表示对应的类别 <span class="math notranslate nohighlight">\(\{x_1,x_2,x_3,\cdots,x_K\}\)</span>
，但是要注意，对于类别变量的各个类别 <span class="math notranslate nohighlight">\(x_k\)</span> 之间是没有任何顺序、大小关系的，各个类别之间是独立的。
因此它的期望和方差也是每个类别单独计算。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-63">
<span class="eqno">(1.8.12)<a class="headerlink" href="#equation-glm-source-content-63" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mathbb{E}[X=x_k] &amp;= \pi_k\\V(X=x_k) &amp;= \pi_k(1-\pi_k)\end{aligned}\end{align} \]</div>
<p>由于类别变量类别数多于 <span class="math notranslate nohighlight">\(2\)</span> 个，所以还需要给出类别之间的协方差。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-64">
<span class="eqno">(1.8.13)<a class="headerlink" href="#equation-glm-source-content-64" title="公式的永久链接"></a></span>\[Cov(X_i,X_j) = - \pi_i \pi_j, \quad i \neq j\]</div>
</section>
<section id="id21">
<h3><span class="section-number">1.8.4. </span>多项式分布<a class="headerlink" href="#id21" title="永久链接至标题"></a></h3>
<p>我们知道，二值离散变量称为伯努利变量(Bernoulli variable)，其概率分布称为伯努利分布(Bernoulli distribution)，
多次伯努利采样称为二项式分布(binomial distribution)，伯努利分布是二项式分布特例，即仅进行单次试验的情况。
相对应的，多值离散变量称为类别变量(categorical variable)，其概率分布称为类别分布(categorical distribution)，
多次类别分布采样称为多项式分布(multinomial distribution)，类别分布是多项式分布的特例。</p>
<p>我们用M表示变量的取值个数，比如对于伯努利变量 <span class="math notranslate nohighlight">\(K=2\)</span> ，用 <span class="math notranslate nohighlight">\(n\)</span> 表示试验次数(采样次数):</p>
<ol class="arabic simple">
<li><p>当 <span class="math notranslate nohighlight">\(K=2,n=1\)</span> 时，是伯努利分布(Bernoulli distribution)。</p></li>
<li><p>当 <span class="math notranslate nohighlight">\(K=2,n&gt;1\)</span> 时，是二项式分布(binomial distribution)。</p></li>
<li><p>当 <span class="math notranslate nohighlight">\(K&gt;2,n=1\)</span> 时，是类别分布(categorical distribution)。</p></li>
<li><p>当 <span class="math notranslate nohighlight">\(K&gt;2,n&gt;1\)</span> 时，是多项式分布(multinomial distribution)。</p></li>
</ol>
<p>假设随机变量 <span class="math notranslate nohighlight">\(X\)</span> 服从多项式分布，<span class="math notranslate nohighlight">\(x_k\)</span> 表示 <span class="math notranslate nohighlight">\(n\)</span> 次试验结果中类别 <span class="math notranslate nohighlight">\(k\)</span>
出现的次数。对照着二项式分布的概率质量函数，可以直接给出多项式分布的概率质量函数。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-65">
<span class="eqno">(1.8.14)<a class="headerlink" href="#equation-glm-source-content-65" title="公式的永久链接"></a></span>\[P(X) = \frac{n!}{x_1! x_2! \cdots x_K! } \prod_{k=1}^K \pi_k^{x_k}\]</div>
<p>同理多项式分布的期望和方差也是每个类别单独计算的。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-66">
<span class="eqno">(1.8.15)<a class="headerlink" href="#equation-glm-source-content-66" title="公式的永久链接"></a></span>\[ \begin{align}\begin{aligned}\mathbb{E}[X=x_k] &amp;= n \pi_k\\V(X=x_k) &amp;= n \pi_k(1-\pi_k)\\Cov(X_i,X_j) &amp;= -n \pi_i \pi_j, \quad i \neq j\end{aligned}\end{align} \]</div>
</section>
<section id="id22">
<h3><span class="section-number">1.8.5. </span>高斯分布<a class="headerlink" href="#id22" title="永久链接至标题"></a></h3>
<p>高斯分布（Gaussian distribution），以德国数学家卡尔·弗里德里希·高斯的姓冠名，
因其是日常生活中最常见的连续值概率分布，
常在自然和社会科学领域中代表一个不明的随机变量，在统计学上十分重要，
经常又被称为正态分布（Normal distribution）、常态分布、正规分布。</p>
<p>一个连续随机变量 <span class="math notranslate nohighlight">\(X\)</span> 若它的概率密度函数具有如下形式，则称它为高斯随机变量或者正态随机变量，
记作 <span class="math notranslate nohighlight">\(X \sim N(\mu,\sigma^2)\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-eq-probability-061">
<span class="eqno">(1.8.16)<a class="headerlink" href="#equation-eq-probability-061" title="公式的永久链接"></a></span>\[f_X(x) = \frac{1}{\sigma\sqrt{2\pi }} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}\]</div>
<p>高斯随机变量的期望和方差由下式给出</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-67">
<span class="eqno">(1.8.17)<a class="headerlink" href="#equation-glm-source-content-67" title="公式的永久链接"></a></span>\[\mathbb{E} [X] = \mu,\quad V(X)=\sigma^2\]</div>
<p>高斯变量的概率密度函数 <a class="reference internal" href="#equation-eq-probability-061">公式(1.8.16)</a> 中的 <span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\sigma^2\)</span>
分别对应着变量的期望和方差，<span class="math notranslate nohighlight">\(\sigma\)</span> 为标准差（standard deviation, SD）。
期望决定分布的”中心”，方差影响着分布的”宽度”，
我们通过图形来直观的感受下。</p>
<figure class="align-center" id="id26">
<span id="fg-probability-005"></span><a class="reference internal image-reference" href="../../../_images/高斯分布_2.jpg"><img alt="../../../_images/高斯分布_2.jpg" src="../../../_images/高斯分布_2.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 1.8.2 </span><span class="caption-text">固定标准差参数 <span class="math notranslate nohighlight">\(\sigma=4\)</span>,不同的期望参数 <span class="math notranslate nohighlight">\(\mu\)</span> 下图形的变化。</span><a class="headerlink" href="#id26" title="永久链接至图片"></a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#fg-probability-005"><span class="std std-numref">图 1.8.2</span></a> 是期望参数不同取值的情况下，高斯分布概率密度函数图形的变化，
为了凸显期望参数的影响， 我们固定方差参数的值。
高斯分布的概率密度函数的曲线呈现一个重型曲线的形状，曲线的最高点就是期望值所在的点，显然期望值是概率最大的点。
随着期望值从 <span class="math notranslate nohighlight">\(-2\)</span> 到 <span class="math notranslate nohighlight">\(4\)</span> 的变化，钟形曲线向右发生平移，
因为曲线的中心在右移，
<strong>期望值的变化会导致概率密度函数的曲线发生平移</strong>。</p>
<figure class="align-center" id="id27">
<span id="fg-probability-006"></span><a class="reference internal image-reference" href="../../../_images/高斯分布_1.jpg"><img alt="../../../_images/高斯分布_1.jpg" src="../../../_images/高斯分布_1.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 1.8.3 </span><span class="caption-text">固定期望参数 <span class="math notranslate nohighlight">\(\mu=0\)</span>,不同的标准差参数 <span class="math notranslate nohighlight">\(\sigma\)</span> 下图形的变化。</span><a class="headerlink" href="#id27" title="永久链接至图片"></a></p>
</figcaption>
</figure>
<p>接下来，我们固定期望值参数为 <span class="math notranslate nohighlight">\(0\)</span>，
观看不同的方差参数对曲线的影响，如 <a class="reference internal" href="#fg-probability-006"><span class="std std-numref">图 1.8.3</span></a> 所示。
可以看出方差 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 越小图形越”尖锐”
，反之，方差 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 越大图形越”宽胖”。</p>
<p>正态分布的累积分布函数（CDF）为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-68">
<span class="eqno">(1.8.18)<a class="headerlink" href="#equation-glm-source-content-68" title="公式的永久链接"></a></span>\[F_X(x;\mu,\sigma)=P(X \le x) = \frac{1}{\sigma\sqrt{2\pi }} \int_{-\infty}^x  \exp \left [ -\frac{(t-\mu)^2}{2 \sigma^2} \right ] dt\]</div>
<p><a class="reference internal" href="#fg-probability-007"><span class="std std-numref">图 1.8.4</span></a> 是正态分布的累积分布函数的图形，
由于均值参数 <span class="math notranslate nohighlight">\(\mu\)</span> 只影响图形的左右平移，不影响图形的形状，
因此我们固定了均值参数 <span class="math notranslate nohighlight">\(\mu=0\)</span> ，观看不同标准差参数 <span class="math notranslate nohighlight">\(\sigma\)</span>
对图形曲线的影响。
可以看到标准差 <span class="math notranslate nohighlight">\(\sigma\)</span> 影响着曲线的斜率，
<span class="math notranslate nohighlight">\(\sigma\)</span> 越小曲线斜率越大，斜率越大说明累积概率上升的越快，
对照着 <a class="reference internal" href="#fg-probability-006"><span class="std std-numref">图 1.8.3</span></a> 的概率密度曲线，不难理解这一点。</p>
<figure class="align-center" id="id28">
<span id="fg-probability-007"></span><a class="reference internal image-reference" href="../../../_images/高斯分布_cdf.jpg"><img alt="../../../_images/高斯分布_cdf.jpg" src="../../../_images/高斯分布_cdf.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 1.8.4 </span><span class="caption-text">固定均值 <span class="math notranslate nohighlight">\(\mu=0\)</span> 时，不同标准差 <span class="math notranslate nohighlight">\(\sigma\)</span> 下正态累计分布函数的变化。</span><a class="headerlink" href="#id28" title="永久链接至图片"></a></p>
</figcaption>
</figure>
<div class="topic">
<p class="topic-title">线性变换之下正态性不变</p>
<p>设 <span class="math notranslate nohighlight">\(X\)</span> 是正态随机变量，其均值为 <span class="math notranslate nohighlight">\(\mu\)</span>，方差为 <span class="math notranslate nohighlight">\(\sigma^2\)</span>。
若 <span class="math notranslate nohighlight">\(a \ne 0\)</span> 和 <span class="math notranslate nohighlight">\(b\)</span> 是两个常数，则随机变量</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-69">
<span class="eqno">(1.8.19)<a class="headerlink" href="#equation-glm-source-content-69" title="公式的永久链接"></a></span>\[Y = a X +b\]</div>
<p>仍然是正态随机变量，并且其均值和方差为别为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-70">
<span class="eqno">(1.8.20)<a class="headerlink" href="#equation-glm-source-content-70" title="公式的永久链接"></a></span>\[\mathbb{E}[Y] = a \mu +b, \quad V(Y) = a^2 \sigma^2\]</div>
<p>记作 <span class="math notranslate nohighlight">\(Y \sim N(a \mu,a^2 \sigma^2)\)</span></p>
</div>
<div class="topic">
<p class="topic-title">多个独立正态随机变量之和仍然是正态变量</p>
<p>设 <span class="math notranslate nohighlight">\(X \sim  N(\mu_X,\sigma^2_X)\)</span> 与 <span class="math notranslate nohighlight">\(Y \sim  N(\mu_Y,\sigma^2_Y)\)</span> 是互相独立的正态随机变量，
那么</p>
<ul class="simple">
<li><p>它们的 <strong>和</strong> <span class="math notranslate nohighlight">\(U=X+Y\)</span> 也满足正态分布 <span class="math notranslate nohighlight">\(U \sim N(\mu_X + \mu_Y,\sigma^2_X+\sigma^2_Y)\)</span></p></li>
<li><p>它们的 <strong>差</strong> <span class="math notranslate nohighlight">\(V=X-Y\)</span> 也满足正态分布 <span class="math notranslate nohighlight">\(V \sim N(\mu_X - \mu_Y,\sigma^2_X-\sigma^2_Y)\)</span></p></li>
</ul>
</div>
<p><strong>标准正态分布</strong></p>
<p>满足 <span class="math notranslate nohighlight">\(\mu=0,\ \sigma^1=1\)</span> 的正态分布称为 <strong>标准正态分布</strong>，
此时其概率密度函数变得简单</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-71">
<span class="eqno">(1.8.21)<a class="headerlink" href="#equation-glm-source-content-71" title="公式的永久链接"></a></span>\[f_X(x) = \frac{1}{\sqrt{2\pi }} e^{-\frac{(x)^2}{2} }\]</div>
<p>标准正态分布的累积分布函数习惯上记作 <span class="math notranslate nohighlight">\(\Phi\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-72">
<span class="eqno">(1.8.22)<a class="headerlink" href="#equation-glm-source-content-72" title="公式的永久链接"></a></span>\[\Phi(x)= \frac{1}{\sqrt{2\pi }} \int_{-\infty}^x  \exp \left ( -\frac{t^2}{2 } \right ) dt\]</div>
<p><strong>任意一个非标准的正态随机变量都可以转换成标准正态随机变量</strong>。
假设一个随机变量 <span class="math notranslate nohighlight">\(X \sim N(\mu,\sigma^2)\)</span>
，则有如下随机变量是标准正态随机变量。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-73">
<span class="eqno">(1.8.23)<a class="headerlink" href="#equation-glm-source-content-73" title="公式的永久链接"></a></span>\[Z = \frac{X-\mu}{\sigma} \sim N(0,1)\]</div>
<p>这个转换关系一定要牢记预选，在本书之后的内容中会多次应用。</p>
<p>我们知道正态分布的概率密度函数曲线是一个左右对称的钟形曲线，
对称的中心线就是期望值 <span class="math notranslate nohighlight">\(\mu\)</span> 的直线，
期望值附近的概率是最大的。
根据连续值随机变量概率密度的定义，概率密度函数曲线和 <span class="math notranslate nohighlight">\(x\)</span> 轴所围成的整个区域面积为 <span class="math notranslate nohighlight">\(1\)</span>
，我们可以把整个区域成一些固定的子区域。
如 <a class="reference internal" href="#fg-probability-008"><span class="std std-numref">图 1.8.5</span></a> 所示，
以 <span class="math notranslate nohighlight">\(0\)</span> 点为中心，向两边延伸，
<span class="math notranslate nohighlight">\([0,1]\)</span> 的区间面积占整个曲线面积的 <span class="math notranslate nohighlight">\(34.13\%\)</span>
，由于曲线是对称的，<span class="math notranslate nohighlight">\([-1,0]\)</span> 的区间面积也是 <span class="math notranslate nohighlight">\(34.13\%\)</span>
。同理， <span class="math notranslate nohighlight">\([1,2]\)</span> 和 <span class="math notranslate nohighlight">\([-2,-1]\)</span> 都是 <span class="math notranslate nohighlight">\(13.6\%\)</span>
，<span class="math notranslate nohighlight">\([2,3]\)</span> 和 <span class="math notranslate nohighlight">\([-3,-2]\)</span> 都是 <span class="math notranslate nohighlight">\(2.14\%\)</span>
。</p>
<figure class="align-center" id="id29">
<span id="fg-probability-008"></span><a class="reference internal image-reference" href="../../../_images/高斯分布_3.jpg"><img alt="../../../_images/高斯分布_3.jpg" src="../../../_images/高斯分布_3.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 1.8.5 </span><span class="caption-text">标准正态分布 <span class="math notranslate nohighlight">\(N(0,1)\)</span> 概率密度函数的区间划分。</span><a class="headerlink" href="#id29" title="永久链接至图片"></a></p>
</figcaption>
</figure>
<p>我们把中心两边合并起来，<span class="math notranslate nohighlight">\([-1,1]\)</span> 的区间面积占比就是 <span class="math notranslate nohighlight">\(68.3\%\)</span>
， <span class="math notranslate nohighlight">\([-2,2]\)</span> 的区间面积占比就是 <span class="math notranslate nohighlight">\(95.5\%\)</span>，
<span class="math notranslate nohighlight">\([-3,3]\)</span> 的区间面积占比就是 <span class="math notranslate nohighlight">\(99.7\%\)</span>，
剩下的两端 <span class="math notranslate nohighlight">\((-\infty,-3)\)</span> 以及 <span class="math notranslate nohighlight">\((3,\infty)\)</span>
加起来不足 <span class="math notranslate nohighlight">\(0.3\%\)</span>。
这意味，如果我们从一个服从标准正态分布 <span class="math notranslate nohighlight">\(N(0,1)\)</span>
的随机变量进行取样(试验)，样本值(试验结果)落在</p>
<ul class="simple">
<li><p>区间 <span class="math notranslate nohighlight">\([-1,1]\)</span> 的概率是 <span class="math notranslate nohighlight">\(68.3\%\)</span> 。</p></li>
<li><p>区间 <span class="math notranslate nohighlight">\([-2,2]\)</span> 的概率是 <span class="math notranslate nohighlight">\(95.5\%\)</span> 。</p></li>
<li><p>区间 <span class="math notranslate nohighlight">\([-3,3]\)</span> 的概率是 <span class="math notranslate nohighlight">\(99.7\%\)</span> 。</p></li>
</ul>
<p>上面是对标准正态分布概率密度函数的区间划分，那非标准正态分布是什么样的呢？
其实是类似的，变化的只是子区间的范围而已。
<a class="reference internal" href="#fg-probability-009"><span class="std std-numref">图 1.8.6</span></a> 是正态分布 <span class="math notranslate nohighlight">\(N(\mu,\sigma^2)\)</span>
的概率密度函数曲线的区间划分，
和标准正态分布相比只是横轴发生了平移和缩放而已，
中心的不再是 <span class="math notranslate nohighlight">\(0\)</span>
而是 <span class="math notranslate nohighlight">\(\mu\)</span>，
子区间扩大了一个标准差 <span class="math notranslate nohighlight">\(\sigma\)</span> 。</p>
<figure class="align-center" id="id30">
<span id="fg-probability-009"></span><a class="reference internal image-reference" href="../../../_images/高斯分布_4.jpg"><img alt="../../../_images/高斯分布_4.jpg" src="../../../_images/高斯分布_4.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 1.8.6 </span><span class="caption-text">正态分布 <span class="math notranslate nohighlight">\(N(\mu,\sigma^2)\)</span> 概率密度函数的区间划分。</span><a class="headerlink" href="#id30" title="永久链接至图片"></a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>区间 <span class="math notranslate nohighlight">\([\mu-1\sigma,\mu+1\sigma]\)</span> 的概率是 <span class="math notranslate nohighlight">\(68.3\%\)</span> 。</p></li>
<li><p>区间 <span class="math notranslate nohighlight">\([\mu-2\sigma,\mu+2\sigma]\)</span> 的概率是 <span class="math notranslate nohighlight">\(95.5\%\)</span> 。</p></li>
<li><p>区间 <span class="math notranslate nohighlight">\([\mu-3\sigma,\mu+3\sigma]\)</span> 的概率是 <span class="math notranslate nohighlight">\(99.7\%\)</span> 。</p></li>
</ul>
<p>正态分布概率密度函数曲线的这个区间划分一定要理解，
在之后讨论假设检验时会使用到。</p>
</section>
<section id="id23">
<h3><span class="section-number">1.8.6. </span>卡方分布<a class="headerlink" href="#id23" title="永久链接至标题"></a></h3>
<p>高斯变量的线性变换后仍然是服从高斯分布的，本节我们看下非线性的结果。
设随机变量 <span class="math notranslate nohighlight">\(Z_i\)</span> 是均值为 <span class="math notranslate nohighlight">\(0\)</span> 方差为 <span class="math notranslate nohighlight">\(1\)</span> 的标准正态分布变量，
随机变量 <span class="math notranslate nohighlight">\(X\)</span> 是由 <span class="math notranslate nohighlight">\(k\)</span> 个独立的 <strong>标准正态分布变量</strong> 的平方和组成。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-74">
<span class="eqno">(1.8.24)<a class="headerlink" href="#equation-glm-source-content-74" title="公式的永久链接"></a></span>\[X = Z_1^2 + Z_2^2 + \cdots + Z_k^2 = \sum_{i=1}^k  Z_i^2\]</div>
<p>则随机变量 <span class="math notranslate nohighlight">\(X\)</span> 被称为服从自由度（degree of freedom, df）为 <span class="math notranslate nohighlight">\(k\)</span> 的卡方分布，记作 <span class="math notranslate nohighlight">\(X \sim \chi^2(k)\)</span> 。
卡方分布（chi-square distribution）， 也可以写作 <span class="math notranslate nohighlight">\(\chi^2\)</span> 分布，
是一种特殊的伽玛分布，是统计推断中应用最为广泛的概率分布之一，例如假设检验和置信区间的计算。
卡方分布的概率密度函数为：</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-75">
<span class="eqno">(1.8.25)<a class="headerlink" href="#equation-glm-source-content-75" title="公式的永久链接"></a></span>\[f(x;k) = \frac{\frac{1}{2}^{\frac{k}{2}}}{\Gamma(\frac{k}{2})} x^{\frac{k}{2}-1} e^{-\frac{x}{2}}
, \quad x \ge 0\]</div>
<p>卡方分布是一个平方和，因此变量值一定是大于等于0的，对于 <span class="math notranslate nohighlight">\(x &lt;0\)</span>，
我们认为 <span class="math notranslate nohighlight">\(f(x)=0\)</span> 。
公式中的符号 <span class="math notranslate nohighlight">\(\Gamma\)</span> 表示 <code class="docutils literal notranslate"><span class="pre">Gamma</span></code> 函数，
<code class="docutils literal notranslate"><span class="pre">Gamma</span></code> 函数相当于阶乘函数在实数域的扩展，
有关 <code class="docutils literal notranslate"><span class="pre">Gamma</span></code> 函数更多的细节我们以后再讨论。
卡方分布的概率密度函数看上去十分复杂，没关系，我们不需要记住，需要的时候翻书就可以了。</p>
<p>自由度为 <span class="math notranslate nohighlight">\(k\)</span> 的卡方变量的平均值是和方差是分别为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-76">
<span class="eqno">(1.8.26)<a class="headerlink" href="#equation-glm-source-content-76" title="公式的永久链接"></a></span>\[\mathbb{E}[X] = k, \quad V(x) = 2k\]</div>
<p>现在我们来看下不同自由度下，卡方分布概率密度函数曲线的变化，
如 <a class="reference internal" href="#fg-probability-011"><span class="std std-numref">图 1.8.7</span></a> 所示，
可以看到随着自由度的增加，卡方分布的曲线逐步变成钟形曲线，
越来越进行正态分布。</p>
<figure class="align-center" id="id31">
<span id="fg-probability-011"></span><a class="reference internal image-reference" href="../../../_images/卡方分布.jpg"><img alt="../../../_images/卡方分布.jpg" src="../../../_images/卡方分布.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 1.8.7 </span><span class="caption-text">随着自由度的增加，卡方分布的曲线逐步变成钟形曲线。</span><a class="headerlink" href="#id31" title="永久链接至图片"></a></p>
</figcaption>
</figure>
<p>卡方分布的累积分布函数为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-77">
<span class="eqno">(1.8.27)<a class="headerlink" href="#equation-glm-source-content-77" title="公式的永久链接"></a></span>\[F(x;k) = \frac{\gamma (\frac{k}{2},\frac{x}{2}) }{\Gamma (\frac{k}{2})}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\gamma\)</span> 为不完全 <span class="math notranslate nohighlight">\(\Gamma\)</span> 函数。</p>
<figure class="align-center" id="id32">
<span id="fg-probability-012"></span><a class="reference internal image-reference" href="../../../_images/卡方分布_累积分布.jpg"><img alt="../../../_images/卡方分布_累积分布.jpg" src="../../../_images/卡方分布_累积分布.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 1.8.8 </span><span class="caption-text">卡方分布的累积分布函数。</span><a class="headerlink" href="#id32" title="永久链接至图片"></a></p>
</figcaption>
</figure>
<p>可以明显看出卡方分布的自由度参数影响着其累积分布函数的斜率，
自由度越小斜率越大，自由度越大斜率越小。</p>
<p><strong>卡方分布的可加性</strong></p>
<p>由卡方变量的定义可得，独立卡方变量之和同样服从卡方分布。
特别地，若 <span class="math notranslate nohighlight">\(X_{1},X_{2},\dots X_{n}\)</span>
分别独立服从自由度为 <span class="math notranslate nohighlight">\(k_{1},k_{2},\dots k_{n}\)</span> 的卡方分布，
那么它们的和 <span class="math notranslate nohighlight">\(\sum _{i=1}^{n} X_{i}\)</span>
服从自由度为 <span class="math notranslate nohighlight">\(\sum _{i=1}^{n}k_{i}\)</span> 的卡方分布。</p>
<p><strong>非中心化卡方分布</strong></p>
<p>卡方分布的定义中要求是 <strong>标准正态分布（期望为0，方差为1）</strong>
的平方和，那如何不是标准正态分布呢？</p>
<p>设随机变量 <span class="math notranslate nohighlight">\(Z_i\)</span> 是均值为 <span class="math notranslate nohighlight">\(\mu_i\)</span> 方差为 <span class="math notranslate nohighlight">\(1\)</span> 的正态分布变量，
随机变量 <span class="math notranslate nohighlight">\(X\)</span> 是由 <span class="math notranslate nohighlight">\(k\)</span> 个独立的 <strong>单位方差正态分布变量</strong> 的平方和组成。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-78">
<span class="eqno">(1.8.28)<a class="headerlink" href="#equation-glm-source-content-78" title="公式的永久链接"></a></span>\[X = Z_1^2 + Z_2^2 + \cdots + Z_k^2 = \sum_{i=1}^k  Z_i^2\]</div>
<p>则随机变量 <span class="math notranslate nohighlight">\(X\)</span> 被称为服从自由度为 <span class="math notranslate nohighlight">\(k\)</span> 的 <strong>非中心化卡方分布（Noncentral chi-squared distribution）</strong>，
为了区分二者，由标准正态分布得到的卡方分布可以称为 <strong>中心化卡方分布</strong>。
非中心化卡方分布的概率密度函数中多了一个非中心化参数 <span class="math notranslate nohighlight">\(\lambda\)</span>
。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-79">
<span class="eqno">(1.8.29)<a class="headerlink" href="#equation-glm-source-content-79" title="公式的永久链接"></a></span>\[f(x;\lambda,k) = e^{-\lambda/2} \sum_{i=0}^{\infty}
\frac{(\lambda/2)^k }{k!} f_{Y_{k+2i}}(x)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(f_{Y_{k+2i}}(x)\)</span> 是自由度为 <span class="math notranslate nohighlight">\(k+2i\)</span> 的中心卡方分布的概率密度函数。
<span class="math notranslate nohighlight">\(\lambda\)</span> 称为非中心化参数（non-centrality parameter），
由下式给出</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-80">
<span class="eqno">(1.8.30)<a class="headerlink" href="#equation-glm-source-content-80" title="公式的永久链接"></a></span>\[\lambda = \sum_{i=1}^k \mu_i^2\]</div>
<p>非中心化卡方分布的概率密度函数变得异常复杂，我们无需关注它的细节，
只需要清楚非中心化卡方分布和中心化卡方分布的区别即可。
非中心化卡方分布的期望和方差为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-81">
<span class="eqno">(1.8.31)<a class="headerlink" href="#equation-glm-source-content-81" title="公式的永久链接"></a></span>\[\mathbb{E}[X] = k+\lambda, \quad V(x) = 2(k+2\lambda)\]</div>
</section>
<section id="t">
<span id="ch-probability-t"></span><h3><span class="section-number">1.8.7. </span>t分布<a class="headerlink" href="#t" title="永久链接至标题"></a></h3>
<p>t分布的推导最早由大地测量学家F <code class="docutils literal notranslate"><span class="pre">riedrich</span> <span class="pre">Robert</span> <span class="pre">Helmert</span></code> 于1876年提出，并由数学家 <code class="docutils literal notranslate"><span class="pre">Lüroth</span></code> 证明。
英国人威廉·戈塞（Willam S. Gosset）于1908年再次发现并发表了t分布，
当时他还在爱尔兰都柏林的吉尼斯（Guinness）啤酒酿酒厂工作。
酒厂虽然禁止员工发表一切与酿酒研究有关的成果，
但允许他在不提到酿酒的前提下，以笔名发表t分布的发现，所以论文使用了“学生”（Student）这一笔名。
之后t检定以及相关理论经由罗纳德·费希尔（Sir Ronald Aylmer Fisher）发扬光大，
为了感谢戈塞的功劳，费希尔将此分布命名为学生t分布（Student’s t）。
t分布是标准正态分布的一个近似分布，当不知道标准正态分布的方差时，
经常用t分布做标准正态分布的一个替代（近似）。</p>
<p>假设 <span class="math notranslate nohighlight">\(X\)</span> 是呈正态分布的独立的随机变量,
它的期望值为 <span class="math notranslate nohighlight">\(\mu\)</span>，
方差为 <span class="math notranslate nohighlight">\(\sigma^2\)</span>，方差未知。
令 <span class="math notranslate nohighlight">\(X_1,X_2,\cdots,X_N\)</span>
为随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的一个独立同分布的观测样本序列，
则样本的均值为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-82">
<span class="eqno">(1.8.32)<a class="headerlink" href="#equation-glm-source-content-82" title="公式的永久链接"></a></span>\[\bar{X}_N = \frac{X_1+X_2+\cdots+X_N}{N}\]</div>
<p>样本的方差为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-83">
<span class="eqno">(1.8.33)<a class="headerlink" href="#equation-glm-source-content-83" title="公式的永久链接"></a></span>\[S_N^2 = \frac{1}{N-1} \sum_{i=1}^N(X_i - \bar{X})^2\]</div>
<p>定义如下变量</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-84">
<span class="eqno">(1.8.34)<a class="headerlink" href="#equation-glm-source-content-84" title="公式的永久链接"></a></span>\[T= \frac{\bar{X}_N - \mu}{\frac{S_N}{\sqrt{N}}}\]</div>
<p>则变量 <span class="math notranslate nohighlight">\(T\)</span> 是一个随机变量，它的分布称为 t-分布，
它的概率密度函数是</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-85">
<span class="eqno">(1.8.35)<a class="headerlink" href="#equation-glm-source-content-85" title="公式的永久链接"></a></span>\[f(t) = \frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}(1+\frac{t^2}{\nu})^{\frac{-(\nu+1)}{2}}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\nu=N-1\)</span>，参数 <span class="math notranslate nohighlight">\(\nu\)</span> 一般称为自由度，<span class="math notranslate nohighlight">\(\Gamma\)</span> 是伽马函数。
当 <span class="math notranslate nohighlight">\(\nu&gt;1\)</span> 时，它的期望是 <span class="math notranslate nohighlight">\(0\)</span>，<span class="math notranslate nohighlight">\(\nu=1\)</span> 时未定义。
当 <span class="math notranslate nohighlight">\(\nu&gt;2\)</span> 时，它的方差是 <span class="math notranslate nohighlight">\(\nu/(\nu-2)\)</span>，否则无穷大。</p>
<p>t分布作为标准正态分布的近似分布，它的概率密度函数曲线和标准正态分布是非常接近的，
并且随着自由度的增加，二者越来越接近，当自由度足够大时，t分布就等价于标准正态分布。
<a class="reference internal" href="#fg-probability-015"><span class="std std-numref">图 1.8.9</span></a> 展示了自由度分别为 <span class="math notranslate nohighlight">\(1, 2, 3, 5, 10, 30\)</span> 时，
t分布和标准正态分布的差异，可以看到当自由度是 <span class="math notranslate nohighlight">\(30\)</span> 时，二者已经基本重合。</p>
<figure class="align-center" id="id33">
<span id="fg-probability-015"></span><a class="reference internal image-reference" href="../../../_images/t分布.jpg"><img alt="../../../_images/t分布.jpg" src="../../../_images/t分布.jpg" style="width: 896.0px; height: 1007.3px;" /></a>
<figcaption>
<p><span class="caption-number">图 1.8.9 </span><span class="caption-text">t分布的概率密度函数和标准正态分布概率密度函数的对比。
当自由度为 <span class="math notranslate nohighlight">\(30\)</span> 时，二者基本重合。</span><a class="headerlink" href="#id33" title="永久链接至图片"></a></p>
</figcaption>
</figure>
<p>从图形可以看出，二者期望是一样的，都是 <span class="math notranslate nohighlight">\(0\)</span>
。当自由度小于 <span class="math notranslate nohighlight">\(30\)</span> 时，
t分布的方差是略大于标准正态分布的。
当我们不知道标准正态分布的方差时，
就可以通过标准正态分布的采样（观测）样本计算一个样本方差，
利用样本方差确定一个t分布，
然后用这个t分布近似模拟原来的标准正态分布进行后续的分析使用，
当然如果你的采样样本数量超过 <span class="math notranslate nohighlight">\(30\)</span> 个，
直接使用样本方差作为标准正态分布的方差估计值，
然后直接使用标准正态分布 <span class="math notranslate nohighlight">\(N(0,S_N)\)</span> 进行分析使用也是可以的，
因为此时t分布和标准正态分布已经没有区别了。</p>
</section>
<section id="f">
<h3><span class="section-number">1.8.8. </span>F分布<a class="headerlink" href="#f" title="永久链接至标题"></a></h3>
<p>卡方分布、t分布和F分布是统计学中正态总体的三大抽样分布，
有关什么是总体分布与抽样分布，我们在 <a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution"><span class="std std-numref">节 3.2</span></a>
会详细讨论。</p>
<p>F分布也是一个连续值分布，它概率密度函数十分复杂，本书不需要过多关注，这里就不再给出概率密度函数函数的具体形式。
我们重点关注卡方分布和F分布的关系。</p>
<p>假设 <span class="math notranslate nohighlight">\(U_1\)</span> 和 <span class="math notranslate nohighlight">\(U_1\)</span> 分别自由度为 <span class="math notranslate nohighlight">\(d_1\)</span> 和 <span class="math notranslate nohighlight">\(d_2\)</span>
的两个 <strong>独立</strong> 卡方随机变量，则如下随机变量服从F分布。</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-86">
<span class="eqno">(1.8.36)<a class="headerlink" href="#equation-glm-source-content-86" title="公式的永久链接"></a></span>\[F = \frac{U_1/d_1}{U_2/d_2}\]</div>
<p>F分布的概率密度函数有两个参数 <span class="math notranslate nohighlight">\(d_1\)</span> 和 <span class="math notranslate nohighlight">\(d_2\)</span>
，记作 <span class="math notranslate nohighlight">\(F(d_1,d_2)\)</span> 。
F分布的期望值为</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-87">
<span class="eqno">(1.8.37)<a class="headerlink" href="#equation-glm-source-content-87" title="公式的永久链接"></a></span>\[\frac{d_2}{d_2-2},\quad d_2 &gt; 2\]</div>
<p>它的方差是</p>
<div class="math notranslate nohighlight" id="equation-glm-source-content-88">
<span class="eqno">(1.8.38)<a class="headerlink" href="#equation-glm-source-content-88" title="公式的永久链接"></a></span>\[\frac{2d_2^2(d_1+d_2-2)}{d_1(d_2-2)^2(d_2-4)}, \quad d_2 &gt; 4\]</div>
<p><a class="reference internal" href="#fg-probability-017"><span class="std std-numref">图 1.8.10</span></a> 是F分布的概率密度函数曲线，
可以看出随着 <span class="math notranslate nohighlight">\(d_1,d_2\)</span> 的增加，F分布会变成一个类似正态分布的曲线，
并且越来越尖锐。</p>
<figure class="align-center" id="id34">
<span id="fg-probability-017"></span><a class="reference internal image-reference" href="../../../_images/F分布概率密度.jpg"><img alt="../../../_images/F分布概率密度.jpg" src="../../../_images/F分布概率密度.jpg" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">图 1.8.10 </span><span class="caption-text">F分布的概率密度</span><a class="headerlink" href="#id34" title="永久链接至图片"></a></p>
</figcaption>
</figure>
<p>到这里我们发现无论是卡方分布、t分布、F分布都和正态分布有关系，
实际上，大部分概率分布都是和正态分布有关系的，
本章最后，我们给出一张图（<a class="reference internal" href="#fg-probability-020"><span class="std std-numref">图 1.8.11</span></a>）来直观感受一下。</p>
<figure class="align-center" id="id35">
<span id="fg-probability-020"></span><a class="reference internal image-reference" href="../../../_images/概率分布之间的关系.png"><img alt="../../../_images/概率分布之间的关系.png" src="../../../_images/概率分布之间的关系.png" style="width: 981.4px; height: 1229.1999999999998px;" /></a>
<figcaption>
<p><span class="caption-number">图 1.8.11 </span><span class="caption-text">概率分布之间的关系图</span><a class="headerlink" href="#id35" title="永久链接至图片"></a></p>
</figcaption>
</figure>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../index.html" class="btn btn-neutral float-left" title="广义线性模型" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html" class="btn btn-neutral float-right" title="2. 最大似然估计" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2018, zhangzhenhu(acmtiger@outlook.com) 禁止一切形式的转载！.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
  


    <script src="https://utteranc.es/client.js"
            repo="zhangzhenhu/blog"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>



</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>